{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def articles():\n",
    "    with open('page_revisions_text', 'rb') as text_file:\n",
    "        pending_article_data = b''\n",
    "        while True:\n",
    "            data = text_file.read(1024 * 1024)\n",
    "            if len(data) == 0:\n",
    "                break\n",
    "\n",
    "            articles = data.split(b'\\0')\n",
    "            articles[0] = pending_article_data + articles[0]\n",
    "            for index, article in enumerate(articles):\n",
    "                if index + 1 == len(articles):\n",
    "                    pending_article_data = article\n",
    "                else:\n",
    "                    yield article\n",
    "\n",
    "        print(pending_article_data)\n",
    "        if len(pending_article_data) != 0:\n",
    "            yield pending_article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SubwordTextEncoder vocab_size=276>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword_text_encoder = tfds.features.text.SubwordTextEncoder.build_from_corpus(articles(), 260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e_',\n",
       " 'er',\n",
       " 'in',\n",
       " 'an',\n",
       " ']]',\n",
       " 'd_',\n",
       " 'on',\n",
       " ', ',\n",
       " 'or',\n",
       " 's_',\n",
       " 'at',\n",
       " 'en',\n",
       " 'ar',\n",
       " ' [[',\n",
       " 'es',\n",
       " 'th',\n",
       " 'al',\n",
       " 'is',\n",
       " 'the_']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword_text_encoder.subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subword_text_encoder.subwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ª—É—á–∏—Ö–º–µ —É–∂–∞—Å–Ω–æ –º–∞–ª—ä–∫ —Ä–µ—á–Ω–∏–∫ –æ—Ç –ø–æ–¥-–¥—É–º–∏. –ó–∞ `vocab_size` –ø–æ–ª—É—á–∞–≤–∞–º–µ 276, –∞ —Ä–µ–∞–ª–Ω–æ –∏–º–∞–º–µ 19 –ø–æ–¥-–¥—É–º–∏ –≤ `subwords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword_text_encoder.vocab_size - len(subword_text_encoder.subwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞–∑–ª–∏–∫–∞—Ç–∞ –≤ —Ä–∞–∑–º–µ—Ä–∏—Ç–µ –µ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª–Ω–æ –±–ª–∏–∑–æ –¥–æ 256. –ò–∑–≥–ª–µ–∂–¥–∞ –æ—á–∞–∫–≤–∞–Ω–∏—è—Ç —Ä–∞–∑–º–µ—Ä, –∫–æ–π—Ç–æ –∑–∞–¥–∞–¥–æ—Ö–º–µ (–≤ —Å–ª—É—á–∞—è - 260) –≤–∫–ª—é—á–≤–∞ –≤ —Å–µ–±–µ —Å–∏ –∏ –µ–¥–∏–Ω–∏—á–Ω–∏—Ç–µ —Å–∏–º–≤–æ–ª–∏.\n",
    "# ü§¶‚Äç‚ôÇÔ∏è\n",
    "\n",
    "<br>\n",
    "\n",
    "–°–º—è—Ç–∞–Ω–µ—Ç–æ –æ—Ç–Ω–µ –ø—Ä–∏–ª–∏—á–Ω–æ –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ, –∞ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–∞ –Ω–µ –¥–æ–≤–µ–¥–µ –¥–æ –Ω–∏—â–æ —Å–º–∏—Å–ª–µ–Ωo. –ò –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è—Ç–∞ –Ω–µ –±–µ—à–µ —Å–ø–æ–º–µ–Ω–∞–ª–∞ –Ω–∏—â–æ –ø–æ –≤—ä–ø—Ä–æ—Å–∞.\n",
    "\n",
    "–ò–∑–≥–ª–µ–∂–¥–∞ —â–µ —Ç—Ä—è–±–≤–∞ –¥–∞ –º–∏–Ω–µ–º –ø—Ä–µ–∑ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞—Ç–∞ –æ—â–µ –≤–µ–¥–Ω—ä–∂, –Ω–æ —Å –ø–æ-–≥–æ–ª—è–º —Ä–µ—á–Ω–∏–∫. –ù–µ–∫–∞ –¥–æ—Ä–∏ –æ–ø–∏—Ç–∞–º–µ —Å **–¥–æ—Å—Ç–∞** –ø–æ-–≥–æ–ª—è–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n"
     ]
    }
   ],
   "source": [
    "subword_text_encoder = tfds.features.text.SubwordTextEncoder.build_from_corpus(articles(), 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SubwordTextEncoder vocab_size=2046>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword_text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the_',\n",
       " ', ',\n",
       " ' [[',\n",
       " 'of_',\n",
       " 's_',\n",
       " 'and_',\n",
       " ']] ',\n",
       " '. ',\n",
       " 'a_',\n",
       " 'in_',\n",
       " 'to_',\n",
       " 'd_',\n",
       " 'is_',\n",
       " 'e_',\n",
       " '[[',\n",
       " ' (',\n",
       " 'ed_',\n",
       " 'y_',\n",
       " ']], ',\n",
       " 'the',\n",
       " 'The_',\n",
       " 't_',\n",
       " 'on_',\n",
       " 'ing_',\n",
       " ']]\\n[[',\n",
       " 'for_',\n",
       " 'was_',\n",
       " 'are_',\n",
       " 'in',\n",
       " '.  ',\n",
       " '% ',\n",
       " ']]',\n",
       " 'as_',\n",
       " 'on',\n",
       " 'with_',\n",
       " 'n_',\n",
       " 'r_',\n",
       " '.\\n\\n',\n",
       " 'al_',\n",
       " 'er',\n",
       " 'an_',\n",
       " 'and',\n",
       " ')|',\n",
       " 'that_',\n",
       " 'by_',\n",
       " 'of',\n",
       " 'l_',\n",
       " 'or_',\n",
       " 'from_',\n",
       " \"''\",\n",
       " 'ly_',\n",
       " 'ro',\n",
       " 'es',\n",
       " 'le',\n",
       " '\" ',\n",
       " \"'''\",\n",
       " ']], [[',\n",
       " '\\\\&undsc',\n",
       " 'es_',\n",
       " 'an',\n",
       " 'at_',\n",
       " ') ',\n",
       " 'is',\n",
       " 'to',\n",
       " 'http',\n",
       " '://',\n",
       " 'ng_',\n",
       " ' \"',\n",
       " ': ',\n",
       " 'Category',\n",
       " 'as',\n",
       " '{{',\n",
       " 'li',\n",
       " 'or',\n",
       " '18',\n",
       " 'us',\n",
       " 'ne',\n",
       " 'st_',\n",
       " 'de',\n",
       " 'it_',\n",
       " 'er_',\n",
       " \" ''\",\n",
       " 'al',\n",
       " '; ',\n",
       " 'ra',\n",
       " 'his_',\n",
       " 'have_',\n",
       " 'ol',\n",
       " 'lo',\n",
       " 'be_',\n",
       " 'hi',\n",
       " 'which_',\n",
       " ']]. ',\n",
       " 'en',\n",
       " 'ng',\n",
       " 'mi',\n",
       " 'ad',\n",
       " 'na',\n",
       " 'ri',\n",
       " 'sup2',\n",
       " 'ed',\n",
       " 'www',\n",
       " 'ge',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'The',\n",
       " 'ga',\n",
       " 'it',\n",
       " 'ha',\n",
       " 'ar',\n",
       " 'Re',\n",
       " '</',\n",
       " 'ing',\n",
       " 'Se',\n",
       " 'age_',\n",
       " \"'' \",\n",
       " 'ta',\n",
       " 'fa',\n",
       " 'la',\n",
       " 'com',\n",
       " 'Census',\n",
       " ']] (',\n",
       " 'th',\n",
       " 'go',\n",
       " 'me',\n",
       " 'be',\n",
       " 'do',\n",
       " 'pi',\n",
       " 'ea',\n",
       " 'ru',\n",
       " 'si',\n",
       " 'La',\n",
       " 'bo',\n",
       " 'Ma',\n",
       " 're',\n",
       " 'ma',\n",
       " 'Be',\n",
       " 'te',\n",
       " '=\"',\n",
       " 'st',\n",
       " 'da',\n",
       " 'ic',\n",
       " 'ci',\n",
       " 'co',\n",
       " 'at',\n",
       " 'Mar',\n",
       " 'ke',\n",
       " 'In_',\n",
       " 'ia',\n",
       " 'ter',\n",
       " 'he_',\n",
       " 'nt',\n",
       " 'An',\n",
       " '10',\n",
       " 'tion',\n",
       " 'fi',\n",
       " ']]\\n*[[',\n",
       " 'ce',\n",
       " '199',\n",
       " 'no',\n",
       " 'Ba',\n",
       " 'Ca',\n",
       " 'fe',\n",
       " 'ni',\n",
       " 'td',\n",
       " 've_',\n",
       " 'has_',\n",
       " 'pro',\n",
       " 'tion_',\n",
       " 'In',\n",
       " 'el',\n",
       " 'ot',\n",
       " 'Ar',\n",
       " 'we',\n",
       " 'ber',\n",
       " 'am',\n",
       " 'ion',\n",
       " 'gu',\n",
       " '| ',\n",
       " 'le_',\n",
       " 'mp',\n",
       " 'et',\n",
       " 'ur',\n",
       " 'ns',\n",
       " 'not_',\n",
       " 'De',\n",
       " 'ct',\n",
       " 'Li',\n",
       " 'pa',\n",
       " ']] [[',\n",
       " 'ie',\n",
       " 'h_',\n",
       " 'ho',\n",
       " 'nd',\n",
       " 'um',\n",
       " 'Me',\n",
       " 'k_',\n",
       " 'Al',\n",
       " 'he',\n",
       " 'ion_',\n",
       " 'were_',\n",
       " 'th_',\n",
       " 'per',\n",
       " 'man',\n",
       " 'mu',\n",
       " 'one_',\n",
       " 'ce_',\n",
       " 'ee',\n",
       " '>\\n',\n",
       " 'der',\n",
       " 'Bo',\n",
       " 'ba',\n",
       " 'ver',\n",
       " 'tt',\n",
       " 'g_',\n",
       " 'who_',\n",
       " 'con',\n",
       " 'So',\n",
       " 'Ro',\n",
       " 'Da',\n",
       " 'ted_',\n",
       " 'pu',\n",
       " 'Pa',\n",
       " 'also_',\n",
       " 'for',\n",
       " 'Wi',\n",
       " 'had_',\n",
       " 'Ho',\n",
       " 'il',\n",
       " 'di',\n",
       " 'ck',\n",
       " 'ist',\n",
       " 'pe',\n",
       " 'vo',\n",
       " 'Le',\n",
       " 'm_',\n",
       " 'ti',\n",
       " 'ph',\n",
       " 'o_',\n",
       " 'population_',\n",
       " 'Sa',\n",
       " 'km',\n",
       " 'ry_',\n",
       " 'ment',\n",
       " 'mo',\n",
       " 'sti',\n",
       " 'rs',\n",
       " 'line',\n",
       " 'Lo',\n",
       " '\\n*',\n",
       " 'this_',\n",
       " ' ==\\n',\n",
       " 'wa',\n",
       " ' $',\n",
       " 'em',\n",
       " 'United_',\n",
       " 'rt',\n",
       " 'Ha',\n",
       " 'but_',\n",
       " '17',\n",
       " 'br',\n",
       " '\\n\\n',\n",
       " 'ow',\n",
       " 'io',\n",
       " 'A_',\n",
       " 'll',\n",
       " 'un',\n",
       " '12',\n",
       " 'Te',\n",
       " 'ton',\n",
       " 'ic_',\n",
       " 'other_',\n",
       " 'Mi',\n",
       " 'op',\n",
       " \"''[[\",\n",
       " 'out_',\n",
       " 'su',\n",
       " '15',\n",
       " 've',\n",
       " 'Ja',\n",
       " 'end',\n",
       " 'ine',\n",
       " 'are',\n",
       " 'by',\n",
       " '\\n|',\n",
       " 'right',\n",
       " 'ment_',\n",
       " 'est',\n",
       " 'fl',\n",
       " 'ry',\n",
       " 'ir',\n",
       " '), ',\n",
       " '% [[',\n",
       " 'te_',\n",
       " 'there_',\n",
       " 'vi',\n",
       " 'wi',\n",
       " 'Si',\n",
       " 'Image',\n",
       " 'ze',\n",
       " \"''' \",\n",
       " 'os',\n",
       " 'County',\n",
       " '25',\n",
       " 'sc',\n",
       " 'que',\n",
       " 'min',\n",
       " 'Po',\n",
       " 'ch_',\n",
       " '}}\\n',\n",
       " '16',\n",
       " 'str',\n",
       " 'us_',\n",
       " 'mb',\n",
       " 'sub',\n",
       " 'bi',\n",
       " 'fr',\n",
       " '198',\n",
       " 'im',\n",
       " 'Mo',\n",
       " 'Con',\n",
       " 'Su',\n",
       " 'their_',\n",
       " 'hu',\n",
       " 'ag',\n",
       " 'pl',\n",
       " 'bu',\n",
       " 'Go',\n",
       " 'To',\n",
       " ' - ',\n",
       " 'ent',\n",
       " 'lt',\n",
       " 'ul',\n",
       " 'dr',\n",
       " 'under_',\n",
       " 'No',\n",
       " 'more_',\n",
       " 'rd',\n",
       " '11',\n",
       " ']]\\n* [[',\n",
       " 'se_',\n",
       " 'ac',\n",
       " 'ric',\n",
       " 'c_',\n",
       " 'tr',\n",
       " 'po',\n",
       " 'Vi',\n",
       " 'He',\n",
       " 'ation_',\n",
       " 'its_',\n",
       " 'ec',\n",
       " 'des',\n",
       " 'rn',\n",
       " 'ate',\n",
       " '  ',\n",
       " 'ue',\n",
       " 'cou',\n",
       " 'de_',\n",
       " 'ly',\n",
       " 'ch',\n",
       " '20',\n",
       " 'rea',\n",
       " 'American',\n",
       " 'land',\n",
       " 'all_',\n",
       " 'pre',\n",
       " 'sta',\n",
       " '><',\n",
       " 'ure',\n",
       " 'Hi',\n",
       " 'Ne',\n",
       " 'gi',\n",
       " ']].  ',\n",
       " 'Pe',\n",
       " 'ge_',\n",
       " 'Fi',\n",
       " 'pri',\n",
       " 'pt',\n",
       " '200',\n",
       " 'ab',\n",
       " 'son',\n",
       " 'lu',\n",
       " 'gra',\n",
       " 'ther',\n",
       " 'Ra',\n",
       " '14',\n",
       " 'rc',\n",
       " 'en_',\n",
       " 'New_',\n",
       " '18_',\n",
       " 'res',\n",
       " 'living_',\n",
       " 'rr',\n",
       " 'nt_',\n",
       " '\\n*[[',\n",
       " 'che',\n",
       " ', [[',\n",
       " '\\n| ',\n",
       " 'Jo',\n",
       " 'ss',\n",
       " 'up_',\n",
       " 'org',\n",
       " 'align',\n",
       " 'Na',\n",
       " \" ''[[\",\n",
       " '00',\n",
       " 'mon',\n",
       " '197',\n",
       " 'od',\n",
       " 'eri',\n",
       " ']] - [[',\n",
       " 'if',\n",
       " 'ex',\n",
       " 'wh',\n",
       " 'first_',\n",
       " 'val',\n",
       " '30',\n",
       " 'Ga',\n",
       " 'she',\n",
       " 'Co',\n",
       " 'ff',\n",
       " 'been_',\n",
       " 'median_',\n",
       " 'll_',\n",
       " 'so',\n",
       " 'ort',\n",
       " 'rl',\n",
       " 'It_',\n",
       " 'work',\n",
       " 'ft',\n",
       " 'vel',\n",
       " 'two_',\n",
       " 'Do',\n",
       " 'oc',\n",
       " 'most_',\n",
       " 'fo',\n",
       " 'ani',\n",
       " 'ian',\n",
       " 'sup',\n",
       " 'ssi',\n",
       " 'ig',\n",
       " 'car',\n",
       " ' = ',\n",
       " 'Ta',\n",
       " 'cont',\n",
       " '===\\n',\n",
       " 'ive',\n",
       " 'ca',\n",
       " 'ne_',\n",
       " 'bl',\n",
       " 'can_',\n",
       " 'inter',\n",
       " '13',\n",
       " 'mat',\n",
       " 'du',\n",
       " '\\n* ',\n",
       " 'dis',\n",
       " 'with',\n",
       " '- ',\n",
       " 'rg',\n",
       " 'nn',\n",
       " 'them',\n",
       " 'sho',\n",
       " 'tu',\n",
       " '24',\n",
       " 'gen',\n",
       " 'ki',\n",
       " \" '''\",\n",
       " 'comp',\n",
       " 'total_',\n",
       " 'Je',\n",
       " 'rep',\n",
       " 'nes',\n",
       " 'ter_',\n",
       " 'tic',\n",
       " 'ten',\n",
       " ']]\\n',\n",
       " '1_',\n",
       " 'Fa',\n",
       " 'act',\n",
       " 'ide',\n",
       " 'ts_',\n",
       " 'rs_',\n",
       " ']])',\n",
       " 'w_',\n",
       " '* ',\n",
       " 'lle',\n",
       " 'ip',\n",
       " 'mm',\n",
       " 'om',\n",
       " 'ts',\n",
       " 'tra',\n",
       " 'For_',\n",
       " 'id',\n",
       " 'ers',\n",
       " 'they_',\n",
       " 'me_',\n",
       " 'ini',\n",
       " 'This_',\n",
       " 'name',\n",
       " 'tri',\n",
       " '; (',\n",
       " 'On',\n",
       " 'ty_',\n",
       " 'people',\n",
       " 'ation',\n",
       " 'int',\n",
       " '.\\n\\n== ',\n",
       " 'ica',\n",
       " 'Du',\n",
       " 'all',\n",
       " 'Cr',\n",
       " 'p_',\n",
       " 'au',\n",
       " 'att',\n",
       " 'ite',\n",
       " 'ya',\n",
       " '196',\n",
       " 'her_',\n",
       " 'used_',\n",
       " 'og',\n",
       " 'He_',\n",
       " 'ess',\n",
       " 'sy',\n",
       " 'ant',\n",
       " 'sk',\n",
       " 'hea',\n",
       " 'nor',\n",
       " 'ies',\n",
       " 'sou',\n",
       " 'ins',\n",
       " 'state',\n",
       " 'Sh',\n",
       " 'Ch',\n",
       " 'nce',\n",
       " 'St',\n",
       " 'census',\n",
       " 'dd',\n",
       " 'wo',\n",
       " 'Bi',\n",
       " 'ye',\n",
       " 'years_',\n",
       " 'inf',\n",
       " 'Bu',\n",
       " 'Fe',\n",
       " 'Di',\n",
       " 'ak',\n",
       " 'ut',\n",
       " 'ren',\n",
       " '2_',\n",
       " 'ran',\n",
       " 're_',\n",
       " 'ard',\n",
       " 'sl',\n",
       " 'use',\n",
       " 'see',\n",
       " '2000',\n",
       " ']].\\n\\n',\n",
       " 'ult',\n",
       " 'cri',\n",
       " 'over',\n",
       " 'acc',\n",
       " 'Pro',\n",
       " 'ish',\n",
       " 'up',\n",
       " 'sec',\n",
       " 'ou',\n",
       " 'chi',\n",
       " 'There_',\n",
       " 'ast',\n",
       " 'war',\n",
       " '==\\n',\n",
       " 'ble',\n",
       " 'US',\n",
       " '0_',\n",
       " 'ori',\n",
       " 'ever',\n",
       " '/ ',\n",
       " 'imp',\n",
       " 'va',\n",
       " 'ps',\n",
       " ')\\n*',\n",
       " 'ina',\n",
       " 'As_',\n",
       " 'Car',\n",
       " 'no_',\n",
       " 'app',\n",
       " 'ial_',\n",
       " 'gre',\n",
       " 'serv',\n",
       " '21',\n",
       " 'Ka',\n",
       " 'bec',\n",
       " 'older',\n",
       " 'spe',\n",
       " 'tal',\n",
       " 'Ea',\n",
       " 'average_',\n",
       " 'such_',\n",
       " 'ent_',\n",
       " 'ain',\n",
       " 'ld',\n",
       " 'bel',\n",
       " 'into_',\n",
       " 'lit',\n",
       " 'ob',\n",
       " \"'', \",\n",
       " 'city_',\n",
       " 'za',\n",
       " 'oo',\n",
       " 'Col',\n",
       " 'eg',\n",
       " 'one',\n",
       " ']])\\n*',\n",
       " '.\\n\\n==',\n",
       " 'nc',\n",
       " 'ls',\n",
       " 'qui',\n",
       " 'ka',\n",
       " 'rac',\n",
       " 'cu',\n",
       " 'ture',\n",
       " 'win',\n",
       " 'ple',\n",
       " 'ali',\n",
       " '65_',\n",
       " 'ical_',\n",
       " 'uc',\n",
       " '= ',\n",
       " 'non',\n",
       " 'tin',\n",
       " 'center',\n",
       " 'Fr',\n",
       " 'ster',\n",
       " \"]]'' \",\n",
       " 'met',\n",
       " 'income_',\n",
       " 'sm',\n",
       " '] ',\n",
       " 'sto',\n",
       " 'sel',\n",
       " 'ria',\n",
       " 'jpg',\n",
       " 'Sc',\n",
       " 'cha',\n",
       " 'some_',\n",
       " 'form',\n",
       " 'tle',\n",
       " 'mil',\n",
       " 'ition',\n",
       " 'nd_',\n",
       " 'tor',\n",
       " 'red_',\n",
       " 'language',\n",
       " 'from',\n",
       " 'ble_',\n",
       " 'ju',\n",
       " 'ok',\n",
       " 'ret',\n",
       " 'Wh',\n",
       " 'Wa',\n",
       " '> ',\n",
       " 'Ri',\n",
       " 'f_',\n",
       " 'those_',\n",
       " '|\\n',\n",
       " 'ute',\n",
       " 'Ju',\n",
       " 'ep',\n",
       " 'ati',\n",
       " '3_',\n",
       " 'ja',\n",
       " 'ert',\n",
       " 'han',\n",
       " 'ei',\n",
       " 'ther_',\n",
       " 'day',\n",
       " 'xi',\n",
       " 'Tr',\n",
       " 'math',\n",
       " 'Br',\n",
       " '==\\n*',\n",
       " 'ice',\n",
       " 'cy',\n",
       " 'ris',\n",
       " 'fin',\n",
       " '22',\n",
       " 'ty',\n",
       " 'tes',\n",
       " 'War',\n",
       " 'Man',\n",
       " '. \\n\\n',\n",
       " ' | ',\n",
       " 'den',\n",
       " 'ron',\n",
       " 'not',\n",
       " 'sea',\n",
       " 'unt',\n",
       " 'Or',\n",
       " \"'' (\",\n",
       " 'shi',\n",
       " 'art',\n",
       " '√©',\n",
       " 'x_',\n",
       " 'Gr',\n",
       " 'North',\n",
       " 'made_',\n",
       " 'rie',\n",
       " 'El',\n",
       " 'ject',\n",
       " 'small',\n",
       " 'rk',\n",
       " ']]\\n|',\n",
       " 'log',\n",
       " 'only_',\n",
       " 'ose',\n",
       " 'land_',\n",
       " '40',\n",
       " '6_',\n",
       " 'bra',\n",
       " 'ap',\n",
       " 'spa',\n",
       " 'II',\n",
       " 'ds',\n",
       " 'many_',\n",
       " 'cas',\n",
       " 'ste',\n",
       " '*[[',\n",
       " 'water',\n",
       " 'ics',\n",
       " 'nce_',\n",
       " ' \\n',\n",
       " 'ers_',\n",
       " 'Par',\n",
       " 'ack',\n",
       " 'ass',\n",
       " \"' \",\n",
       " 'net',\n",
       " 'film',\n",
       " 'ade',\n",
       " 'av',\n",
       " '23',\n",
       " 'bor',\n",
       " 'ian_',\n",
       " 'sen',\n",
       " '50',\n",
       " 'music',\n",
       " 'rm',\n",
       " 'would_',\n",
       " 'produc',\n",
       " 'eat',\n",
       " 'sing',\n",
       " 'ident',\n",
       " 'can',\n",
       " 'tro',\n",
       " 'some',\n",
       " 'ys',\n",
       " 'ai',\n",
       " 'West',\n",
       " 'her',\n",
       " '--',\n",
       " 'any_',\n",
       " 'mes',\n",
       " 'Pi',\n",
       " 'Bl',\n",
       " 'ms',\n",
       " 'gn',\n",
       " 'own',\n",
       " 'Am',\n",
       " 'ive_',\n",
       " 'Ru',\n",
       " 'Th',\n",
       " 'tel',\n",
       " 'ere',\n",
       " ']] - ',\n",
       " 'sin',\n",
       " 'I_',\n",
       " 'than_',\n",
       " 'port',\n",
       " 'ful',\n",
       " 'iz',\n",
       " 'Ph',\n",
       " 'age',\n",
       " 'even',\n",
       " 'dom',\n",
       " 'area_',\n",
       " 'lli',\n",
       " 'pos',\n",
       " 'Au',\n",
       " 'ence',\n",
       " 'We',\n",
       " 'i_',\n",
       " 'vis',\n",
       " 'mar',\n",
       " 'vers',\n",
       " '4_',\n",
       " 'dec',\n",
       " 'ita',\n",
       " 'dge',\n",
       " 'large',\n",
       " 'American_',\n",
       " 'lea',\n",
       " 'bli',\n",
       " '5_',\n",
       " ']]) ',\n",
       " 'sur',\n",
       " 'ach',\n",
       " 'when_',\n",
       " 'nic',\n",
       " 'set',\n",
       " 'Pri',\n",
       " 'after_',\n",
       " '8_',\n",
       " 'high',\n",
       " '. [[',\n",
       " 'Hu',\n",
       " 'zi',\n",
       " 'Wo',\n",
       " 'ds_',\n",
       " 'Sp',\n",
       " 'chan',\n",
       " 'htm',\n",
       " '26',\n",
       " 'cor',\n",
       " 'ship',\n",
       " 'tho',\n",
       " 'qua',\n",
       " '\\n* [[',\n",
       " '195',\n",
       " 'gl',\n",
       " '194',\n",
       " 'every_',\n",
       " 'Sta',\n",
       " 'cra',\n",
       " 'rt_',\n",
       " 'family_',\n",
       " \"]]'' (\",\n",
       " 'ld_',\n",
       " 'hn',\n",
       " ':\\n',\n",
       " 'col',\n",
       " 'contr',\n",
       " 'Fl',\n",
       " 'ous_',\n",
       " 'links',\n",
       " 'ance',\n",
       " 'ey',\n",
       " '>\\n<',\n",
       " 'reco',\n",
       " '45',\n",
       " 'At',\n",
       " 'gs',\n",
       " 'King',\n",
       " 'inc',\n",
       " ']]\\n*',\n",
       " ' &',\n",
       " 'Ke',\n",
       " 'left',\n",
       " 'term',\n",
       " 'mis',\n",
       " '7_',\n",
       " 'ar_',\n",
       " 'White',\n",
       " 'ons',\n",
       " 'ili',\n",
       " 'known_',\n",
       " 'xt',\n",
       " 'ves',\n",
       " 'Gu',\n",
       " '\\n[[',\n",
       " 'don',\n",
       " 'ction',\n",
       " 'sol',\n",
       " 'use_',\n",
       " 'States_',\n",
       " 'As',\n",
       " 'pol',\n",
       " '27',\n",
       " '28',\n",
       " 'exp',\n",
       " ';&#',\n",
       " 'system',\n",
       " 'ck_',\n",
       " 'German',\n",
       " 'ven',\n",
       " 'town_',\n",
       " 'mit',\n",
       " 'alt',\n",
       " 'pres',\n",
       " 'def',\n",
       " 'vil',\n",
       " 'les',\n",
       " '31',\n",
       " 'way',\n",
       " 'Ti',\n",
       " 'Cha',\n",
       " 'ure_',\n",
       " 'men',\n",
       " 'ny',\n",
       " 'count',\n",
       " 'mas',\n",
       " 'lla',\n",
       " 'jo',\n",
       " 'cle',\n",
       " 'uni',\n",
       " 'oi',\n",
       " 'elect',\n",
       " 'Min',\n",
       " 'lic',\n",
       " 'mer',\n",
       " 'Va',\n",
       " 'make',\n",
       " 'mor',\n",
       " 'ib',\n",
       " 'nte',\n",
       " 'pan',\n",
       " 'gro',\n",
       " 'ens',\n",
       " 'und',\n",
       " ' |',\n",
       " 'For',\n",
       " '). ',\n",
       " 'now',\n",
       " 'emp',\n",
       " 'External_',\n",
       " '\\n|-\\n|',\n",
       " '9_',\n",
       " 'thumb',\n",
       " 'nde',\n",
       " \"''\\n\",\n",
       " 'ally_',\n",
       " 'tte',\n",
       " 'Af',\n",
       " 'rd_',\n",
       " 'Ni',\n",
       " '}}',\n",
       " 'trans',\n",
       " 'writ',\n",
       " 'leg',\n",
       " 'dist',\n",
       " 'ua',\n",
       " '44',\n",
       " 'you',\n",
       " 'ect',\n",
       " 'bro',\n",
       " 'sh',\n",
       " 'female',\n",
       " 'rel',\n",
       " 'olog',\n",
       " '100_',\n",
       " '>[[',\n",
       " 'wor',\n",
       " 'ay',\n",
       " 'star',\n",
       " 'part',\n",
       " 'ward',\n",
       " '||',\n",
       " 'found',\n",
       " 'ling',\n",
       " ']]\\n\\n[[',\n",
       " 'ef',\n",
       " '|-',\n",
       " 'ug',\n",
       " 'Ap',\n",
       " '2005',\n",
       " 'square_',\n",
       " 'cons',\n",
       " 'located_',\n",
       " ';]]',\n",
       " '29',\n",
       " 'led_',\n",
       " 'les_',\n",
       " 'cal',\n",
       " 'Comm',\n",
       " 'may_',\n",
       " ')]]',\n",
       " 'ved_',\n",
       " 'die',\n",
       " 'nk',\n",
       " 'ks',\n",
       " 'dat',\n",
       " 'tur',\n",
       " 'inv',\n",
       " 'ext',\n",
       " 'size_',\n",
       " 'per_',\n",
       " 'Ac',\n",
       " 'ember_',\n",
       " 'ien',\n",
       " 'nu',\n",
       " 'over_',\n",
       " '64',\n",
       " 'Fo',\n",
       " 'about_',\n",
       " 'density',\n",
       " '2003',\n",
       " 'af',\n",
       " 'rou',\n",
       " 'ke_',\n",
       " 'back',\n",
       " ...]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword_text_encoder.subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–æ–≤–∏—è—Ç —Ä–∞–∑–º–µ—Ä –µ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "350030507"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(subword_text_encoder.encode(a)) for a in articles())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–µ–∫–∞ –∑–∞–ø–∏—à–µ–º —Ä–µ—á–Ω–∏–∫–∞ –≤—ä–≤ —Ñ–∞–π–ª:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword_text_encoder.save_to_file('vocab_2046')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏ –∏–∑–ø—ä–ª–Ω–µ–Ω–∏–µ –ø–æ-–Ω–∞–¥–æ–ª—É, —Ç–µ—Ç—Ä–∞–¥–∫–∞—Ç–∞ –∑–∞–±–∏. –î–æ–±—Ä–µ –µ, —á–µ –∏–º–∞–º–µ –∑–∞–ø–∏—Å–∞–Ω —Ä–µ—á–Ω–∏–∫–∞. –ü—Ä–æ–¥—ä–ª–∂–∞–≤–∞–º–µ –æ—Ç —Ç—É–∫, –∫–∞—Ç–æ –∏–Ω–¥–µ–∫—Å–∏—Ç–µ –Ω–∞ –∏–∑–ø—ä–ª–Ω–µ–Ω–∏–µ—Ç —Ä–µ–¥–æ–≤–µ —â–µ —Å–∞ –º–∞–ª–∫–æ –æ–±—ä—Ä–∫–∞–Ω–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword_text_encoder = tfds.features.text.SubwordTextEncoder.load_from_file('vocab_2046')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞ –≤–∏–¥–∏–º –∫–∞–∫ –±–∏ –∏–∑–≥–ª–µ–∂–¥–∞–ª–æ –æ–±—É—á–µ–Ω–∏–µ —Å –∫–æ–¥–∏—Ä–∞–Ω–∏—Ç–µ —Å—Ç–∞—Ç–∏–∏..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((64, None), (64, None)), types: (tf.uint16, tf.uint16)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BATCHED_ITEM_LENGTH = 128\n",
    "BUFFER_SIZE = 256\n",
    "TYPE=np.uint16\n",
    "\n",
    "def articles_generator():\n",
    "    for index, article in enumerate(itertools.islice(articles(), 0, 2000)):\n",
    "        yield np.array(subword_text_encoder.encode(article + b'\\0'), dtype=TYPE)\n",
    "\n",
    "    # Pad the article count to the batch size\n",
    "    # We do this to ensure that no data is dropped\n",
    "    index += 1\n",
    "    while index % BATCH_SIZE != 0:\n",
    "        yield np.array([0], dtype=TYPE)\n",
    "        index += 1\n",
    "\n",
    "def subbatches():\n",
    "    dataset = tf.data.Dataset.from_generator(articles_generator, output_types=TYPE)\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "    dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=([None]), drop_remainder=True)\n",
    "\n",
    "    for batch in dataset.as_numpy_iterator():\n",
    "        remaining = batch\n",
    "        while remaining.shape[1] > 1:\n",
    "            yield remaining[:, :BATCHED_ITEM_LENGTH]\n",
    "            remaining = remaining[:, BATCHED_ITEM_LENGTH-1:]\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(subbatches, output_types=TYPE, output_shapes=(BATCH_SIZE, None))\n",
    "dataset = dataset.map(lambda batch: (batch[:, :-1], batch[:, 1:]))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "def average_batch_length(true_labels, predictions):\n",
    "    return tf.shape(true_labels)[1]\n",
    "\n",
    "model = build_model(vocab_size = subword_text_encoder.vocab_size, embedding_dim=256, rnn_units=1024, batch_size=BATCH_SIZE)\n",
    "model.compile(optimizer='adam', loss=loss, metrics=[average_batch_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints' # Directory where the checkpoints will be saved\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\") # Name of the checkpoint files\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelStateResetter(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.last_total_length = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        average_batch_length = logs.get('average_batch_length', 0)\n",
    "        total_length = int(round(average_batch_length * (batch + 1)))\n",
    "        current_batch_length = total_length - self.last_total_length\n",
    "        self.last_total_length = total_length\n",
    "        \n",
    "        if current_batch_length < BATCHED_ITEM_LENGTH - 1:\n",
    "            self.model.reset_states()\n",
    "        \n",
    "model_state_resetter_callback = ModelStateResetter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "5206/5206 [==============================] - 416s 80ms/step - loss: 0.6492 - average_batch_length: 126.5745\n",
      "Epoch 2/12\n",
      "5175/5175 [==============================] - 417s 81ms/step - loss: 0.4488 - average_batch_length: 126.6187\n",
      "Epoch 3/12\n",
      "5216/5216 [==============================] - 423s 81ms/step - loss: 0.3978 - average_batch_length: 126.5909\n",
      "Epoch 4/12\n",
      "5279/5279 [==============================] - 440s 83ms/step - loss: 0.3670 - average_batch_length: 126.5431\n",
      "Epoch 5/12\n",
      "5431/5431 [==============================] - 454s 84ms/step - loss: 0.3390 - average_batch_length: 126.5853\n",
      "Epoch 6/12\n",
      "5311/5311 [==============================] - 434s 82ms/step - loss: 0.3330 - average_batch_length: 126.5718\n",
      "Epoch 7/12\n",
      "5164/5164 [==============================] - 416s 81ms/step - loss: 0.3316 - average_batch_length: 126.5833\n",
      "Epoch 8/12\n",
      "5400/5400 [==============================] - 435s 81ms/step - loss: 0.3087 - average_batch_length: 126.6080\n",
      "Epoch 9/12\n",
      "5344/5344 [==============================] - 429s 80ms/step - loss: 0.3054 - average_batch_length: 126.6287\n",
      "Epoch 10/12\n",
      "5289/5289 [==============================] - 426s 81ms/step - loss: 0.3027 - average_batch_length: 126.6156\n",
      "Epoch 11/12\n",
      "5082/5082 [==============================] - 411s 81ms/step - loss: 0.3102 - average_batch_length: 126.5815\n",
      "Epoch 12/12\n",
      "4920/4920 [==============================] - 396s 81ms/step - loss: 0.3159 - average_batch_length: 126.6079\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 12\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    print('Epoch %d/%d' % (epoch + 1, total_epochs))\n",
    "    model.fit(dataset, callbacks=[checkpoint_callback, model_state_resetter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: 25541\n",
      "Encded: 9379\n"
     ]
    }
   ],
   "source": [
    "with open('page_revisions_text', 'rb') as text_file:\n",
    "    data = text_file.read()\n",
    "\n",
    "article = data.split(b'\\0')[120]\n",
    "del data\n",
    "\n",
    "encoded_article = np.array(subword_text_encoder.encode(article + b'\\0'), dtype=TYPE)\n",
    "\n",
    "print('Raw:', len(article))\n",
    "print('Encded:', len(encoded_article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huffman\n",
    "\n",
    "def huffman_archive_size(model, text):\n",
    "    archived_size = 0\n",
    "    ones = 0\n",
    "    input_eval = np.array([[0]], dtype=TYPE)\n",
    "  \n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    model.reset_states()\n",
    "\n",
    "    for index, byte in enumerate(text):\n",
    "        if index % 50 == 0:\n",
    "            print('%d/%d' % (index, len(text)))\n",
    "\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "  \n",
    "        probabilities = tf.nn.softmax(predictions[0])\n",
    "        codebook = huffman.codebook([index, tensor.numpy()] for index, tensor in enumerate(probabilities))\n",
    "\n",
    "        code = codebook[byte]\n",
    "        ones += code.count('1')\n",
    "        archived_size += len(code)\n",
    "\n",
    "        input_eval = tf.expand_dims([byte], 0)\n",
    "  \n",
    "    return ones, archived_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocab_size = subword_text_encoder.vocab_size, embedding_dim=256, rnn_units=1024, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/9379\n",
      "50/9379\n",
      "100/9379\n",
      "150/9379\n",
      "200/9379\n",
      "250/9379\n",
      "300/9379\n",
      "350/9379\n",
      "400/9379\n",
      "450/9379\n",
      "500/9379\n",
      "550/9379\n",
      "600/9379\n",
      "650/9379\n",
      "700/9379\n",
      "750/9379\n",
      "800/9379\n",
      "850/9379\n",
      "900/9379\n",
      "950/9379\n",
      "1000/9379\n",
      "1050/9379\n",
      "1100/9379\n",
      "1150/9379\n",
      "1200/9379\n",
      "1250/9379\n",
      "1300/9379\n",
      "1350/9379\n",
      "1400/9379\n",
      "1450/9379\n",
      "1500/9379\n",
      "1550/9379\n",
      "1600/9379\n",
      "1650/9379\n",
      "1700/9379\n",
      "1750/9379\n",
      "1800/9379\n",
      "1850/9379\n",
      "1900/9379\n",
      "1950/9379\n",
      "2000/9379\n",
      "2050/9379\n",
      "2100/9379\n",
      "2150/9379\n",
      "2200/9379\n",
      "2250/9379\n",
      "2300/9379\n",
      "2350/9379\n",
      "2400/9379\n",
      "2450/9379\n",
      "2500/9379\n",
      "2550/9379\n",
      "2600/9379\n",
      "2650/9379\n",
      "2700/9379\n",
      "2750/9379\n",
      "2800/9379\n",
      "2850/9379\n",
      "2900/9379\n",
      "2950/9379\n",
      "3000/9379\n",
      "3050/9379\n",
      "3100/9379\n",
      "3150/9379\n",
      "3200/9379\n",
      "3250/9379\n",
      "3300/9379\n",
      "3350/9379\n",
      "3400/9379\n",
      "3450/9379\n",
      "3500/9379\n",
      "3550/9379\n",
      "3600/9379\n",
      "3650/9379\n",
      "3700/9379\n",
      "3750/9379\n",
      "3800/9379\n",
      "3850/9379\n",
      "3900/9379\n",
      "3950/9379\n",
      "4000/9379\n",
      "4050/9379\n",
      "4100/9379\n",
      "4150/9379\n",
      "4200/9379\n",
      "4250/9379\n",
      "4300/9379\n",
      "4350/9379\n",
      "4400/9379\n",
      "4450/9379\n",
      "4500/9379\n",
      "4550/9379\n",
      "4600/9379\n",
      "4650/9379\n",
      "4700/9379\n",
      "4750/9379\n",
      "4800/9379\n",
      "4850/9379\n",
      "4900/9379\n",
      "4950/9379\n",
      "5000/9379\n",
      "5050/9379\n",
      "5100/9379\n",
      "5150/9379\n",
      "5200/9379\n",
      "5250/9379\n",
      "5300/9379\n",
      "5350/9379\n",
      "5400/9379\n",
      "5450/9379\n",
      "5500/9379\n",
      "5550/9379\n",
      "5600/9379\n",
      "5650/9379\n",
      "5700/9379\n",
      "5750/9379\n",
      "5800/9379\n",
      "5850/9379\n",
      "5900/9379\n",
      "5950/9379\n",
      "6000/9379\n",
      "6050/9379\n",
      "6100/9379\n",
      "6150/9379\n",
      "6200/9379\n",
      "6250/9379\n",
      "6300/9379\n",
      "6350/9379\n",
      "6400/9379\n",
      "6450/9379\n",
      "6500/9379\n",
      "6550/9379\n",
      "6600/9379\n",
      "6650/9379\n",
      "6700/9379\n",
      "6750/9379\n",
      "6800/9379\n",
      "6850/9379\n",
      "6900/9379\n",
      "6950/9379\n",
      "7000/9379\n",
      "7050/9379\n",
      "7100/9379\n",
      "7150/9379\n",
      "7200/9379\n",
      "7250/9379\n",
      "7300/9379\n",
      "7350/9379\n",
      "7400/9379\n",
      "7450/9379\n",
      "7500/9379\n",
      "7550/9379\n",
      "7600/9379\n",
      "7650/9379\n",
      "7700/9379\n",
      "7750/9379\n",
      "7800/9379\n",
      "7850/9379\n",
      "7900/9379\n",
      "7950/9379\n",
      "8000/9379\n",
      "8050/9379\n",
      "8100/9379\n",
      "8150/9379\n",
      "8200/9379\n",
      "8250/9379\n",
      "8300/9379\n",
      "8350/9379\n",
      "8400/9379\n",
      "8450/9379\n",
      "8500/9379\n",
      "8550/9379\n",
      "8600/9379\n",
      "8650/9379\n",
      "8700/9379\n",
      "8750/9379\n",
      "8800/9379\n",
      "8850/9379\n",
      "8900/9379\n",
      "8950/9379\n",
      "9000/9379\n",
      "9050/9379\n",
      "9100/9379\n",
      "9150/9379\n",
      "9200/9379\n",
      "9250/9379\n",
      "9300/9379\n",
      "9350/9379\n",
      "\n",
      "Compressed length: 36972\n",
      "Compression ratio for encoded: 0.4927497601023563\n",
      "Compression ratio for raw: 0.18094436396382288\n",
      "Potential compression ratio with arithmetic coding: 0.17942582493059045\n"
     ]
    }
   ],
   "source": [
    "ones, archived_size = huffman_archive_size(model, encoded_article)\n",
    "print('\\nCompressed length:', archived_size)\n",
    "\n",
    "compression_ratio = archived_size / (len(encoded_article) * 8)\n",
    "print('Compression ratio for encoded:', compression_ratio)\n",
    "\n",
    "compression_ratio = archived_size / (len(article) * 8)\n",
    "print('Compression ratio for raw:', compression_ratio)\n",
    "\n",
    "k = (ones / archived_size)\n",
    "compression_ratio = compression_ratio * (-k * np.log2(k) - (1-k) * np.log2(1-k))\n",
    "print('Potential compression ratio with arithmetic coding:', compression_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–æ–≤–∞ —Å–∞ –Ω–∞–π-–¥–æ–±—Ä–∏—Ç–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏ –¥–æ –º–æ–º–µ–Ω—Ç–∞. –í—Ä–æ—è—Ç–Ω–æ —Å –ø–æ-–≥–æ–ª—è–º —Ä–µ—á–Ω–∏–∫ —â–µ –ø–æ—Å—Ç–∏–≥–Ω–µ–º –∏ –ø–æ–≤–µ—á–µ. –ù–æ –Ω–µ–∫–∞ –ø—Ä–µ–¥–∏ —Ç–æ–≤–∞ –¥–∞ –≤–∏–¥–∏–º, –¥–∞–ª–∏ –º–æ–∂–µ–º –¥–∞ –Ω–∞—Ç—Ä–µ–Ω–∏—Ä–∞–º–µ –Ω–µ–≤—Ä–æ–Ω–Ω–∞—Ç–∞ –º—Ä–µ–∂–∞ –º–∞–ª–∫–æ –ø–æ–≤–µ—á–µ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size = subword_text_encoder.vocab_size, embedding_dim=256, rnn_units=1024, batch_size=BATCH_SIZE)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.compile(optimizer='adam', loss=loss, metrics=[average_batch_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "5267/5267 [==============================] - 426s 81ms/step - loss: 0.2922 - average_batch_length: 126.5667\n",
      "Epoch 2/5\n",
      "5199/5199 [==============================] - 426s 82ms/step - loss: 0.2953 - average_batch_length: 126.5749\n",
      "Epoch 3/5\n",
      "5329/5329 [==============================] - 437s 82ms/step - loss: 0.2849 - average_batch_length: 126.6281\n",
      "Epoch 4/5\n",
      "5226/5226 [==============================] - 416s 80ms/step - loss: 0.2908 - average_batch_length: 126.6112\n",
      "Epoch 5/5\n",
      "5321/5321 [==============================] - 433s 81ms/step - loss: 0.2846 - average_batch_length: 126.5982\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 5\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    print('Epoch %d/%d' % (epoch + 1, total_epochs))\n",
    "    model.fit(dataset, callbacks=[checkpoint_callback, model_state_resetter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocab_size = subword_text_encoder.vocab_size, embedding_dim=256, rnn_units=1024, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/9379\n",
      "50/9379\n",
      "100/9379\n",
      "150/9379\n",
      "200/9379\n",
      "250/9379\n",
      "300/9379\n",
      "350/9379\n",
      "400/9379\n",
      "450/9379\n",
      "500/9379\n",
      "550/9379\n",
      "600/9379\n",
      "650/9379\n",
      "700/9379\n",
      "750/9379\n",
      "800/9379\n",
      "850/9379\n",
      "900/9379\n",
      "950/9379\n",
      "1000/9379\n",
      "1050/9379\n",
      "1100/9379\n",
      "1150/9379\n",
      "1200/9379\n",
      "1250/9379\n",
      "1300/9379\n",
      "1350/9379\n",
      "1400/9379\n",
      "1450/9379\n",
      "1500/9379\n",
      "1550/9379\n",
      "1600/9379\n",
      "1650/9379\n",
      "1700/9379\n",
      "1750/9379\n",
      "1800/9379\n",
      "1850/9379\n",
      "1900/9379\n",
      "1950/9379\n",
      "2000/9379\n",
      "2050/9379\n",
      "2100/9379\n",
      "2150/9379\n",
      "2200/9379\n",
      "2250/9379\n",
      "2300/9379\n",
      "2350/9379\n",
      "2400/9379\n",
      "2450/9379\n",
      "2500/9379\n",
      "2550/9379\n",
      "2600/9379\n",
      "2650/9379\n",
      "2700/9379\n",
      "2750/9379\n",
      "2800/9379\n",
      "2850/9379\n",
      "2900/9379\n",
      "2950/9379\n",
      "3000/9379\n",
      "3050/9379\n",
      "3100/9379\n",
      "3150/9379\n",
      "3200/9379\n",
      "3250/9379\n",
      "3300/9379\n",
      "3350/9379\n",
      "3400/9379\n",
      "3450/9379\n",
      "3500/9379\n",
      "3550/9379\n",
      "3600/9379\n",
      "3650/9379\n",
      "3700/9379\n",
      "3750/9379\n",
      "3800/9379\n",
      "3850/9379\n",
      "3900/9379\n",
      "3950/9379\n",
      "4000/9379\n",
      "4050/9379\n",
      "4100/9379\n",
      "4150/9379\n",
      "4200/9379\n",
      "4250/9379\n",
      "4300/9379\n",
      "4350/9379\n",
      "4400/9379\n",
      "4450/9379\n",
      "4500/9379\n",
      "4550/9379\n",
      "4600/9379\n",
      "4650/9379\n",
      "4700/9379\n",
      "4750/9379\n",
      "4800/9379\n",
      "4850/9379\n",
      "4900/9379\n",
      "4950/9379\n",
      "5000/9379\n",
      "5050/9379\n",
      "5100/9379\n",
      "5150/9379\n",
      "5200/9379\n",
      "5250/9379\n",
      "5300/9379\n",
      "5350/9379\n",
      "5400/9379\n",
      "5450/9379\n",
      "5500/9379\n",
      "5550/9379\n",
      "5600/9379\n",
      "5650/9379\n",
      "5700/9379\n",
      "5750/9379\n",
      "5800/9379\n",
      "5850/9379\n",
      "5900/9379\n",
      "5950/9379\n",
      "6000/9379\n",
      "6050/9379\n",
      "6100/9379\n",
      "6150/9379\n",
      "6200/9379\n",
      "6250/9379\n",
      "6300/9379\n",
      "6350/9379\n",
      "6400/9379\n",
      "6450/9379\n",
      "6500/9379\n",
      "6550/9379\n",
      "6600/9379\n",
      "6650/9379\n",
      "6700/9379\n",
      "6750/9379\n",
      "6800/9379\n",
      "6850/9379\n",
      "6900/9379\n",
      "6950/9379\n",
      "7000/9379\n",
      "7050/9379\n",
      "7100/9379\n",
      "7150/9379\n",
      "7200/9379\n",
      "7250/9379\n",
      "7300/9379\n",
      "7350/9379\n",
      "7400/9379\n",
      "7450/9379\n",
      "7500/9379\n",
      "7550/9379\n",
      "7600/9379\n",
      "7650/9379\n",
      "7700/9379\n",
      "7750/9379\n",
      "7800/9379\n",
      "7850/9379\n",
      "7900/9379\n",
      "7950/9379\n",
      "8000/9379\n",
      "8050/9379\n",
      "8100/9379\n",
      "8150/9379\n",
      "8200/9379\n",
      "8250/9379\n",
      "8300/9379\n",
      "8350/9379\n",
      "8400/9379\n",
      "8450/9379\n",
      "8500/9379\n",
      "8550/9379\n",
      "8600/9379\n",
      "8650/9379\n",
      "8700/9379\n",
      "8750/9379\n",
      "8800/9379\n",
      "8850/9379\n",
      "8900/9379\n",
      "8950/9379\n",
      "9000/9379\n",
      "9050/9379\n",
      "9100/9379\n",
      "9150/9379\n",
      "9200/9379\n",
      "9250/9379\n",
      "9300/9379\n",
      "9350/9379\n",
      "\n",
      "Compressed length: 35729\n",
      "Compression ratio for encoded: 0.4761834950421154\n",
      "Compression ratio for raw: 0.17486100779139424\n",
      "Potential compression ratio with arithmetic coding: 0.17294407775223483\n"
     ]
    }
   ],
   "source": [
    "ones, archived_size = huffman_archive_size(model, encoded_article)\n",
    "print('\\nCompressed length:', archived_size)\n",
    "\n",
    "compression_ratio = archived_size / (len(encoded_article) * 8)\n",
    "print('Compression ratio for encoded:', compression_ratio)\n",
    "\n",
    "compression_ratio = archived_size / (len(article) * 8)\n",
    "print('Compression ratio for raw:', compression_ratio)\n",
    "\n",
    "k = (ones / archived_size)\n",
    "compression_ratio = compression_ratio * (-k * np.log2(k) - (1-k) * np.log2(1-k))\n",
    "print('Potential compression ratio with arithmetic coding:', compression_ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
