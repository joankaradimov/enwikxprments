{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nummber of articles: 243427\n",
      "Nummber of bytes in articles: 887891160\n"
     ]
    }
   ],
   "source": [
    "with open('page_revisions_text', 'rb') as text_file:\n",
    "    data = text_file.read()\n",
    "    \n",
    "articles = data.split(b'\\0')\n",
    "del data\n",
    "\n",
    "print('Nummber of articles:', len(articles))\n",
    "print('Nummber of bytes in articles:', sum(len(a) for a in articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique bytes in the file\n",
    "vocab = sorted(set(b''.join(articles)))\n",
    "print ('{} unique characters'.format(len(vocab)))\n",
    "del vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.frombuffer(b'. '.join(articles[:2000]), dtype=np.uint8)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 500\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 500), (64, 500)), types: (tf.uint8, tf.uint8)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = 256\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = 256,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 438 steps\n",
      "Epoch 1/30\n",
      "438/438 [==============================] - 91s 208ms/step - loss: 2.4028\n",
      "Epoch 2/30\n",
      "438/438 [==============================] - 88s 202ms/step - loss: 1.6178\n",
      "Epoch 3/30\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 1.4318\n",
      "Epoch 4/30\n",
      "438/438 [==============================] - 89s 204ms/step - loss: 1.3498\n",
      "Epoch 5/30\n",
      "438/438 [==============================] - 89s 204ms/step - loss: 1.2984\n",
      "Epoch 6/30\n",
      "438/438 [==============================] - 89s 204ms/step - loss: 1.2612\n",
      "Epoch 7/30\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 1.2329\n",
      "Epoch 8/30\n",
      "438/438 [==============================] - 89s 204ms/step - loss: 1.2095\n",
      "Epoch 9/30\n",
      "438/438 [==============================] - 89s 204ms/step - loss: 1.1902\n",
      "Epoch 10/30\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 1.1737\n",
      "Epoch 11/30\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 1.1596\n",
      "Epoch 12/30\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 1.1469\n",
      "Epoch 13/30\n",
      "438/438 [==============================] - 89s 204ms/step - loss: 1.1363\n",
      "Epoch 14/30\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 1.1267\n",
      "Epoch 15/30\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 1.1188\n",
      "Epoch 16/30\n",
      "438/438 [==============================] - 100s 228ms/step - loss: 1.1116\n",
      "Epoch 17/30\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 1.1054\n",
      "Epoch 18/30\n",
      "438/438 [==============================] - 89s 204ms/step - loss: 1.0999\n",
      "Epoch 19/30\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 1.0951\n",
      "Epoch 20/30\n",
      "438/438 [==============================] - 86s 197ms/step - loss: 1.0910\n",
      "Epoch 21/30\n",
      "438/438 [==============================] - 86s 197ms/step - loss: 1.0872\n",
      "Epoch 22/30\n",
      "438/438 [==============================] - 87s 198ms/step - loss: 1.0849\n",
      "Epoch 23/30\n",
      "438/438 [==============================] - 87s 198ms/step - loss: 1.0823\n",
      "Epoch 24/30\n",
      "438/438 [==============================] - 87s 198ms/step - loss: 1.0799\n",
      "Epoch 25/30\n",
      "438/438 [==============================] - 87s 198ms/step - loss: 1.0783\n",
      "Epoch 26/30\n",
      "438/438 [==============================] - 86s 197ms/step - loss: 1.0768\n",
      "Epoch 27/30\n",
      "438/438 [==============================] - 87s 198ms/step - loss: 1.0755\n",
      "Epoch 28/30\n",
      "438/438 [==============================] - 87s 198ms/step - loss: 1.0802\n",
      "Epoch 29/30\n",
      "438/438 [==============================] - 86s 197ms/step - loss: 1.0793\n",
      "Epoch 30/30\n",
      "438/438 [==============================] - 87s 198ms/step - loss: 1.1596\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=30, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 438 steps\n",
      "Epoch 1/10\n",
      "438/438 [==============================] - 88s 200ms/step - loss: 1.1072\n",
      "Epoch 2/10\n",
      "438/438 [==============================] - 89s 202ms/step - loss: 1.0914\n",
      "Epoch 3/10\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 1.0802\n",
      "Epoch 4/10\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 1.0828\n",
      "Epoch 5/10\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 1.3470\n",
      "Epoch 6/10\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 1.4616\n",
      "Epoch 7/10\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 1.4482\n",
      "Epoch 8/10\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 1.3859\n",
      "Epoch 9/10\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 1.3255\n",
      "Epoch 10/10\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 1.2870\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 438 steps\n",
      "Epoch 1/10\n",
      "438/438 [==============================] - 88s 202ms/step - loss: 1.2668\n",
      "Epoch 2/10\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 2.0387\n",
      "Epoch 3/10\n",
      "438/438 [==============================] - 89s 204ms/step - loss: 2.1147\n",
      "Epoch 4/10\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 2.04690s - loss: 2.046\n",
      "Epoch 5/10\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 2.0339\n",
      "Epoch 6/10\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 2.0237\n",
      "Epoch 7/10\n",
      "438/438 [==============================] - 89s 204ms/step - loss: 2.0180\n",
      "Epoch 8/10\n",
      "438/438 [==============================] - 89s 203ms/step - loss: 2.0147\n",
      "Epoch 9/10\n",
      "438/438 [==============================] - 89s 204ms/step - loss: 2.0117\n",
      "Epoch 10/10\n",
      "438/438 [==============================] - 89s 204ms/step - loss: 2.0076\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt_1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            65536     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 256)            262400    \n",
      "=================================================================\n",
      "Total params: 4,266,240\n",
      "Trainable params: 4,266,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total length: 74824\n"
     ]
    }
   ],
   "source": [
    "import huffman\n",
    "\n",
    "def huffman_archive_size(model, text):\n",
    "    archived_size = 0\n",
    "    ones = 0\n",
    "    input_eval = [s for s in b' ']\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "  \n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    model.reset_states()\n",
    "\n",
    "    for byte in text:\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "  \n",
    "        probabilities = tf.nn.softmax(predictions[0])\n",
    "        codebook = huffman.codebook([index, tensor.numpy()] for index, tensor in enumerate(probabilities))\n",
    "\n",
    "        code = codebook[byte]\n",
    "        ones += code.count('1')\n",
    "        archived_size += len(code)\n",
    "\n",
    "        # using a categorical distribution to predict the byte returned by the model\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([byte], 0)\n",
    "  \n",
    "    return ones, archived_size\n",
    "\n",
    "article = articles[120]\n",
    "ones, archived_size = huffman_archive_size(model, article)\n",
    "print('\\nTotal length:', archived_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36619552875768374"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_ratio = archived_size / (len(article) * 8)\n",
    "compression_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3613850212453604"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = (ones / archived_size)\n",
    "compression_ratio * (-k * np.log2(k) - (1-k) * np.log2(1-k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Да опитаме с LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = 256,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 438 steps\n",
      "Epoch 1/30\n",
      "438/438 [==============================] - 110s 251ms/step - loss: 2.6043\n",
      "Epoch 2/30\n",
      "438/438 [==============================] - 110s 251ms/step - loss: 1.9103\n",
      "Epoch 3/30\n",
      "438/438 [==============================] - 111s 252ms/step - loss: 1.6561\n",
      "Epoch 4/30\n",
      "438/438 [==============================] - 110s 252ms/step - loss: 1.5245\n",
      "Epoch 5/30\n",
      "438/438 [==============================] - 110s 252ms/step - loss: 1.4461\n",
      "Epoch 6/30\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.3930\n",
      "Epoch 7/30\n",
      "438/438 [==============================] - 111s 252ms/step - loss: 1.3529\n",
      "Epoch 8/30\n",
      "438/438 [==============================] - 110s 252ms/step - loss: 1.3211\n",
      "Epoch 9/30\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.2948\n",
      "Epoch 10/30\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.2722\n",
      "Epoch 11/30\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.2520\n",
      "Epoch 12/30\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.2343\n",
      "Epoch 13/30\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.2185\n",
      "Epoch 14/30\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.2040\n",
      "Epoch 15/30\n",
      "438/438 [==============================] - 111s 252ms/step - loss: 1.1905\n",
      "Epoch 16/30\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.1786\n",
      "Epoch 17/30\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.1671\n",
      "Epoch 18/30\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.1575\n",
      "Epoch 19/30\n",
      "438/438 [==============================] - 108s 247ms/step - loss: 1.1469\n",
      "Epoch 20/30\n",
      "438/438 [==============================] - 107s 245ms/step - loss: 1.1375\n",
      "Epoch 21/30\n",
      "438/438 [==============================] - 107s 245ms/step - loss: 1.1288\n",
      "Epoch 22/30\n",
      "438/438 [==============================] - 107s 245ms/step - loss: 1.1208\n",
      "Epoch 23/30\n",
      "438/438 [==============================] - 108s 246ms/step - loss: 1.1132\n",
      "Epoch 24/30\n",
      "438/438 [==============================] - 108s 246ms/step - loss: 1.1054\n",
      "Epoch 25/30\n",
      "438/438 [==============================] - 107s 245ms/step - loss: 1.0984\n",
      "Epoch 26/30\n",
      "438/438 [==============================] - 108s 246ms/step - loss: 1.0922\n",
      "Epoch 27/30\n",
      "438/438 [==============================] - 107s 245ms/step - loss: 1.0855\n",
      "Epoch 28/30\n",
      "438/438 [==============================] - 110s 252ms/step - loss: 1.0795\n",
      "Epoch 29/30\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.0737\n",
      "Epoch 30/30\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.0685\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=30, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 438 steps\n",
      "Epoch 1/10\n",
      "438/438 [==============================] - 109s 250ms/step - loss: 1.0630\n",
      "Epoch 2/10\n",
      "438/438 [==============================] - 110s 252ms/step - loss: 1.0580\n",
      "Epoch 3/10\n",
      "438/438 [==============================] - 110s 252ms/step - loss: 1.0531\n",
      "Epoch 4/10\n",
      "438/438 [==============================] - 111s 252ms/step - loss: 1.0487\n",
      "Epoch 5/10\n",
      "438/438 [==============================] - 111s 252ms/step - loss: 1.0442\n",
      "Epoch 6/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.0396\n",
      "Epoch 7/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.0356\n",
      "Epoch 8/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.0320\n",
      "Epoch 9/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.0284\n",
      "Epoch 10/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.0246\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 438 steps\n",
      "Epoch 1/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.0222\n",
      "Epoch 2/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.0187\n",
      "Epoch 3/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.0157\n",
      "Epoch 4/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.0151\n",
      "Epoch 5/10\n",
      "438/438 [==============================] - 111s 254ms/step - loss: 1.0102\n",
      "Epoch 6/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.0063\n",
      "Epoch 7/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.0062\n",
      "Epoch 8/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.0011\n",
      "Epoch 9/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 0.9982\n",
      "Epoch 10/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 0.9961\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 438 steps\n",
      "Epoch 1/10\n",
      "438/438 [==============================] - 110s 252ms/step - loss: 0.9946\n",
      "Epoch 2/10\n",
      "438/438 [==============================] - 111s 252ms/step - loss: 0.9929\n",
      "Epoch 3/10\n",
      "438/438 [==============================] - 110s 252ms/step - loss: 0.9916\n",
      "Epoch 4/10\n",
      "438/438 [==============================] - 111s 252ms/step - loss: 0.9882\n",
      "Epoch 5/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 0.9864\n",
      "Epoch 6/10\n",
      "438/438 [==============================] - 111s 252ms/step - loss: 0.9848\n",
      "Epoch 7/10\n",
      "438/438 [==============================] - 111s 252ms/step - loss: 0.9854\n",
      "Epoch 8/10\n",
      "438/438 [==============================] - 111s 252ms/step - loss: 0.9826\n",
      "Epoch 9/10\n",
      "438/438 [==============================] - 110s 252ms/step - loss: 0.9981\n",
      "Epoch 10/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 0.9847\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 438 steps\n",
      "Epoch 1/10\n",
      "438/438 [==============================] - 110s 251ms/step - loss: 0.9796\n",
      "Epoch 2/10\n",
      "438/438 [==============================] - 110s 252ms/step - loss: 0.9772\n",
      "Epoch 3/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 0.9769\n",
      "Epoch 4/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 0.9751\n",
      "Epoch 5/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 0.9908\n",
      "Epoch 6/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 1.0036\n",
      "Epoch 7/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 0.9790\n",
      "Epoch 8/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 0.9757\n",
      "Epoch 9/10\n",
      "438/438 [==============================] - 111s 254ms/step - loss: 0.9733\n",
      "Epoch 10/10\n",
      "438/438 [==============================] - 107s 244ms/step - loss: 1.0122\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 438 steps\n",
      "Epoch 1/10\n",
      "438/438 [==============================] - 105s 239ms/step - loss: 0.9844\n",
      "Epoch 2/10\n",
      "438/438 [==============================] - 105s 239ms/step - loss: 0.9730\n",
      "Epoch 3/10\n",
      "438/438 [==============================] - 105s 241ms/step - loss: 0.9733\n",
      "Epoch 4/10\n",
      "438/438 [==============================] - 105s 240ms/step - loss: 1.1343\n",
      "Epoch 5/10\n",
      "438/438 [==============================] - 105s 239ms/step - loss: 2.2117\n",
      "Epoch 6/10\n",
      "438/438 [==============================] - 105s 240ms/step - loss: 1.9382\n",
      "Epoch 7/10\n",
      "438/438 [==============================] - 105s 239ms/step - loss: 1.8546\n",
      "Epoch 8/10\n",
      "438/438 [==============================] - 105s 240ms/step - loss: 1.8012\n",
      "Epoch 9/10\n",
      "438/438 [==============================] - 106s 242ms/step - loss: 1.7596\n",
      "Epoch 10/10\n",
      "438/438 [==============================] - 105s 240ms/step - loss: 1.7240\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да опитраме с различни оптимизатори"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 438 steps\n",
      "438/438 [==============================] - 112s 255ms/step - loss: 2351.2793\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "  vocab_size = 256,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.keras import backend_config\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.ops import nn\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import variables as variables_module\n",
    "\n",
    "epsilon = backend_config.epsilon\n",
    "\n",
    "def get_graph():\n",
    "    if context.executing_eagerly():\n",
    "        global _GRAPH\n",
    "        if _GRAPH is None:\n",
    "            _GRAPH = func_graph.FuncGraph('keras_graph')\n",
    "        return _GRAPH\n",
    "    else:\n",
    "        return ops.get_default_graph()\n",
    "\n",
    "def flatten(x):\n",
    "    return array_ops.reshape(x, [-1])\n",
    "\n",
    "def cast(x, dtype):\n",
    "    return math_ops.cast(x, dtype)\n",
    "  \n",
    "def _is_symbolic_tensor(x):\n",
    "    return tensor_util.is_tensor(x) and not isinstance(x, ops.EagerTensor)\n",
    "\n",
    "# This is based around the `sparse_categorical_crossentropy` implementation in Keras:\n",
    "# https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/backend.py#L4507-L4582\n",
    "def loss(target, output, from_logits=False, axis=-1):\n",
    "    if not from_logits:\n",
    "        if (isinstance(output, (ops.EagerTensor, variables_module.Variable)) or output.op.type != 'Softmax'):\n",
    "            epsilon_ = constant_op.constant(epsilon(), dtype=output.dtype.base_dtype)\n",
    "            output = clip_ops.clip_by_value(output, epsilon_, 1 - epsilon_)\n",
    "            output = math_ops.log(output)\n",
    "        else:\n",
    "            # When softmax activation function is used for output operation, we\n",
    "            # use logits from the softmax function directly to compute loss in order\n",
    "            # to prevent collapsing zero when training.\n",
    "            # See b/117284466\n",
    "            assert len(output.op.inputs) == 1\n",
    "            output = output.op.inputs[0]\n",
    "  \n",
    "    if isinstance(output.shape, (tuple, list)):\n",
    "        output_rank = len(output.shape)\n",
    "    else:\n",
    "        output_rank = output.shape.ndims\n",
    "\n",
    "    if output_rank is not None:\n",
    "        axis %= output_rank\n",
    "        if axis != output_rank - 1:\n",
    "            permutation = list(itertools.chain(range(axis), range(axis + 1, output_rank), [axis]))\n",
    "            output = array_ops.transpose(output, perm=permutation)\n",
    "    elif axis != -1:\n",
    "        raise ValueError(\n",
    "            'Cannot compute sparse categorical crossentropy with `axis={}` on an '\n",
    "            'output tensor with unknown rank'.format(axis))\n",
    "  \n",
    "    target = cast(target, 'int64')\n",
    "  \n",
    "    # Try to adjust the shape so that rank of labels = rank of logits - 1.\n",
    "    output_shape = array_ops.shape_v2(output)\n",
    "    target_rank = target.shape.ndims\n",
    "  \n",
    "    update_shape = (target_rank is not None and output_rank is not None and target_rank != output_rank - 1)\n",
    "    if update_shape:\n",
    "        target = flatten(target)\n",
    "        output = array_ops.reshape(output, [-1, output_shape[-1]])\n",
    "  \n",
    "    if __builtins__.any([_is_symbolic_tensor(v) for v in [target, output]]):\n",
    "        with get_graph().as_default():\n",
    "            res = huffman_code_lengths(labels=target, logits=output)\n",
    "    else:\n",
    "        res = huffman_code_lengths(labels=target, logits=output)\n",
    "  \n",
    "    if update_shape and output_rank >= 3:\n",
    "        # If our output includes timesteps or spatial dimensions we need to reshape\n",
    "        return array_ops.reshape(res, output_shape[:-1])\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "def huffman_code_lengths(labels, logits):\n",
    "    category_count = logits.shape[-1] or 0\n",
    "    return tf.reduce_sum(-tf.math.log(tf.one_hot(labels, depth=category_count) * tf.nn.softmax(logits) + 0.0001), axis=-1)\n",
    "    \n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "history = model.fit(dataset, epochs=1, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 438 steps\n",
      "Epoch 1/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 2350.9236\n",
      "Epoch 2/10\n",
      "438/438 [==============================] - 111s 253ms/step - loss: 2350.7481\n",
      "Epoch 3/10\n",
      "438/438 [==============================] - 113s 258ms/step - loss: 2350.6212\n",
      "Epoch 4/10\n",
      "438/438 [==============================] - 113s 258ms/step - loss: 2350.5294\n",
      "Epoch 5/10\n",
      "438/438 [==============================] - 114s 260ms/step - loss: 2350.4818\n",
      "Epoch 6/10\n",
      "438/438 [==============================] - 113s 258ms/step - loss: 2350.4627\n",
      "Epoch 7/10\n",
      "438/438 [==============================] - 112s 256ms/step - loss: 2350.4120\n",
      "Epoch 8/10\n",
      "438/438 [==============================] - 112s 256ms/step - loss: 2350.4118\n",
      "Epoch 9/10\n",
      "438/438 [==============================] - 113s 258ms/step - loss: 2350.3612\n",
      "Epoch 10/10\n",
      "438/438 [==============================] - 113s 259ms/step - loss: 2350.3427\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 3015\n",
      "Compression ratio: 0.75375\n",
      "Potential compression ratio with arithmetic coding: 0.75375\n"
     ]
    }
   ],
   "source": [
    "article = articles[120][:500]\n",
    "ones, archived_size = huffman_archive_size(model, article)\n",
    "print('Total length:', archived_size)\n",
    "\n",
    "compression_ratio = archived_size / (len(article) * 8)\n",
    "print('Compression ratio:', compression_ratio)\n",
    "\n",
    "k = (ones / archived_size)\n",
    "compression_ratio * (-k * np.log2(k) - (1-k) * np.log2(1-k))\n",
    "print('Potential compression ratio with arithmetic coding:', compression_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 438 steps\n",
      "Epoch 1/10\n",
      "438/438 [==============================] - 115s 262ms/step - loss: 1.1215e-05\n",
      "Epoch 2/10\n",
      "438/438 [==============================] - 115s 263ms/step - loss: 4.7817e-09\n",
      "Epoch 3/10\n",
      "438/438 [==============================] - 114s 259ms/step - loss: 4.2009e-09\n",
      "Epoch 4/10\n",
      "438/438 [==============================] - 113s 258ms/step - loss: 3.4190e-09\n",
      "Epoch 5/10\n",
      "438/438 [==============================] - 112s 257ms/step - loss: 3.1524e-09\n",
      "Epoch 6/10\n",
      "438/438 [==============================] - 112s 256ms/step - loss: 2.7139e-09\n",
      "Epoch 7/10\n",
      "438/438 [==============================] - 114s 260ms/step - loss: 2.6236e-09\n",
      "Epoch 8/10\n",
      "438/438 [==============================] - 114s 260ms/step - loss: 2.4980e-09\n",
      "Epoch 9/10\n",
      "438/438 [==============================] - 112s 256ms/step - loss: 2.4849e-09\n",
      "Epoch 10/10\n",
      "438/438 [==============================] - 111s 254ms/step - loss: 2.5084e-09\n"
     ]
    }
   ],
   "source": [
    "epsilon = backend_config.epsilon\n",
    "\n",
    "def huffman_code_lengths(labels, logits):\n",
    "    category_count = logits.shape[-1] or 0\n",
    "    return tf.reduce_sum(tf.one_hot(labels, depth=category_count) * tf.nn.softmax(logits), axis=-1)\n",
    "\n",
    "model = build_model(\n",
    "  vocab_size = 256,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "history = model.fit(dataset, epochs=10, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 4551\n",
      "Compression ratio: 1.13775\n",
      "Potential compression ratio with arithmetic coding: 1.13775\n"
     ]
    }
   ],
   "source": [
    "article = articles[120][:500]\n",
    "ones, archived_size = huffman_archive_size(model, article)\n",
    "print('Total length:', archived_size)\n",
    "\n",
    "compression_ratio = archived_size / (len(article) * 8)\n",
    "print('Compression ratio:', compression_ratio)\n",
    "\n",
    "k = (ones / archived_size)\n",
    "compression_ratio * (-k * np.log2(k) - (1-k) * np.log2(1-k))\n",
    "print('Potential compression ratio with arithmetic coding:', compression_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
