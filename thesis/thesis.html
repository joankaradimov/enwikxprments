<html>
<head>
  <title>Компресиране на някои представяния на знание с приложение на рекурентни невронни мрежи</title>
  <meta name="author" content="Йоан Карадимов">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.20.0/themes/prism.min.css" rel="stylesheet" />
  <link href="prism-vs.css" rel="stylesheet" />
  <style>
    * { box-sizing: border-box; }

    p { text-align: justify; }

    html { font-size: 19px; }
    h1 { font-size: 3rem; text-align: center; }
    h2 { font-size: 2rem; text-align: center; }
    h3 { font-size: 1.8rem; }
    h3 { font-size: 1.5rem; }
    sub { font-size: .5rem; }
    sup { font-size: .5rem; }

    @page {
      size: A4;
      padding: 2cm;
      margin: 2cm;
    }

    .pages {
      width: 21cm;
      min-height: 29.7cm;
      padding: 2cm;
      margin: 1cm auto;
      border: 1px #D3D3D3 solid;
      border-radius: 2px;
    }

    @media print {
      html {
        font-size: 19px;
      }

      .pages {
        padding: 0;
        margin: 0;
        border: initial;
        border-radius: initial;
        width: initial;
        min-height: initial;
        box-shadow: initial;
        background: initial;
        page-break-after: always;
      }

      a {
        color: black;
        text-decoration: none;
      }
    }

    .contents ol { counter-reset: item; }
    .contents ol > li { display: block; }
    .contents ol > li:before {
      content: counters(item, ".") " ";
      counter-increment: item;
    }

    .references { font-size: 0.8rem; }
    .references ol { counter-reset: item; padding-left: 0; }
    .references ol > li { display: block; padding: 0 0 9px 35px;}
    .references ol > li:before {
      content: "[" counters(item, '') "]";
      counter-increment: item;
      margin-left: -35px;
      min-width: 30px;
      display: inline-block;
    }

    .dataset-fields li {
      padding-bottom: 15px;
    }

    .dataset-fields-table {
      width: 100%;
      border-collapse: collapse;
      border: 1px solid black;
      margin: 20px 0;
    }
    .dataset-fields-table tr { border: 1px solid black; }
    .dataset-fields-table td { text-align: center; }
    .dataset-fields-table tr:first-child { border-bottom: 4px double black; }
    .dataset-fields-table td:first-child + td { border-left: 4px double black; }

    .glossary-table {
      width: 100%;
      border-collapse: collapse;
      border: 1px solid black;
    }
    .glossary-table th { border-bottom: 4px double black; }
    .glossary-table th { border-left: 1px solid black; }
    .glossary-table td { border-left: 1px solid black; }

    pre[class*="language-"] {
      border: none;
      overflow: hidden;
    }

    code[class*="language-"] {
      white-space: pre-wrap;
      word-break: break-word;
    }
  </style>

  <script>
MathJax = {
  options: {
    renderActions: {
      find: [10, function (doc) {
        for (var node of document.querySelectorAll('script[type^="math/tex"]')) {
          var display = !!node.type.match(/; *mode=display/);
          var math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
          var text = document.createTextNode('');
          node.parentNode.replaceChild(text, node);
          math.start = {node: text, delim: '', n: 0};
          math.end = {node: text, delim: '', n: 0};
          doc.math.push(math);
        }
      }, '']
    }
  }
};
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.20.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.20.0/plugins/autoloader/prism-autoloader.min.js"></script>
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
  <script>
jQuery(function ($) {
  // This populates the superscript reference numbers
  $('.references li').each((index, $element) => {
    // TODO: drop the <sup> elements that are already present in a tags
    $(`a[href="#${$element.id}"]`).html(`<sup>[${index + 1}]</sup>`);
  });
});
  </script>
</head>
<body>
  <div class="pages" style="text-align: center;">
    <img src="SU.png" style="width: 131px; height: 131px;">
    <div style="margin-top: 1cm; font-size: 1.4rem;">
      <div>Софийски университет „Св. Климент Охридски“</div>
      <div>Факултет по математика и информатика</div>
      <div>Катедра „Компютърна информатика“</div>
    </div>

    <div style="margin-top: 2cm; font-size: 1.5rem;">ДИПЛОМНА РАБОТА</div>
    <hr>
    <h2>
      Компресиране на някои представяния на знание с приложение на рекурентни невронни мрежи
    </h2>
    <hr>

    <div style="margin-top: 2cm; font-size: 1.2rem;">
      <div>Дипломант</div>
      <div>Йоан Владимиров Карадимов</div>
      <div>ф.н. 23393, магистърска програма „Изкуствен интелект“</div>
    </div>

    <div style="margin-top: 1cm; font-size: 1.2rem;">
      <div>Ръководител</div>
      <div>доц. Калин Георгиев Николов</div>
    </div>

    <div style="font-size: 1.2rem; position: absolute; bottom: 0; left: 0; right: 0;">
      София, юли 2020
    </div>
  </div>

  <div class="pages">
    <div style="font-weight: bold;">Анотация</div>
    <div style="width: 80%; padding-top: 1cm;">
      С формализацията си на интелект, представена в книгата „Universal Artificial Intelligence. Sequential Decisions
      Based on Algorithmic Probability“, Чарлз Хътър и други автори демонстрират връзката между изкуствен интелект и
      компресия на данни. По този начин съпоставят нивото на компресия, което може да бъде постигнато за даден набор от
      данни, с интелекта на агента който извършва компресията.

      Тази дипломна работа предлага метод за постигане на компресия чрез комбинация от ентропийно кодиране и
      вероятностен предсказващ модел. Моделът се базира на рекурентни невронни мрежи и бива обучен с голям обем човешко
      знание, базирано на енциклопедията Wikipedia. Авторът мотивира нуждата от модел с минимален размер и предлага
      подходи за постигането му. Изброяват се параметрите на модела и се изследва влиянието на тези параметри върху
      способността на модела да научава и компресира данни.
    </div>
  </div>

  <div class="pages contents">
    <h2>1 Съдържание</h2>
    <ol>
      <li>Съдържание</li>
      <li>
        Въведение
        <ol>
          <li>Математическо моделиране на интелекта</li>
          <li>Предмет на дипломната работа</li>
          <li>Цел на дипломната работа</li>
        </ol>
      </li>
      <li>
        Компресията като мярка на интелект
        <ol>
          <li>Ентропия на Шанън и оптималната компресия</li>
          <li>Сложност по Колмогоров и универсална индукция на Соломонов</li>
          <li>AIXI и AIXItl</li>
          <li>Теорема за най-бърз и най-кратък алгоритъм</li>
        </ol>
      </li>
      <li>
        Наборите от данни <i>enwikX</i>
        <ol>
          <li>MediaWiki page export format</li>
          <li>Wikitext</li>
          <li>
            Изследване на данните
            <ol>
              <li>Статии</li>
              <li>Възможни символи</li>
              <li>Малки и големи букви</li>
              <li>Заглавия</li>
              <li>Автори</li>
              <li>Дати на създаване</li>
            </ol>
          </li>
        </ol>
      </li>
      <li>
        Приз на Хътър
        <ol>
          <li>Текущи най-добри резултати</li>
          <li>Забележки относно правилата</li>
        </ol>
      </li>
      <li>
        Предложен подход за компресия на наборите от данни <i>enwikX</i>
      </li>
      <li>
        Предварителна обработка на данните
        <ol>
          <li>Предварителна обработка на метаданни</li>
          <li>Предварителна обработка на текст</li>
        </ol>
      </li>
      <li>
        Рекурентни невронни мрежи
        <ol>
          <li>Класически подход и проблеми</li>
          <li>
            Обработка на свойства (feature engineering)
            <ol>
              <li>Размер на азбуката</li>
              <li>Размер на речник от под-думи</li>
            </ol>
          </li>
          <li>Свръх-нагаждане (overfitting)</li>
        </ol>
      </li>
      <li>
        Провеждане на експерименти и анализ на резултатите
        <ol>
          <li>Брой на категориите</li>
          <li>LSTM срещу GRU</li>
          <li>Способност за наизустяване</li>
          <li>Способност за предсказване</li>
          <li>Статистика на предсказанията</li>
          <li>Брой и размер на рекурентните слоеве</li>
        </ol>
      </li>
      <!--li>
        Ентропийно кодиране
        <ol>
          <li>Кодиране на Хъфман</li>
          <li>Аритметично кодиране</li>
        </ol>
      </li-->
      <li>Методи и материали</li>
      <li>
        Заключение
        <ol>
          <li>Резултати и дискусии</li>
          <li>Бъдещи цели</li>
        </ol>
      </li>
      <li>Библиография</li>
    </ol>
  </div>

  <!-- div class="pages contents">
    <h2>Речник на използваните понятия</h2>

    <p>
      <i>Таблица 1</i> представя речник на понятията, използвани в дипломната работа, за които липсва изрична дефиниция.
    </p>

    <table class="glossary-table">
      <tr>
        <th>Понятие</th>
        <th>Описание</th>
      </tr>
      <tr>
        <td>базова оценка</td>
        <td>
          baseline
        </td>
      </tr>
      <tr>
        <td>влагане на свойства</td>
        <td>
          feature embedding
        </td>
      </tr>
      <tr>
        <td>слой за влагане</td>
        <td>
          embedding layer
        </td>
      </tr>
      <tr>
        <td>рекурентен слой</td>
        <td>
          recurrent layer
        </td>
      </tr>
      <tr>
        <td>напълно свързан слой</td>
        <td>
          dense layer
        </td>
      </tr>
      <tr>
        <td>n-грами</td>
        <td>
          енторки
        </td>
      </tr>
      <tr>
        <td>маркиращ език</td>
        <td>
          markup language
        </td>
      </tr>
      <tr>
        <td>партиди от данни</td>
        <td>
          batch-ове
        </td>
      </tr>
      <tr>
        <td>компресия без загуби</td>
        <td>
          lossless компресия
        </td>
      </tr>
      <tr>
        <td>компресия със смесване на контексти</td>
        <td>
          context mixing
        </td>
      </tr>
      <tr>
        <td>синтактичен анализ</td>
        <td>
          parse-не
        </td>
      </tr>
      <tr>
        <td>единична точност</td>
        <td>
          single precision
        </td>
      </tr>
      <tr>
        <td>свойство</td>
        <td>
          feature
        </td>
      </tr>
      <tr>
        <td>извличане на свойства</td>
        <td>
          feature extraction
        </td>
      </tr>
      <tr>
        <td>обработване на свойства</td>
        <td>
          feature engineering
        </td>
      </tr>
      <tr>
        <td>категория</td>
        <td>
          категорийните данни са добър кандидат за свойства при обучение
        </td>
      </tr>
      <tr>
        <td>силен изкуствен интелект</td>
        <td>
          strong artificial intelligence
        </td>
      </tr>
      <tr>
        <td>обща интелигентност</td>
        <td>
          general intelligence
        </td>
      </tr>
    </table>

    <p><i>таблица 1</i></p>
  </div -->

  <div class="pages">
    <h2>2 Въведение</h2>

    <p>
      Постигането на силен изкуствен интелект и обща интелигентност е една от задачите, с които изкуственият
      интелект (ИИ) се сблъсква още от създаването си. Въпреки относително бурното развитие на ИИ през последните
      години, прогресът в областта на общата интелигентност за момента е слаб. От части това може да се обясни с
      хардуерни ограничения, лимитиращи способността ни да достигаме смислени практически резултати. От друга страна -
      преди въобще да се търси смислен резултат в контекста на силния изкуствен интелект, трябва да бъде дефиниран
      начин за проверка и тестване на въпросния резултат.
    </p>

    <p>
      Добре известен е тестът на Тюринг <a href="#reference-turing-test"></a>. Но още в първата глава
      "The Imitation Game" на основополагащата си статия, Тюринг отказва да се ангажира с дефиниция на интелект, като
      вместо това избира да търси алтернативен въпрос. Подобен, но по-малко известен, е тестът на работното място
      (The Employment Test)<a href="#reference-employment-test"></a>, формулиран от Нилс Нилсън.
      Целта е да се провери дали агент може да се справи поне толкова
      добре колкото човек в служба, съществена за икономиката. Тестът на студента-робот (The Robot College Student
      Test)<a href="#reference-robot-college-test"></a> на Бен Гьорцел цели да провери способността на агент
      успешно да премине и завърши пълен университетски курс.
    </p>

    <p>
      Множество други автори предлагат подобни тестове. И въпреки че много от тях представят интересни перспективи,
      обединяващо е, че никой не предлага строга формализация или подход към решаването на проблема. Иначе казано -
      изброените тестове дават крайна цел, но не и явна функция, която да оптимизираме, за да стигнем до силен
      изкуствен интелект. За такова начинание, за начало, е нужна общоприета дефиниция на интелект. По темата,
      Джон МакКарти казва: "Проблемът е, че като цяло, все още не можем да характеризираме кои изчислителни процедури да
      наречем интелигентни"<a href="#reference-mc-carthy-on-intelligence"></a>. Липсата на такава
      общоприета и твърда дефиниция на интелект оставя място за интерпретация, но и създава интересни възможности за
      изследвания.
    </p>

    <h3>Математическо моделиране на интелекта</h3>

    <p>
      В книгата си Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability, в търсене на
      числен модел на интелекта, Маркъс Хътър предлага формализма на универсалния алгоритмичен агент&mdash;AIXI.
      AIXI е агент за обучение с утвърждение, чиято цел е да максимизира целева награда, оценена с число. Това се
      извършва на базата на предишни действия, както и на взаимодействие с дадена среда, описана чрез машина на Тюринг.
      Хътър доказва, че оптималното поведение на такъв агент е да предполага, че средата, с която взаимодейства,
      е описана от най-кратката възможна програма<a href="#reference-hutter-ai-theory"></a>.
    </p>

    <p>
      Агентът не разполага с функцията, описваща средата в явен вид. Така неговата цел става построяването на най-малък
      възможен модел на въпросната среда, базирайки се на взаимодействието си с нея. Тъй като такъв модел би бил описан
      с програма, целта на агента, на практика, е да компресира наблюденията върху средата си до най-кратката такава
      програма<a href="#reference-mahoney-on-compression"></a>.
    </p>

    <div style="font-style: italic; display: inline-block; margin-bottom: 12px;">
      <div style="float: left; width: 38%; margin: 0 3%;">
        [...] being able to compress well is closely related to acting intelligently, thus reducing the slippery concept
        of intelligence to hard file size numbers. In order to compress data, one has to find regularities in them, which
        is intrinsically difficult.
      </div>

      <div style="float: right; width: 50%; margin: 0 3%;">
        [...] способността да се компресира добре е тясно свързана с възможността да действаш интелигентното. Така
        неясната идея за интелигентност може да се сведе до размери на файлове и числа. А за да се компресират данни, е
        нужно да се намерят закономерности в тях, което е трудно.
      </div>
    </div>
    <div style="text-align: right; font-style: italic; margin-bottom: 12px;">
      Маркъс Хътър<a href="#reference-hutter-prize"></a>
    </div>

    <p>
      Разглеждането на компресия като еквивалентна на интелект предоставя мощен инструмент за оценка. Разполагайки с
      набор от данни, способността за компресия на данните дава числова мярка на интелекта, който ги компресира.
    </p>

    <h3>Предмет на дипломната работа</h3>

    <p>
      През 2006-та година Маркъс Хътър стартира Приза на Хътър (Hutter Prize)<a href="#reference-hutter-prize">
      </a>&mdash;практическа задача за компресия на човешко знание.
      Целта на тази дипломна работа е да очертае подход за компресия, съвместим с изискването на състезанието. Това се постига,
      като се строи модел за предсказване на данни, чрез използването на актуални изследвания и технологии.
      Изследването набляга на прилагането на рекурентни невронни мрежи за строенето на модела&mdash;и
      по-специално&mdash;LSTM и GRU. Тези архитектури са показали своята ефективност в предсказването на последователности
      от данни<a href="#reference-unreasonable-rnn-effectiveness"></a>.
    </p>

    <p>
      Построените предсказващи модели дават оценки на вероятностите за възможни следващи символи в набора от данни.
      Компресия се постига чрез ентропийно кодиране, като на символи с по-висока вероятност се
      съпоставят по-кратки последователности от битове, а на по-малко вероятни&mdash;по-дълги. Разглеждат се кодиране
      на Хъфман и аритметично кодиране, както и отражението им върху обработката на свойства,
      което се прилага.
    </p>

    <p>
      Интересен е въпросът&mdash;ако се сравняват алгоритми за компресия&mdash;какъв набор от данни да бъде избран.
      Очевиден отговор е "цялото човешко знание". Но такъв отговор няма добра практическа стойност.
      Нужна е някаква текстова репрезентация на възможно най-голяма част от човешкото знание.
      Като еталонен тест <!-- benchmark --> Джим Бауъри предлага текстовото съдържание на
      Wikipedia<a href="#reference-jim-bowery-c-prize"></a>. Тя е един добър кандидат, тъй като
      притежава съществен обем. От друга страна е подходяща заради отвореността и лесния достъп до данните ѝ.
      Хътър подкрепя идеята и решава да използва Wikipedia за целите на Приза на Хътър.
    </p>

    <p>
      Това е и практическият проблем, който разглеждаме&mdash;постигането на добра компресия на данните в Wikipedia.
      В частност&mdash;върху набори от данни наречени <i>enwikX</i>, където X ∈ {5, 6, 7, 8, 9} и съдържа първите
      10<sup>X</sup> байта от XML репрезентация на данните в Wikipedia.
    </p>

    <p>
      Естествен въпрос е какво означава "добра" компресия. Често използвани мерки са "по-добра от съществуващия машинен подход",
      както и "по-добра от човек". В контекста на силния изкуствен интелект, второто представлява по-голям интерес.
      Изследвайки ентропия и компресия, Клод Шанън провежда експерименти с хора, опитвайки се да оцени способността
      им да служат като модел за предсказване на текст. Използвайки 27 символа - буквите от латиницата в английския,
      допълнени със символ за интервал - Шанън оценява способността на хората на еквивалентна на компресия в
      границите между 0.6 и 1.3 бита за символ<a href="#reference-shannon-human-baseline"></a>.
      Махони провежда симулации с конкретни модели и оценява същата мярка на около 1 бит за символ, като добавянето на
      пунктуация и разграничаването на големи/малки букви вдига мярката до 1.25 бита за
      символ<a href="#reference-mahoney-on-compression"></a>.
    </p>

    <p>
      Необичайно в случая е, че избягването на свръх-нагаждането (overfitting) няма да бъде цел. Интересно е, че за
      компресия чрез невронни мрежи може дори да бъде полезно<a href="#reference-overfitting-as-memorization"></a><a href="#reference-overfitting-for-compression"></a>.
      Друго нехарактерно е ръчната обработка на свойства от данните.
      Този подход, необходим за много от алгоритмите за машинно самообучение, обикновено се заобиколя при невронните мрежи.
      Но в случая обработката на свойства се налага и има паралели с някои подходи при класически алгоритми за компресия.
    </p>

    <h3>Цел на дипломната работа</h3>

    <p>
      Тази дипломна работа си поставя редица цели. Следва описание на въпросните цели, като и разбиването им в план.
      Във всяка от точките на плана са описани съответстващи им задачи, както и главите от работата, в които задачите са
      решени.
    </p>

    <h4 style="margin-bottom: 0;">Изследване на данните</h4>

    <p style="margin-top: 10px;">
      В глава "4 Наборите от данни enwikX" се изследват наборите от данни, с които се оценява Hutter Prize. Първоначално
      данните се разглеждат на високо ниво, като се обръща внимание на присъствието на два формални езика и един естествен
      (английски) език, вложени един в друг. В последствие се обръща внимание на структурата и конкретни полета с метаданни.
      Търсят се възможности за опростяване на потенциалните тренировъчни данни. Изследват се и детайли, които биха били
      полезни на етап трениране на модел.
    </p>

    <h4 style="margin-bottom: 0;">Формулиране на подход</h4>

    <p style="margin-top: 10px;">
      В глава "6 Предложен подход за компресия на наборите от данни enwikX" се формулира груб подход за компресия,
      като имплементационните детайли се оставят за по-късно. Обръща се внимание на начина за постигане на компресия,
      както и на възстановяването на оригиналните данни при декомпресия.
    </p>

    <h4 style="margin-bottom: 0;">Предварителна обработка на данните</h4>

    <p style="margin-top: 10px;">
      В глава "7 Предварителна обработка на данните" се описва обработката на XML данните и отделянето на метаданните
      от същинското текстово съдържание на статиите. Разглежда се наивна стратегия за компресиране на метаданните, която
      не използва подходи от машинното самообучение и изкуствения интелект, но притежава интересни паралели с
      преобразуванието на Бъроуз-Уилър. Текстовото съдържание се интерпретира като последователност от категории.
      Разглеждат се начини за намаляване на броя на категориите и за съкращаване на последователността от категории.
    </p>

    <h4 style="margin-bottom: 0;">Моделиране с невронни мрежи</h4>

    <p style="margin-top: 10px;">
      В глава "6.2 Рекурентни невронни мрежи" се разглежда способността на рекурентни невронни мрежи да служат
      като модели за предсказване на последователности от данни. Представят се резултати от други изследвания и се описва
      общата им структура, таксономията им и развитието им във времето. Мотивира се избора на разглежданите архитектури
      и се дават някои алтернативи. Дефинират се свръх-нагаждане (overfitting) и обработка на свойства (feature engineering)
      и се описват начините, по които употребата им в предложения подход се различава от традиционните им приложения.
    </p>

    <h4 style="margin-bottom: 0;">Провеждане на експерименти</h4>

    <p style="margin-top: 10px;">
      В глава "9 Провеждане на експерименти и анализ на резултатите" се разглежда избора на конкретна архитектура на
      рекурентни невронни мрежи. Изследват се хиперпараметрите на въпросните невронни мрежи - брой слоеве, размер на
      слой за влагане, размер на рекурентен слой, както и по-общите хиперпараметри -
      размер на азбуката от символи и брой на n-грами, които се разглеждат.
    </p>

    <!--h4 style="margin-bottom: 0;">Имплементации и изследване на ентропийни кодирания</h4>

    <p style="margin-top: 10px;">
      В глава "10 Ентропийно кодиране" се сравняват асимптотичните сложности и някои практически
      проблеми при две от най-разпространените ентропийни кодировки. Сравнява се представянето им във времето и се
      демонстрират идеи за оптимизации.
    </p-->

    <h4 style="margin-bottom: 0;">Заключение</h4>

    <p style="margin-top: 10px;">
      В глава "11 Заключение" се правят изводи и се очертават насоки и бъдещи цели за изследване.
    </p>

  </div>

  <div class="pages">

    <h2>3 Компресията като мярка на интелект</h2>

    <p>
     През 1950 излиза публикацията на Алан Тюринг наречена „Изчислителни машини и интелект“<a href="#reference-turing-test"><sup>[x]</sup></a>.
     В нея Тюринг задава фундаменталния въпрос „могат ли машините да мислят?“. Точният отговор на този въпрос изисква
     точни определения на понятията „машина“ и „мислене“. Намеквайки, че това е изключително трудно, вместо определения,
     Тюринг предлага своя „критерий за «мислене»“&mdash;известната „игра на имитиране“. Играта се играе от трима участници:
     машина, човек и разпитвач (също човек). Разпитвачът може да общува с машината и човека писмено,
     без да ги вижда или чува. И машината, и човекът се представят пред разпитвача като хора. Целта на разпитвача е,
     задавайки им въпроси, да определи кой от двамата е машината и кой&mdash;човекът. Целта на човека е да помогне
     на разпитвача, а целта на машината е да заблуди разпитвача, т.е. да имитира човека максимално добре.
     Машината отговаря на критерия на Тюринг, ако успява да заблуди разпитвачите достатъчно често.
    </p>

    <p>
     Мислещите машини, които има предвид Тюринг, не са произволни. Тюринг се интересува от цифровите изчислителни машини,
     които по това време придобиват огромно значение, а днес са част от почти всеки аспект на живота ни.
     За да изследва въпроса теоретично, той използва математически модел на изчислителни машини&mdash;добре известните
     ни машини на Тюринг. Тези машини са въведени от него години преди това<a href="#reference-turing-machine"><sup>[x]</sup></a>,
     когато изследва „проблема за разрешимост“ на Хилберт и Акерман: има ли алгоритъм, който човек може да следва,
     за да реши дали произволна формула от предикатната логика е тавтология или не? Т.е. в проблема за разрешимост,
     Тюринг използва машините си за модел на човешки сметачи („human computer“), а в проблема за мислещите
     машинни&mdash;за модел на машинни сметачи („digital computer“). Така се стига до следния алгоритмичен вариант на
     проблема за мислещите машини: могат ли алгоритмите, изпълними от хора, да мислят? Това е основният теоретичен
     проблем в областта на изкуствения интелект.
    </p>

    <p>
     Тюринг предлага критерия си най-вече, за да провокира диалог около това дали машините могат да мислят<a href="#reference-turing-provocation"><sup>[x]</sup></a>.
     И наистина, водещи учени от онова време откликват. Особено интересна е критиката на Клод Шанън и Джон Маккарти
     към критерия на Тюринг. Двамата отбелязват, че принципно могат да бъдат събрани достатъчно пълни наблюдения на
     всевъзможни въпроси и отговори (да речем от вече случили се игри на имитиране) в огромен „речник“
     достъпен до машината. Разполагайки с такъв речник, машината може просто да поглежда отговора на всеки въпрос,
     които разпитвачът ѝ задава по време на игра. Понеже речника е достатъчно пълен, това би заблудило разпитвачите
     достатъчно, за да покрие критерия на Тюринг. Проблемът е, че по никакъв начин не бихме нарекли такава машина
     мислеща. Тази забележка посочва важен недостатък в критерия на Тюринг. Въпреки че една такава машина би
     била чудесен имитатор, на нея ѝ липсва важно свойство, присъщо на хората: <em>способността да анализират нови ситуации</em>.
     Наистина, за да разболичим такъв имитатор, просто трябва да намерим клас от въпроси неприсъстващи в речника
     и да ги подскажем на разпитвача. Това не е никак трудно, поради неизчерпаемостта на човешкия език.
    </p>

    <p>
     Неспособността за анализиране на нови ситуации е пряко следствие от устройството на машината&ndash;речник.
     Както казва Ада Лъвлейс относно аналитична машина на Чарлз Бабидж:
    </p>
    <div style="font-style: italic; display: inline-block; margin-bottom: 12px;">
      <div style="float: left; width: 38%; margin: 0 3%;">
        The Analytical Engine has no pretensions to originate anything. It can do <u>whatever we know how to order it</u> to perform.
      </div>
      <div style="float: right; width: 50%; margin: 0 3%;">
        Аналитичната машина не претендира да е първоизточник на нищо. Тя може да направи <u>само това което ние знаем как да ѝ наредим</u> да направи.
      </div>
    </div>
    <p>
     Можем да интерпретираме първата част на това твърдение (тази за първоизточника), като възражение срещу
     възможността машините да мислят самостоятелно. Но всъщност, както отбелязва Дългас Хартри<a href="#reference-turing-test"><sup>[x]</sup></a>,
     втората част на твърдението (тази за „нарежданията“) не подкрепя първата. Хартри не изключва да се създаде машина,
     която може да мисли самостоятелно или пък дори да се учи. С други думи, машината може първоначално да следва
     заложените ѝ „нареждания“, но след това, мислейки самостоятелно и учейки се, да се развие отвъд заложеното в нея.
     Наистина, тази възможност не противоречи на втората част на твърдението на Лъвлейс: до каквото и да се развие
     машината, ние все пак принципно можем да го разберем и да наредим същото на друга машина.
    </p>

    <p>
     Цитирайки Хартри, Тюринг напълно се съгласява с него. Тюринг отбелязва, че основният проблем при създаването на
     мислеща машина не е необходимата изчислителна мощ, която той вярва че може да се постигне. Основният проблем е способността ни
     да програмираме една такава машина да мисли. Той предлага, вместо да създадем машина, която може да имитира мозъка на възрастен,
     да създадем машина, която може да имитира мозъка на дете, надявайки се програмирането ѝ да е много по-просто.
     Учейки такава машина&ndash;дете, ще можем да я развием до машина способна да имитира възрастен.
     Така се стига до може би основният практически проблем в изкуствения интелект: как да създадем алгоритми, които могат да учат?
    </p>

    <h3>3.1 Проблем за индукцията, подход на Бейс&ndash;Лаплас</h3>

    <p>
     Проблема за обучението на алгоритми е твърде всеобхватен, за да бъде атакуван директно. Важен подпроблем е това
     как хората правят връзката между причина и следствие. В най-опростен вид, причинно-следствените връзки могат да бъдат
     изказани като универсални твърдения за заобикалящия ни свят, например „водата завира при 100 градуса по Целзий“.
     Характерното за причинно-следствените връзки е, че те не са непременно верни, т.е. те не са тавтологии от рода на
     „водата завира когато завира“. Понеже дедукцията от тавтологии ражда само тавтологии, формирането на такива връзки
     изисква да се вземат предвид факти от опита със заобикалящия свят.
     Например, твърдението, че водата завира при 100 градуса, изисква потвърждение в достатъчно много опити,
     както и липсата на опити, сочещи противното. Такива изводи, от частното (опити) към общото (универсални твърдения)
     са известни като <em>индуктивни изводи</em>, а процесът на извеждане&mdash;<em>индукция</em>.
    </p>

    <p>
     Изследвайки формирането на причинно-следствени връзки, Дейвид Хюм прави доста изненадващо наблюдение: индукцията
     не може да бъде сведена до дедуктивна логика. Не съществува верен „принцип на индукцията“, който комбиниран
     със факти, извлечени от опита, да позволява чрез чисто дедуктивни разсъждения да стигнем до индуктивен извод.
     Аргументът на Хюм е неформален. Според него такова свеждане изисква заобикалящият ни свят да бъде по-еднообразен
     отколкото е: валиден принцип на индукцията би предполагал бъдещето твърде силно да прилича на миналото. <em>Проблемът за индукцията</em> е да се покаже,
     че всъщност индукцията може да се сведе до дедуктивна логика. И до ден днешен няма консенсус по въпроса.
     Самият Хюм вярва, че човешката способност за индукция е вроден навик, неподкрепен от нищо рационално<a href="#reference-hume-habit"><sup>x</sup></a>.
    </p>

    <p>
     Независимо един от друг, Томас Бейс и Пиер-Симон Лаплас, задават друг въпрос относно причинно-следствените връзки:
     колко вероятни са те? По-точно, ако <i>h</i> е хипотетична връзка, а <i>x</i> са данни от опити,
     пита се колко е вероятна хипотезата при дадените данни?  Отогворът е известното равенство:
    </p>
    <p>
      <script type="math/tex; mode=display">P(h | x) = \frac{P(x | h) P(h)}{P(x)}.</script>
    </p>
    <p>
     Чрез това равенство, правата зависимост от причинно-следствена връзка към опити може да бъде „обърната“,
     позволявайки извеждането на най-вероятните връзки съвместими с дадени опити.  Това изисква преминаването от
     класически модели на света към вероятностни модели: всяка хипотеза <i>h</i> е твърдение, което напълно
     определя вероятността <i>P(x|h)</i> на данните при условие, че хипотезата е вярна. При сравнение на различните
     хипотези безусловната вероятност <i>P(x)</i> на данните не е нужна, понеже не зависи от хипотезата и може да се съкрати.
     Остава да се определи само <em>априорната вероятност</em> <i>P(h)</i> на всяка от хипотезите. В общия случай тази вероятност
     е неизвестна и бива заместена с приближение, изразяващо частичното знание или уклона на този, който прави извода.
     При пипсата на знание, Лаплас предлага <i>P(h)=1</i> за всяка хипотеза, въпреки че в общия случай това води до
     невалиден избор на вероятностна мярка.
    </p>

    <p>
     Лаплас дава чудесен пример<a href="#reference-kolmogorov-introduction"><sub>[x]</sub></a> за приложението на подхода
     при определянето дали даден обект е резултат от регулярен или от случаен процес.
     Ако на някоя маса видим множество от букви подредени в низа „константинопол“, коя от следните хипотези е по-вероятна:
     <em>h<sub>1</sub></em>&mdash;човек умишлено да ги е подредил в този ред, или <em>h<sub>2</sub></em>&mdash;подредбата да е резултат от случаен процес, да речем равновероятно
     измежду 14 буквените низове? Понеже не знаем коя от двете хипотези е по-вероятна, полагаме <em>P(h<sub>i</sub>)=1, i=1..2</em>.
     При равновероятния процес, низът „константинопол“ има точно същата вероятност&mdash;30<sup>-14</sup>&mdash;както
     и всеки произволен нерегулярен низ, например анаграмата му „сннтлкиоаотноп“. От друга страна, първата е обикновена дума
     от речта, макар и не толкова често срещана, докато втората не просто не присъства във речта, а дори и трудно се произнася.
     Много по-вероятно е човек да подреди буквите в първия низ, отколкото във втория. Не само това, но и вероятността човек
     да произведе първия низ е много по-голяма от вероятността случаен процес да го произведе:
    </p>
    <p>
     <script type="math/tex; mode=display">P(\textit{„константинопол“} | h_1) \gg P(\textit{„константинопол“} | h_2).</script>
    </p>
    <p>
     Тогава <em>P(h<sub>1</sub>|„константинопол“) ≫ P(h<sub>2</sub>|„константинопол“)</em>, т.е. по-вероятно е човек
     да е подредил думата, отколкото това да е резултат от чисто случаен процес.
    </p>

    <h3>3.2 Соломонова индукция и Колмогорова сложност</h3>
    <p>
     Основният недостатък при подхода на Бейс-Лаплас е неяснотата около избора на априорна вероятност <em>P(h)</em>.
     Поддръжниците на този тип индуктивен извод основно се делят на два лагера. Първите твърдят, че изборът на априорната вероятност
     е напълно субективен и изразява вярванията на избиращия. Т.е. за тях подхода е консистентен начин за индуктивен извод от
     субективни предположения или вярвания. Поддръжниците в другия лагер са против субективния елемент и се опитват да поставят
     подхода на Бейс-Лаплас на обективни основни. Може би най-подробният опит за това е работата на Рудолф Карнап<a href="#reference-logical-probability"><sup>[x]</sup></a><a href="#reference-carnap-kaput"><sup>[x]</sup></a>.
     Карнап създава вероятностно смятане базирано на формална логика. В смятането възможните състояния на света се описват от
     специален клас логически формули. Комбинирани една с друга, формулите от този клас могат да изразят сравнително широк клас от хипотези.
     Всяка вероятностна мярка върху специалния клас се разширява до вероятностна мярка <em>m</em> върху класа
     от изразими хипотези. Както и при подхода на Бейс-Лаплас, индуктивните изводи следват от максимизирането на условната вероятност
     <em>m(h|x)</em> на хипотезата <em>h</em> при условие данните <em>x</em>.  Понеже и хипотезите и данните се описват еднотипно
     чрез логически формули, е по-лесно да се работи директно със съвместната вероятност:
    </p>
    <p>
     <script type="math/tex; mode=display">m(h|x) = \frac{m(x \land h)}{m(x)}.</script>
    </p>
    <p>
    Разбира се, ключовият въпрос е как да се избере вероятностната мярка <em>m</em>? Карнап постига няколко интересни резултата,
    но никой от тях не е задоволителен и те не биват широко възприети<a href="#reference-carnap-kaput"><sup>[x]</sup></a>.
    </p>

    <p>
     Опитвайки се да базира индукцията на обективни критерии, Рей Соломонов развива подхода на Карнап в революционно нова посока<a href="#reference-solomonoff-report"><sup>[x]</sup></a><a href="#reference-solomonoff-inductive-1"><sup>[x]</sup></a><a href="#reference-solomonoff-inductive-2"><sup>[x]</sup></a>.
     Соломонов предполага, че данните в опитите се генерират от изчислителен процес. Неизвесните хипотези са не логически
     формули, както при Карнап, а алгоритми. Соломонов допълнително предполага принципа на Окам в следния вид:
     по-простите хипотези, т.е. по-простите алгоритми, са по-вероятни от по-сложните. За да реализира това предложение
     Соломонов фиксира произволна универсална машина на Тюринг <em>U</em>, която служи като универсален програмен език за
     описание на алгоритмите. Разглежданите алгоритми <em>h</em> не обработват входни данни (т.е. описват затворени светове),
     а само произвеждат изход <em>x=U(h)</em> или пък въобще не терминират.  За сложност на даден алгоритъм <em>h</em> Соломонов
     взима неговата дължина <em>l(h)</em>. Принципът на Окам се реализира чрез следната мярка:
    </p>
    <p>
     <script type="math/tex; mode=display">m(h) = 2^{-l(h)}.</script>
    </p>
    <p>
     Особеното тук е, че така дефинираната мярка <em>m</em> всъщност не отговаря на условията за вероятностна мярка. Вероятностите на всички
     хипотези не се сумират до единица, а дори може да се окаже, че сумата им е безкрайна. За да се осигури крайност, могат
     да се приложат различни подходи. Най-простият, предложен от Грегори Чайтин<a href="#reference-chaitin-information-theory"><sup>[x]</sup></a>,
     е универсалната машина <em>U</em> да се избере така, че да няма два терминиращи алгоритъма, които да са префикс един на друг.
     С други думи, работим с универсален програмен език, в който нито една програма не е префикс на друга програма. В такъв случай,
     неравенството на Крафт гарантира, че сумата е по-малка или равна на единица. Сумата е равна на известната Чайтинова константа <em>Ω</em>,
     която дава вероятността, случайно генериран алгоритъм, да терминира. Понеже не всички алгоритми терминират, константата <em>Ω</em>
     е строго по-малка от единица, т.е. мярката <em>m</em> е подвероятностна мярка. Соломонов предлага лесен начин за нормализация
     до вероятностна мярка, който води до еквивалентна теория. Все пак мярката <em>m</em> е по-предпочитана, защото с нея се работи по-лесно.
    </p>

    <p>
     За Соломонов индуктивният извод на хипотеза-алгоритъм <em>h</em> всъщност е само междинна цел. Крайната му цел е
     предсказването на резултати от бъдещи опити, използвайки резултати от минали опити. По-конкретно, той използва мярката
     <em>m</em> за предсказване на възможните продължения на наблюдавани редици. Например, на въпроса как да бъде продължена
     редицата
    </p>
    <p>
     <script type="math/tex; mode=display">2, 3, 5, 7, 11, 13, 17, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71</script>
    </p>
    <p>
     човек обикновено би отговорил (след малко сметки) 73, понеже дадената редицата се състои от първите 19 прости числа
     и следващото просто число е 73. Подхода на Соломонов дава обективен довод за този тип изводи. Интуицията е, че ако дадените
     първи <em>n</em> прости числа са достатъчно много, то всички кратки (и съответно вероятни) алгоритми, които генерират поне
     първите <em>n</em> прости числа, съдържат в себе си кратък алгоритъм за генериране на всички прости числа. Тогава е необходим
     по-малко допълнителен код, за да се продължи с генерирането на първите <em>n+1</em> прости числа, отколкото другояче.
     Алгоритмите с по-малко допълнителен код са по-вероятни от друргите, и съответно е по-вероятно редицата да
     продължи с следващото просто число, отколкото с нещо друго, генерирано от по-малко вероятен алгоритъм.
    </p>

     <p>
     За да се развие тази интуиция до общ метод, трябва да се обърне внимание на една подробност. В общия случай,
     предсказанията на най-вероятните хипотези не винаги са най-вероятни. Това може да се получи, когато група по-малко
     вероятни хипотези правят едно и също предсказание, което се различава от това на най-вероятната, но общата вероятностна
     маса на тази група е по-голяма от тази на най-вероятната хипотеза. Затова Соломонов предлага предсказанията да се
     извършват не на базата на най-вероятната хипотеза, а на най-вероятното продължение. Това продължение се определя от
     разширението на мярката <em>m</em> от хипотези <em>h</em> до редици <em>x</em>:
    </p>
    <p>
     <script type="math/tex; mode=display">m(x) = \sum_{U(h)=x} 2^{-l(h)}.</script>
    </p>
    <p>
     Математически, подходът на Соломонов се изразява като предсказване на символите на случайна редица <em>T</em>.
     Всяка фиксирана редица <em>x</em> задава случайното събитие <em>x*={x⪯T}</em> редицата <em>T</em> да започва с <em>x</em>.
     Вероятността на това събитие е
    </p>
    <p>
     <script type="math/tex; mode=display">m({x*}) = \sum_{x\preceq U(h)} 2^{-l(h)}.</script>
    </p>
    Вероятността редицата <em>T</em> да продължи с <em>y</em> при условие, че е започнала с <em>x</em> се задава с условната
    вероятност
    <p>
     <script type="math/tex; mode=display">m({xy*} | {x*}) = \frac{m({x*} | {xy*}) m({xy*})}{m({x*})} = \frac{m({xy*})}{m({x*})}.</script>
    </p>
    <p>
     Така метода на Соломонов взима предвид всички хипотези съвместими с наблюденията, а не само най-вероятната.
     Това е реализация на древния принцип на епикурейците за множеството обяснения.  Прецезирайки Лукреций<a href="#reference-lucretius-multiple"><sup>[x]</sup></a>,
     въпреки че измежду всевъзможните теории обясняващи дадени наблюдения само една рисува пълната картина,
     то няма полза да се твърди коя е тя без допълнителни сведения.
    </p>

    <p>
     Целта на Соломонов е универсален метод за индуктивни. И&nbsp;наистина, мярката <em>m</em> има няколко уникални свойства,
     които оправдават такава квалификация.  Леонид Левин показва<a href="#reference-coding-theorem"><sup>[x]</sup></a>,
     че мярката <em>m</em> доминира всяка полуизчислима подвероятностна мярка <em>p</em> с точност до мултипликативна константа <em>c<sub>p</sub></em>:
    </p>
    <p>
     <script type="math/tex; mode=display">c_p m(x) \ge p(x).</script>
    </p>
    <p>
    Левин също така прави връзката между <em>m</em> и префиксния вариант на Колмогоровата сложност, която е дължината <em>К(x)</em>
    на най-кратката програма, която генерира дадена редица.  Логаритъма на <em>m</em> е равен на <em>K</em> с точност до константа:
    </p>
    <p>
     <script type="math/tex; mode=display">\log m(x) = \frac{1}{K(x)} + c.</script>
    </p>
    <p>
     Това показва, че предсказания могат да се правят и на базата на Колмогоровата сложност, приближавайки <em>m(x)</em> с <em>2<sup>-K(x)</sup></em>.
     Така приципа на Окам придобива по-явен, неворятностен вид: при предсказание избираме това продължение <em>y</em> на <em>x</em>, което води до
     най-малка сложност на <em>xy</em>.
    </p>
    <p>
     След резултатите на Левин, Соломонов успява да докаже<a href="#reference-solomonoff-optimality"></a>, че при достатъчно данни,
     <em>m</em> прави приблизително оптимални предсказания. По-точно, ако случайната редица <em>T</em> се генирара със
     изчислима вероятностна полумярка <em>p</em>, то с течение на времето разликата между предсказанията
     на <em>m</em> и на <em>p</em> клони към нула. Този резултат оправдава квалификацията на <em>m</em> като универсалната априорна вероятност.
    </p>

    <!--
    операционно ученето се проявавя като способността за индуктивно мислене

    Хюм: разумни причини = дедукция от факти или логически принципи.  Индукцията изисква неразумни причини.  Бейс и Лаплас се оптиват да разширят общносттна на кои причини считаме за разумни използвайки вероятности: принципа на безраличност на Лаплас.  Лаплас и регулярност.  Соломонов, регулярност = алгоритъм.  Предичане в случая на регулярност.

    Лаплас причинност:

    - Регулярни причини (РП)

    - Нерегулярни причини (НП)

    Кое е по-правдоподобно обяснение за дадено събитие?

    Функция на правдоподобие:

    - P( X | РП )

    - P( X | НП )

    Бейсово предвиждане, компоненти:

    P( X_n+1 = x_n+1 | x_1, ..., x_n ) = P( X | Y ) P( Y ) / P( X )

    Карнап, частни слуичаи на индукция: предвиждане на безкрайни последователности и предричане на крайни последователности.

    Вероятностни мерки и полумерки върху безкрайни редици (непрекъснати)

    Соломонов: Универсална вероятностна полумярка

    регулярни обекти, универсални кодове, метода на апостериорния максимум

    Теорема за кодирането на Левин

    Теорема за кодирането на Шенън

    Връзка между Шенън и Колмогоровкса сложност

    поелементо кодиране

    неравенство на Крафт-Макмилан

    Използвани материали
    -->

    <!--p>
      Формализмът е
      обвързан с идеите за универсална индукция на Рей Соломонов и за сложност по Колмогоров. Но Хътър отбелязва, че
      въпреки възможността да се счете AIXI за формална дефиниция на математическо решение на ИИ, такова решение не е
      практично заради неизчислимостта му<a href="#reference-hutter-incomputablility"><sup>[x]</sup></a>.
    </p>

    <p>
      Като стъпка в посоката на практическа теория на ИИ Хътър предлага AIXItl &mdash; изчислим модел на AIXI, разглеждащ
      програми ограничени по размер и време за изчисление. Това се допълва от представената от него теорема за най-бърз
      и най-кратък алгоритъм<a href="#"><sup>[todo: цитат - страница 219]</sup></a>, която грубо казано твърди, че
      съществува единствена програма, която е най-бързата и най-кратката. Иначе казано - има единствена програма, която
      води до най-добро приближение на AIXItl до AIXI.
      [todo: малко повече пояснения, може би? Или мястото им не е в увода...]
    </p>

    <p>
      Добрите модели се описват от по-кратки програми. За по-кратка програма, може да се каже че е компресирана.
      В такъв случай може да се твърди, че демонстрирането на възможност за добро компресиране е тясно свързано с
      построяването на добър предвиждащ модел.
      От друга страна, разбирането на входните данни, изискващо интелект, позволява да се построи добър предвиждащ модел за данните.
      Отворен е въпросът, дали построяването на добър модел предполага интелект. Хътър хипотетизира, че двете са еквивалентни [todo: citation needed].
      Базирайки се на това, може да се твърди, че описването на модел в по-кратка (компресирана) програма е еквивалентно на интелект.
    </p-->


    <!--
    <h3>Ентропия на Шанън и оптималната компресия</h3>

    <p>
      <script type="math/tex; mode=display">H(X) = -\sum_i P_X(x_i) \log_b{P_X(x_i)}</script>
    </p>

    <div>ЧЕРНОВА</div>
    <i>
    <p>
      Как се третира предсказващ модел, който просто знае тестовия набор от данни наизуст? Тогава,
      изчислявайки информационната ентропия на този набор от данни, ще се получи нулева ентропия. Но за произволен друг
      набор от данни, ентропията би била не-нулева и висока. На практика това означава, че съществува алгоритъм за
      компресия, произвеждащ най-добрата възможна компресия (0 бита) за нашия набор от данни, но даващ много лоша
      компресия за всичко друго.
    </p>

    <p>
      На пръв поглед този проблем може да се избегне, ако вместо един набор от данни, оценяваме и втори. Но тогава,
      можем просто да вземем модел, който знае двата набора от данни наизуст. Тогава компресията ще работи почти идеално
      за тях, но ще се проваля за всичко друго.
    </p>
    </i>

    <h3>Теорема за най-бърз и най-кратък алгоритъм</h3>
     -->
  </div>

  <div class="pages">
    <h2>4 Наборите от данни <i>enwikX</i></h2>

    <p>
      Съществуват няколко набора от данни, носещи подобните имена: <i>enwik6</i>, <i>enwik7</i>, <i>enwik8</i>,
      <i>enwik9</i> и <i>enwik10</i>. Общият вид на имената е enwikX, където X ∈ {5, 6, 7, 8, 9}. Всеки от тях
      представлява първите 10<sup>X</sup> байта от една определена текстова репрезентация на свободно достъпната
      енциклопедия Wikipedia. Данните са взети единствено от англоезичната версия.
    </p>

    <p>
      Въпросните набори от данни са широко използвани в
      изследвания в областта на ИИ, и по-конкретно - в областите на моделиране и синтез на естествени езици. Честият
      избор на набори от данни базирани на Wikipedia при тези изследвания е продиктуван от няколко фактора:
      <ul>
        <li>
          Данните са достъпни под два лиценза - "GNU Free Documentation License" и "Creative Commons Attribution-ShareAlike".
          И двата позволяват употребата и разпространението на данните с много малко ограничения.
        </li>
        <li>
          Данните имат относително високо качество, често с множество автори и редактори за всяка от статиите.
        </li>
        <li>
          Данните са взети от файл, чийто оригинален размер е 4.8 GiB - над 2 500 000 страници текст, при 2000
          символа на страница - безспорно значително количество текст.
        </li>
      </ul>
    </p>

    <p>
      Трябва да се отбележи, че данните са във вида, в който са съществували в Wikipedia на 3 март 2006. Към момента
      обемът на Wikipedia е над 4 пъти по-голям. Разумно е и да спекулираме, че качеството на статиите се е повишило.
      Но въпреки това, enwikX не се обновяват &ndash; това гарантира консистентност на изследванията във времето.
    </p>

    <h3>4.1 MediaWiki page export format</h3>

    <p>
      Преди започване на изследване на данните, е добре да се разгледа репрезентацията им. Вече беше споменато, че
      наборът от данни е текстов, но не беше уточнен конкретен формат и съдържание. Те ще бъдат предмет на интерес
      занапред в дипломната работа и си заслужава да бъдат разгледани по-подробно. Първите 15 реда могат да бъдат
      получени с Unix командата "<i>head -n 15 enwik9</i>":

      <pre style="font-size: 0.6rem;">
        <code class="language-xml">
&lt;mediawiki xmlns=&quot;http://www.mediawiki.org/xml/export-0.3/&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd&quot; version=&quot;0.3&quot; xml:lang=&quot;en&quot;&gt;
&lt;siteinfo&gt;
  &lt;sitename&gt;Wikipedia&lt;/sitename&gt;
  &lt;base&gt;http://en.wikipedia.org/wiki/Main_Page&lt;/base&gt;
  &lt;generator&gt;MediaWiki 1.6alpha&lt;/generator&gt;
  &lt;case&gt;first-letter&lt;/case&gt;
    &lt;namespaces&gt;
    &lt;namespace key=&quot;-2&quot;&gt;Media&lt;/namespace&gt;
    &lt;namespace key=&quot;-1&quot;&gt;Special&lt;/namespace&gt;
    &lt;namespace key=&quot;0&quot; /&gt;
    &lt;namespace key=&quot;1&quot;&gt;Talk&lt;/namespace&gt;
    &lt;namespace key=&quot;2&quot;&gt;User&lt;/namespace&gt;
    &lt;namespace key=&quot;3&quot;&gt;User talk&lt;/namespace&gt;
    &lt;namespace key=&quot;4&quot;&gt;Wikipedia&lt;/namespace&gt;
    &lt;namespace key=&quot;5&quot;&gt;Wikipedia talk&lt;/namespace&gt;
        </code>
      </pre>

      Моментално е видно, че става въпрос за XML-базиран формат. Форматът се нарича <i>MediaWiki page export format</i>,
      описан е в XML схема<a href="#reference-media-wiki-xml-xsd"><sup>[x]</sup></a>, и съдържа статии, метаданни за
      статиите, както и общи метаданни. Откъсът по-горе описва общи метаданни, и по-специално в случая &ndash; името на
      сайта, адреса му, версията на използвания формат, както и част от основните секции в Wikipedia.
    </p>

    <p>
      Трябва да бъде отбелязано, че никой от <i>enwikX</i> наборите от данни не представлява валиден XML документ.
      Причината е, че всеки от тях съдържа определено количество данни, започващи от началото на валиден документ
      от тип MediaWiki page export format. Взимането на съдържание единствено от началото и изрязването му от края
      води до незатворени XML етикети. Това от своя страна неизбежно води до споменатата липса на валидност.
    </p>

    <p>
      По-напред в съдържанието се намират данни и за самите статии. Например - редове от 120 до 140 могат да бъдат
      разгледани чрез "<i>head -n 120 enwik9 | tail -n 20</i>"

      <pre style="font-size: 0.6rem;">
        <code class="language-xml">
&lt;/revision&gt;
&lt;/page&gt;
&lt;page&gt;
  &lt;title&gt;AdA&lt;/title&gt;
  &lt;id&gt;11&lt;/id&gt;
  &lt;revision&gt;
    &lt;id&gt;15898946&lt;/id&gt;
    &lt;timestamp&gt;2002-09-22T16:02:58Z&lt;/timestamp&gt;
    &lt;contributor&gt;
      &lt;username&gt;Andre Engels&lt;/username&gt;
      &lt;id&gt;300&lt;/id&gt;
    &lt;/contributor&gt;
    &lt;minor /&gt;
    &lt;text xml:space=&quot;preserve&quot;&gt;#REDIRECT [[Ada programming language]]&lt;/text&gt;
  &lt;/revision&gt;
&lt;/page&gt;
&lt;page&gt;
  &lt;title&gt;Anarchism&lt;/title&gt;
  &lt;id&gt;12&lt;/id&gt;
  &lt;revision&gt;
        </code>
      </pre>

      Това е статия за програмния език Ада. Като метаданни могат да бъдат открити името на статията и уникалният ѝ числов
      идентификатор. Прави впечатление XML етикетът <i>&lt;revision&gt;</i>. Причината за съществуването му е, че Wikipedia
      поддържа история от редакциите на всяка статия, наричани ревизии. В <i>enwikX</i> наборите от данни
      винаги присъства единствено последната ревизия. Това е и мястото, на което могат да бъдат открити някои от
      най-важните данни - уникален числов идентификатор на ревизията, време на създаване, автор, индикация дали
      редакцията е малка, коментар (липсващ тук), и текст на статията.
    </p>

    <h3>4.2 Wikitext</h3>

    <p>
      В примера по-горе, текстът на примерната статия е "#REDIRECT [[Ada programming language]]". Очевидно пунктуацията
      и специалните символи са повече, отколкото бихме очаквали от текст на естествен език. Причината е, че текстът е
      във формат Wikitext&mdash;маркиращ език, специално създаден за целите на Wikipedia.
    </p>

    <p>
      Форматът позволява декларирането на секции, подсекции, връзки към други статии и техни секции, автоматично
      пренасочване към други статии и техни секции, влагане на изображения, списъци с подточки, таблици и други.
      Между редове 135 и 140, например, може да бъде открито:
    </p>

    <pre style="font-size: 0.6rem;">
      <code class="language-wiki">
The word '''anarchism''' is [[etymology|derived from]] the [[Greek language|Greek]] ''[[Wiktionary:&amp;amp;#945;&amp;amp;#957;&amp;amp;#945;&amp;amp;#961;&amp;amp;#967;&amp;amp;#943;&amp;amp;#945;|&amp;amp;#945;&amp;amp;#957;&amp;amp;#945;&amp;amp;#961;&amp;amp;#967;&amp;amp;#943;&amp;amp;#945;]]'' (&amp;quot;without [[archon]]s (ruler, chief, king)&amp;quot;). Anarchism as a [[political philosophy]], is the belief that ''rulers'' are unnecessary and should be abolished, although there are differing interpretations of what this means. Anarchism also refers to related [[social movement]]s) that advocate the elimination of authoritarian institutions, particularly the [[state]].&amp;lt;ref&amp;gt;[http://en.wikiquote.org/wiki/Definitions_of_anarchism Definitions of anarchism] on Wikiquote, accessed 2006&amp;lt;/ref&amp;gt; The word &amp;quot;[[anarchy]],&amp;quot; as most anarchists use it, does not imply [[chaos]], [[nihilism]], or [[anomie]], but rather a harmonious [[anti-authoritarian]] society. In place of what are regarded as authoritarian political structures and coercive economic institutions, anarchists advocate social relations based upon [[voluntary association]] of autonomous individuals, [[mutual aid]], and [[self-governance]].

While anarchism is most easily defined by what it is against, anarchists also offer positive visions of what they believe to be a truly free society. However, ideas about how an anarchist society might work vary considerably, especially with respect to economics; there is also disagreement about how a free society might be brought about.
      </code>
    </pre>

    <p>
      Откъсът съдържа примери за връзки към други статии и техни секции (оградени с двойни квадратни скоби), удебелен
      текст (тройки кавички) и други. Присъствието на толкова много специални символи, поставя под въпрос
      съпоставимостта с базовата оценка от между 0.6 и 1.3 бита за символ, която Шанън дава.
      Оценката, както вече беше споменато, е за азбука от 27 символа, която игнорира числа, пунктуация, големи и малки букви
      и други възможно специални или служебни символи. Това поставя въпроса - какво може да бъде видяно в данните?
    </p>

    <h3>4.3 Изследване на данните</h3>

    <p>
      Изследването на данните е стандартен подход и цели първоначално запознаване с данните и постигане на определено
      ниво на разбиране за предметната област. Търсят се проблеми в данните, обръща се внимание на обема им, на
      присъствието на каквито и да е зависимости, статистически разпределения или скрита структура. Идентифицират се и
      части от тях, които биха представлявали интерес при по-обстойно изследване. Всичко това се постига с поредица
      кратки програми и с помощта на визуални репрезентации<a href="#reference-data-exploration"><sup>[x]</sup></a>.
    </p>

    <p>
      Разглеждайки <i>enwikX</i> наборите от данни, както и тяхната XML схема, може да се достигне до списък от имена на
      полета с метаданни. В търсенето на подход за компресия, от значение са типовете на въпросните метаданни. Чрез
      кратка помощна програма са изчислени и приложени сумарните размерите на данните, разбити по поле. В размера са
      включени XML етикети, XML атрибути, както и водещите интервали преди тях.
    </p>

    <ul class="dataset-fields">
      <li>
        <b>Общи метаданни</b>:<br>
        Метаданни, които не са конкретни за определена статия, а общи за набора от данни.<br>
        Размер в байтове: 1&nbsp;403
      </li>
      <li>
        <b>Заглавие на статия</b><br>
        Текстов низ, съдържащ заглавието.<br>
        Размер в байтове: 9&nbsp;221&nbsp;250
        <!-- 4352710 (за самите данни) + 243427 * 20 (за XML) -->
      </li>
      <li>
        <b>Ограничения върху статията</b><br>
        Текстов низ в специфичен формат, описващ забрани за редактиране, преименуване и други.<br>
        Размер в байтове: 46&nbsp;572
      </li>
      <li>
        <b>Идентификатор на статия</b><br>
        Число, уникално идентифициращо статия.<br>
        Размер в байтове: 4&nbsp;782&nbsp;069
        <!-- ... (за самите данни) + 243427 * 14 (за XML) -->
      </li>
      <li>
        <b>Идентификатор на ревизия</b><br>
        Число, уникално идентифициращо ревизия на статия.<br>
        Размер в байтове: 5&nbsp;842&nbsp;240
      </li>
      <li>
        <b>Дата на създаване на ревизия</b><br>
        Текстов низ във формат ISO 8601, отбелязващ дата и време на създаване на ревизията.<br>
        Размер в байтове: 12&nbsp;171&nbsp;350
        <!-- 887881909 (за самите данни) + 243427 * 41 (за XML) -->
      </li>
      <li>
        <b>Автор на ревизия</b><br>
        Вложен XML елемент, съдържащ или потребителско име и идентификатор на автор, или низ с IP адреса му.<br>
        Размер в байтове: 23&nbsp;850&nbsp;049
        <!-- 1048139 + 1762812 + 205398 * (20 + 30 + 18 + 21) + 515024 + 38028 * (20 + 18 + 21) -->
      </li>
      <li>
        <b>Флаг за малка ревизия</b><br>
        Празен XML елемент, чието присъствие индикира, че промяната в ревизията е малка.<br>
        Размер в байтове: 1&nbsp;581&nbsp;072
      </li>
      <li>
        <b>Коментар към ревизия</b><br>
        Текстов низ, съдържащ коментари към ревизията.<br>
        Размер в байтове: 5&nbsp;582&nbsp;279
      </li>
      <li>
        <b>Текст за ревизията</b><br>
        Текстов низ, с текста на ревизията на статията.<br>
        Размер в байтове: 898&nbsp;105&nbsp;843
        <!-- 887881909 (за самите данни) + 243427 * 41 (за XML) -->
      </li>
    </ul>

    <p>
      Оставащото място е заето от отварящи и затварящи XML етикети, отнасящи се за всяка от статиите.
    </p>

    <p>
      Естествено е опитите за компресия да бъдат концентрирани върху частта от данните, която заема най-много място.
      Не е изненадващо, че огромната част от данните са в текстовете на статиите. Относително голямо място заемат и
      метаданните за авторите. Трети по размер са датите на създаване. Четвърти&mdash;заглавията на статиите.
      Тези, най-големи по обем, части от данните се изследват допълнително.
    </p>

    <h4 style="clear: both; padding-top: 30px;">4.3.1 Статии</h4>

    <p>
      Общият брой статии е 243&nbsp;427. След премахването на XML етикетите и атрибутите остава текст с размер
      888&nbsp;134&nbsp;586 байта. След декодирането на UTF-8, в който е записан текстът, броят символи става
      885&nbsp;671&nbsp;734. Средният брой символи за статия е приблизително 3&nbsp;638.35, Максималният e 738&nbsp;818,
      а минималният е 0. При работа с данните трябва да се обърне внимание на въпросните празни статии, като бъдат
      пропуснати или обработени по по-специфичен начин.
    </p>

    <div>
      <div style="float: left; margin: 10px 30px 0 0;">
        <img src="article-lengths.png">
        <i style="display: block; padding-left: 30px;">(фиг. 1)</i>
        <img src="article-lengths-log.png">
        <i style="display: block; padding-left: 30px;">(фиг. 2)</i>
      </div>

      <p>
        На <i>фиг. 1</i> по хоризонтала са изобразени възможните дължини на статии, а по вертикала - броят статии със
        съответната дължина. Причината графиката на пръв поглед да изглежда празна, е че кривата плътно следва
        абсцисната и ординатната оси. Това ни подсказва, че съществуват малък брой много дълги статии, но огромната част
        от статиите са относително кратки.
      </p>

      <p>
        Данните придобиват малко повече смисъл, ако изобразим възможните дължини на статии по логаритмична скала
        (<i>фиг. 2</i>). Тогава кривата започва да прилича на нормално разпределение. Това подсказва за
        доближаване до логнормално разпределение (разпределение на Галтон) в дължините на статиите.
      </p>

      <p>
        Наблюдава се мода на разпределението при около 30 символа за статия. Освен това повечето статии имат
        под 100 символа. Това подсказва, че по-късно, при формирането на партиди от данни, дължини над 100
        би трябвало да са достатъчни, за да бъде трениран моделът с цялостния текст на повечето статии.
      </p>
    </div>

    <h4 style="clear: both; padding-top: 30px;">4.3.2 Възможни символи</h4>

    <p>
      Броят символи, които се срещат в текстовете на статиите, е 10&nbsp;797. Във <i>фиг. 3</i> е представена честотата
      на срещане на всеки символ. Най-вляво по хоризонтала са изобразени най-често срещаните символи, а
      най-вдясно&mdash;най-рядко срещаните, като скалата е логаритмична (т.е. на числото <i>n</i> отговаря n-тия
      най-често срещан символ). По вертикала е изобразен броят срещания за всеки символ&mdash;отново върху
      логаритмична скала.
    </p>

    <div style="float: left; margin: 10px 30px 10px 0;">
      <img src="chars-density.png">
      <i style="display: block; padding-left: 30px;">(фиг. 3)</i>
    </div>

    <p>
      За такъв тип данни очакваме да важи емпирично установеният закон на Ципф<a href="#reference-zipfs-law"><sup>[x]</sup></a>.
      Върху логаритмично-логаритмична графика това би изглеждало като права линия от най-често срещания символ (горе-ляво)
      до най-рядко срещания (долу-дясно). Откриваме интересно отклонение в рамките на първите 100 символа. Освен това
      е интересно, че ако разгледаме единствено 10-те най-често срещани символа, те изпълняват закона на Ципф.
      С известно отклонение същото може да се каже и за първите 90 символа.
    </p>

    <p>
      Първоначалната интуиция на автора е, че първите 10 символа изпъкват, защото са част от Wikitext синтаксиса.
      При проверка се оказва, че символите са: интервал и буквите <i>e</i>, <i>a</i>, <i>t</i>, <i>i</i>, <i>o</i>, <i>n</i>, <i>r</i>, <i>s</i>, <i>l</i>.
      Интуицията е невярна. Но в рамките на следващите 90 символа могат да бъдат видени символите от Wikitext синтаксиса.
      Също така там се среща цялата латиница, всички десетични цифри, много пунктуационни знаци и кирилската буква <i>и</i>.
    </p>

    <div>
      <div style="float: left; margin: 10px 30px 10px 0;">
        <img src="chars-distribution.png">
        <i style="display: block; padding-left: 30px;">(фиг. 4)</i>
      </div>
    </div>

    <p>
      Интересен е и друг прочит на данните. Графиката във <i>фиг. 4</i> изброява по хоризонтала символите по честота на
      срещане&mdash;подобно на <i>фиг. 3</i>, но този път по линейна скала. За всеки символ по вертикала са разположени
      сумата от броя срещанията на въпросния символ, както и броят срещания на всички по-рядко срещани от него символи.
      Така <i>фиг. 4</i> до известна степен прилича на графика на дискретното кумулативно разпределение, но ротирано
      около ординатната ос. Изобразени по този начин, данните могат да отговорят на въпроса:
    </p>

    <div style="text-align: center;">
      ако се вземе подмножеството от <i>n</i> на брой най-често срещани символа,<br>
      каква част от символите в набора от данни<br>
      ще бъдат извън въпросното множество?
    </div>

    Например:
    <ul>
      <li>
        ако се вземат първите 2&nbsp;000 символа, ще останат приблизително 10<sup>5</sup> символа в набора от данни,
        които не са сред тези 2&nbsp;000 символа;
      </li>
      <li>Ако се вземат 6&nbsp;000 символа, ще останат приблизително 10<sup>4</sup>;</li>
      <li>При 10&nbsp;000 символа, ще останат приблизително 10<sup>3</sup>.</li>
    </ul>

    В граничните случаи:
    <ul>
      <li>ако се вземат всички символи, ще останат 0 невзети (долу-дясно в графиката);</li>
      <li>ако не се вземат никакви символи, всички ще останат невзети (горе-ляво в графиката).</li>
    </ul>

    <p>
      Тази интерпретация намира приложение в глава "9.1 Брой на категориите".
    </p>

    <h4 style="clear: both; padding-top: 30px;">4.3.3 Малки и големи букви</h4>

    <p>
      Интересен е един от подходите, приложен в алгоритъма <i>nncp</i><a href="#reference-nncp"><sup>[x]</sup></a>. Във въпросния
      алгоритъм Фабрис Белар използва предварителна обработка на текстови файлове, директно взета от имплементацията
      на алгоритъма <i>cmix</i><a href="#reference-cmix"><sup>[x]</sup></a>. Една от стъпките е всички букви да бъдат превърнати
      в малки.
    </p>

    <p>
      За брой на символите се получава 10&nbsp;333. Това е само с 464 по-малко от броя на всички символи. Изглежда огромната
      част от символите нямат съответстваща малка буква. Това може да бъде обяснено както с пунктуационни или специални
      символи, така и със символи от писмености които не разграничават малки от големи букви (напр. много източно
      азиатски писмености). Но в случая чистата бройка не е от такова значение. По-голям интерес представляват
      разпределението и плътността.
    </p>

    <div style="display: inline-flex;">
      <div style="float: left;">
        <img src="lower-chars-density.png">
        <i style="display: block; padding-left: 30px;">(фиг. 5)</i>
      </div>
      <div style="float: left; margin-top: 3px;">
        <img src="lower-chars-distribution.png">
        <i style="display: block; padding-left: 30px;">(фиг. 6)</i>
      </div>
    </div>

    <p>
      На <i>фиг. 5</i> и <i>фиг. 6</i> са изобразени графики аналогични съответно с <i>фиг. 3</i> и <i>фиг. 4</i>.
      Със син цвят са изобразени данните от <i>фиг. 3</i> и <i>фиг. 4</i>, а в оранжев цвят са наложени данните получени
      при преобразувани главни букви към малки. Двете криви се движат плътно заедно. В такъв мащаб е трудно да се
      забележи смислена разлика.
    </p>

    <div>
      <div style="float: left; margin: 10px 30px 10px 0;">
        <img src="lower-chars-distribution-zoomed.png">
        <i style="display: block; padding-left: 30px;">(фиг. 7)</i>
      </div>
    </div>

    <p>
      Разликата става по-очевидна, когато се разгледат първите 200 най-често срещани символа в двата случая.
      Кривата за тях е изобразена на <i>фиг. 7</i>. Данните са идентични с тези в горната лява част на <i>фиг. 6</i>.
    </p>

    <p>
      Ако големите букви бъдат запазени, е възможно с около 100 символа да се обхванат над 99% от всички символи в
      набора от данни. Ако бъдат взети единствено малки букви, същото може да бъде постигнато с около 70 символа.
    </p>

    <p>
      Ползите, които могат да бъдат извлечени, се разглеждат в глава "9.1 Брой на категориите".
    </p>

    <h4 style="clear: both; padding-top: 30px;">4.3.4 Заглавия</h4>

    <div>
      <div style="float: left; margin: 10px 30px 10px 0;">
        <img src="title-lengths.png">
        <i style="display: block; padding-left: 30px; padding-bottom: 30px;">(фиг. 8)</i>
        <img src="title-lengths-log.png">
        <i style="display: block; padding-left: 30px;">(фиг. 9)</i>
      </div>

      <div>
        <p>
          След премахването на XML етикетите, общата дължина на заглавията в байтове е 4&nbsp;596&nbsp;137, а в
          символи&mdash;4&nbsp;590&nbsp;258. Естествено, броят заглавия е колкото броя статии&mdash;243&nbsp;427.
          Най-дългото заглавие е 188 символа, а най-краткото&mdash;0. Подобно на текста на статиите, и при заглавията
          присъстват празни данни.
        </p>

        <p>
          На <i>фиг. 8</i> по хоризонтала са изобразени възможните дължини на заглавия, а по вертикала - броят заглавия
          със съответната дължина. По подобие на текста на статиите наблюдаваме доближаване до логнормално
          разпределение&mdash;по-добре видимо на <i>фиг. 9</i>.
        </p>

        <p>
          Броят уникални символи е 275&mdash;драстично по-малко от същата мярка при статиите. Очевидното обяснение е, че
          в английската Wikipedia има основно заглавия на английски. Докато в пълния текст на статията има препратки,
          етимологии и всевъзможни връзки към други езици.
        </p>
      </div>
    </div>

    <h4 style="clear: both; padding-top: 30px;">4.3.5 Автори</h4>
    <p>
      За всяка от статиите присъства запис с автор на ревизията. Общо - 243&nbsp;427 записа за автор. При проверка с
      кратка програма може да бъде открито, че голямата част от авторите се повтарят. Броят на уникалните записи е:
    </p>

    <ul>
      <li>19&nbsp;506 записа с потребителско име и числов идентификатор</li>
      <li>26&nbsp;258 записа с IP адрес</li>
      <li>12 записа на интервали от IP адреси, записани като IP адрес (например - "129.33.49.xxx")</li>
      <li>7 записа със служебен автор, записани като IP адрес (например - "Conversion script")</li>
    </ul>

    <p>
      Общият брой силно надвишава броя на уникалните автори&mdash;има 45&nbsp;783 уникални записа, което е приблизително
      19% от всички. С проста дедупликация може да бъде извлечена известна компресия.
    </p>

    <p>
      Това се прилага в глава "7 Предварителна обработка на данните".
    </p>

    <h4 style="clear: both; padding-top: 30px;">4.3.6 Дати на създаване</h4>

    <p>
      Както вече беше споменато, в данните се използва форматът ISO 8601, например: "2005-12-27T18:46:47Z". Времевата
      резолюция е секунди. Максималната стойност е "2006-03-04T06:14:28Z", а минималната&mdash;"2002-02-25T15:43:11Z".
    </p>

    <p>
      Тези данни се обработват в "7 Предварителна обработка на данните".
    </p>
  </div>

  <div class="pages">
    <h2>5 Hutter Prize</h2>

    <p>
      През 2006-та година, Маркъс Хътър стартира Приза на Хътър (Hutter Prize)<a href="#reference-hutter-prize">
      </a>&mdash;практическа задача за компресия на човешко знание. Самата задача променя правилата си във времето.
      Правилата, очертани в текущата итерация на Hutter Prize, са следните:

      <ul>
        <li>
          да се създаде компресираща програма, която да компресира определен набор от данни и да произведе декомпресираща програма;
        </li>
        <li>
          изпълнението на декомпресираща програма да произведе файл идентичен на оригиналния набор от данни;
        </li>
        <li>
          компресирането и декомпресирането да се изпълняват сумарно за по-малко от 100 часа, да заемат по-малко от 10GiB
          оперативна памет и по-малко от 100GiB дисково пространство;
        </li>
        <li>
          да се постигне възможно най-малка сума от размерите на компресираща и декомпресираща програми.
        </li>
      </ul>
    </p>

    <h3>5.1 Текущи най-добри резултати</h3>

    <p>
      В текущия си вид, Hutter Prize има единствен победител&mdash;Александър Ратушняк на 4 юли 2019 с алгоритъма
      <i>phda9</i> (версия 1.8). Достигнатият размер е 116&nbsp;673&nbsp;681 байта<a href="#reference-hutter-prize"><sup>[x]</sup></a>.
    </p>

    <p>
      В предишни итерации на състезанието, когато използваният набор от данни е <i>enwik8</i>, победители са:
    </p>

    <ul>
      <li>4 ноември 2017 - Александър Ратушняк. Размер&mdash;15&nbsp;284&nbsp;944 байта, достигнат с алгоритъма <i>phda9</i>;</li>
      <li>23 май 2009 - Александър Ратушняк. Размер&mdash;15&nbsp;949&nbsp;688 байта, достигнат с алгоритъма <i>decomp8</i>;</li>
      <li>14 май 2007 - Александър Ратушняк. Размер&mdash;16&nbsp;481&nbsp;655 байта, достигнат с алгоритъма <i>paq8hp12</i>;</li>
      <li>25 септември 2006 - Александър Ратушняк. Размер&mdash;байта, 17&nbsp;073&nbsp;018 достигнат с алгоритъма <i>paq8hp5</i>;</li>
      <li>24 март 2006 - Мат Махони. Размер&mdash;18&nbsp;324&nbsp;887 байта, достигнат с алгоритъма <i>paq8f</i>.</li>
    </ul>

    <p>
      Всички тези алгоритми използват модел за предсказване на вероятности. Вероятностите се използват като входни данни
      на алгоритъм за ентропийно кодиране. Това е стандартен подход, използван и в тази дипломна работа. Повече
      имплементационни детайли са описани в глава "6 Предложен подход за компресия на наборите от данни enwikX".
    </p>

    <p>
      Друго общо за алгоритмите е, че спадат в категорията на подходи за компресия със смесване на контексти.
      При тях има няколко статистически модела, които дават независими предсказвания за следващи категории. Категориите
      могат да бъдат байтове или символи, но обикновено се работи на ниво битове. Вероятностите получени от всеки от
      моделите се смесват по определен начин и се получават окончателни вероятности за всяка
      категория<a href="#reference-context-mixing"></a>.
    </p>

    <p>
      Интересен е и алгоритъмът <i>cmix</i>, който постига размер 14&nbsp;838&nbsp;332 байта за <i>enwik8</i> и
      115&nbsp;714&nbsp;367 байта за <i>enwik9</i>. С такъв резултат <i>cmix</i> би бил първи, но не се вмества в
      изискванията за време и памет&mdash;за <i>enwik9</i> са му нужни приблизително 334 часа и приблизително 25GiB
      оперативна памет<a href="#reference-compression-benchmark"></a>.
      Подходът при него отново е смесване на контексти, но един от няколкото модела, които участват, е LSTM рекурентна
      невронна мрежа.
    </p>

    </p>
      Алгоритъм, който вече беше споменат в друг контекст е <i>nncp</i>. Алгоритъмът отново е комбинация от предсказващ
      модел и ентропийно кодиране. Постигнатият резултат е 119&nbsp;167&nbsp;224 байта<a href="#reference-compression-benchmark"></a>.
      Причината да представлява интерес е, че за модел се използва единствено LSTM рекурентна невронна мрежа.
      И въпреки че не постига най-добър резултат, той все пак успява да се доближи до него, и демонстрира приложимостта
      на подхода.
    </p>

    <!--
      Паралел на смесването на контексти, който заслужава да се спомене, са ансамболовите методи за машинно
      самообучение и хибридните препоръчващи системи. При тях отново има множество модели, чиито резултати се mix-ират.
      Изследвано е, че те често получават по-добри резултати от единствен модел:
        - https://arxiv.org/pdf/1901.03888
        - https://arxiv.org/abs/1106.0257
        - https://dl.acm.org/doi/10.1109/TKDE.2005.99
      Възможността за mix-ирането на предложения подход с други модели е интересна възможност, но остава извън целите
      на тази дипломна работа.
    -->

    <h3>5.2 Забележки относно правилата</h3>

    <p>
      За да бъде предложеният подход за компресия съвместим правилата на Приза на Хътър, трябва преди всичко да
      имплементира компресия без загуби. Доброто представяне във времето също е фактор, който трябва да
      се вземе предвид. Но това не е първостепенна цел. В голямата част от случаите, ще бъде счетено, че 100 часа
      време за изпълнение са напълно достатъчни, без да се навлиза в изчисления и оценки на предложения подход.
      Времето за изпълнение се споменава изрично като проблем, единствено когато проблем е установен в резултат на
      практически експерименти.
    </p>

    <p>
      От първостепенно значение остават размерите на компресиращата и декомпресираща програми. Но за да бъде давана
      оценка на размерите, първо трябва да бъде предложен конкретен подход.
    </p>

  </div>

  <div class="pages">
    <h2>6 Предложен подход за компресия на наборите от данни <i>enwikX</i></h2>

    <p>
      Построените предсказващи модели дават оценки на вероятностите за възможни следващи символи в набора от данни.
      Компресия се постига чрез ентропийно кодиране, като на символи с по-висока вероятност се
      съпоставят по-кратки последователности от битове, а на по-малко вероятни&mdash;по-дълги. Разглеждат се кодиране
      на Хъфман и аритметично кодиране, както и отражението им върху обработката на свойства, което прилагаме.
    </p>

    <p>
      За целите на компресията, всеки символ от азбуката на възможни символи се разглежда като категория.
      А цялостният текст се разглежда като дискретна последователност от категории.
      Компресирането и декомпресирането на тази дискретна последователност от категории се случва чрез ентропийно кодиране.
      Съществуват различни алгоритми за ентропийно кодиране, като някои примери са кодиране на Шанън<a href="#reference-shannon-coding"></a>,
      кодиране на Шанън-Фано<a href="#reference-shannon-fano-coding"></a>, кодиране на Хъфман<a href="#reference-huffman-coding"></a>,
      аритметично кодиране<a href="#reference-arithmetic-coding-1"></a><a href="#reference-arithmetic-coding-2"></a>,
      интервално кодиране<a href="#reference-range-coding"></a> и други.
    </p>

    <p>
      Общото за ентропийните кодирания е, че съпоставят по-кратки последователности от битове на по-вероятни символи, а
      на по-малко вероятни&mdash;по-дълги. За да се реализира ентропийно кодиране е необходим вероятностен модел и има
      много различни начина да се определи такъв&mdash;вериги на Марков, скрити модели на Марков, n-грамни модели и т.н.
      В тази дипломна работа изследваме построяването на модел, използващ рекурентни невронни мрежи.
      Те са избрани заради способността им да запомнят зависимости, както краткосрочно, така и
      дългосрочно<a href="#reference-lstm"></a><a href="#reference-gru"></a>, както и да
      предсказват категории, отчитайки контекст<a href="#reference-unreasonable-rnn-effectiveness"></a>.
    </p>

    <p>
      В допълнение на това, описаният по-нагоре подход се прилага единствено за заглавието и текстовото съдържание на
      всяка от статиите. За останалите полета от XML набора от данни (описани в "4.3 Изследване на данните"),
      се прилага отделна обработка, описана в "7 Предварителна обработка на данните".
    </p>

    <p>
      По описания начин получаваме следния алгоритъм за компресия:<br>
      <ul>
        <li>Обработва се XML набора от данни, извличат се полетата, и се отделят заглавието и текстовото съдържание от метаданните.</li>
        <li>Метаданните преминават през отделна обработка и се записват в декомпресиращата програма</li>
        <li>Заглавието и текстовото съдържание се конкатенират, а след това, от тях се получава дискретна последователност от категории.</li>
        <li>Категории се оформят в партиди от данни и се използват за трениране на рекурентна невронна мрежа</li>
        <li>Полученият предсказващ модел се записва в декомпресиращата програма</li>
        <li>
          За всяка от категориите в дискретна последователност:
          <ul>
            <li>Подаваме текущата категория на модела</li>
            <li>Взимаме двойките категория-вероятност, които моделът предсказва</li>
            <li>Подаваме текущата категория и двойките категория-вероятност на избраното ентропийно кодиране</li>
            <li>В декомпресиращата програма записваме низа от битове, върнати от ентропийното кодиране</li>
            <li>Повтаряме докато има оставащи категории</li>
          </ul>
        </li>
      </ul>
    </p>

    <p>
      Трябва да се отбележи, че никъде в компресирания файл не присъстват вероятностите, нужни за ентропийно кодиране.
      Вместо това те се изчисляват от модела на всяка стъпка.
    </p>

    <p>
      Получаваме и следния алгоритъм за декомпресия:<br>
      <ul>
        <li>Декомпресиращата програма изчита поредсказващия модел.</li>
        <li>
          За всеки от битовете, получени при ентропийно кодиране:
          <ul>
            <li>Взимаме двойките категория-вероятност, които моделът предсказва</li>
            <li>Четем битове докато получим еднозначна категория (спрямо ентропийното кодиране)</li>
            <li>Запомняме получената категория в списък от категории</li>
            <li>Подаваме получената категория на модела</li>
            <li>Повтаряме докато има оставащи битове</li>
          </ul>
        </li>
        <li>Декомпресиращата програма изчита метаданните.</li>
        <li>Комбинираме метаданните и списъкът от получени категории, за да реконструираме оригиналните данни</li>
      </ul>
    </p>

    <p>
      В следващите няколко глави ще бъдат разгледани отделните аспекти на така описания подход и отражението им върху
      крайната компресия:
    </p>

    <ol>
      <li>
        предварителната обработка на метаданните;
      </li>
      <li>
        размер на модела;
      </li>
      <li>
        точността на предсказвания на модела и информационната ентропията, която ни дава;
      </li>
      <li>
        практическата ентропийна компресия (ненадвишаваща информационната ентропията), която достигаме.
      </li>
    </ol>
  </div>

  <div class="pages">
    <h2>7 Предварителна обработка на данните</h2>

    <p>
      Целта на тази част от компресията е да се отделят голямата част от полетата от набора от данни и за обучението на
      модела да се оставят единствено заглавието и текстовото съдържание от метаданните.
    </p>

    <h3>7.1 Предварителна обработка на метаданни</h3>

    <p>
      За обработка на метаданни за статии, трябва XML набора от данни първо да премине през синтактичен анализ.
      Голяма част от данните са повтарящи се имена на етикети и атрибути, които имат твърда структура.
      Достатъчно е да се запази реда на полетата, за да се възстанови оригиналното XML съдържание в декомпресиращата програма.
      По този начин остават единствено данните в полетата, описани в глава "4.3 Изследване на данните".
      Те могат грубо да бъдат интерпретирани като матрица във вид:

      <table class="dataset-fields-table">
        <tr>
          <td></td>
          <td>заглавия</td>
          <td>идентификатори</td>
          <td>...</td>
          <td>автори</td>
          <td>коментари</td>
          <td>текстове</td>
        </tr>
        <tr>
          <td>статия 1</td>
          <td>заглавие 1</td>
          <td>идентификатор 1</td>
          <td>...</td>
          <td>автор 1</td>
          <td>коментар 1</td>
          <td>текст 1</td>
        </tr>
        <tr>
          <td>статия 2</td>
          <td>заглавие 2</td>
          <td>идентификатор 2</td>
          <td>...</td>
          <td>автор 2</td>
          <td>коментар 2</td>
          <td>текст 2</td>
        </tr>
        <tr>
          <td></td>
          <td colspan="6">...</td>
        </tr>
        <tr>
          <td>статия n</td>
          <td>заглавие n</td>
          <td>идентификатор n</td>
          <td>...</td>
          <td>автор n</td>
          <td>коментар n</td>
          <td>текст n</td>
        </tr>
        <tr>
          <td></td>
          <td colspan="6">...</td>
        </tr>
      </table>

      Обхождайки тези полета, ги срещаме първо от ляво надясно по колони, а след това от горе надолу по редове.
      Интересно е да транспонираме тази матрица до вида:

      <table class="dataset-fields-table">
        <tr>
          <td></td>
          <td>статия 1</td>
          <td>статия 2</td>
          <td>...</td>
          <td>статия n</td>
          <td>...</td>
        </tr>
        <tr>
          <td>заглавия</td>
          <td>заглавие 1</td>
          <td>заглавие 2</td>
          <td>...</td>
          <td>заглавие n</td>
          <td>...</td>
        </tr>
        <tr>
          <td>идентификатори</td>
          <td>идентификатор 1</td>
          <td>идентификатор 2</td>
          <td>...</td>
          <td>идентификатор n</td>
          <td>...</td>
        </tr>
        <tr>
          <td></td>
          <td colspan="2"></td>
          <td>...</td>
          <td></td>
          <td>...</td>
        </tr>
        <tr>
          <td>автори</td>
          <td>автор 1</td>
          <td>автор 2</td>
          <td>...</td>
          <td>автор n</td>
          <td>...</td>
        </tr>
        <tr>
          <td>коментари</td>
          <td>коментар 1</td>
          <td>коментар 2</td>
          <td>...</td>
          <td>коментар n</td>
          <td>...</td>
        </tr>
        <tr>
          <td>текстове</td>
          <td>текст 1</td>
          <td>текст 2</td>
          <td>...</td>
          <td>текст n</td>
          <td>...</td>
        </tr>
      </table>

      Така еднотипни данни ще бъдат групирани заедно. На практика често ще има и последователности от нарастващи числа,
      числа с близки стойности, както и подобни текстове. Групирането на подобни стойности заедно е един от известните
      начини за постигане на компресия. Това е изследвано и обосновано от алгоритми като преобразуванието на
      Бъроуз-Уилър<a href="#reference-bwt"></a><a href="#reference-bwt-theory-and-practice"></a>.
    </p>

    <p>
      <div>Забележка:</div>
      Свойство, което се губи в описания до тук подход, е възможността за компресия "в движение".
      Това се изразява във възможността да се компресира началото на данните, без да са налични всички данни до края.
      Преобразуванието на Бъроуз-Уилър постига въпросното свойство, разглеждайки данните като последователност от блокове,
      и пренареждайки данните единствено в рамките на един блок. Такъв подход би усложнил имплементация на предварителната
      обработка, но вероятно би понижил изискванията за памет. Първото считаме на нежелано, а второто&mdash;за ненужно,
      тъй като условията на Приза на Хътър по никакъв начин не налагат такова условие.
    </p>

    <p>
      Следващата стъпка е да се преобразуват възможно най-много стойности от текстови в двоични и да бъдат записани така.
      Идентификаторите на статия и ревизия са десетични числа в текстовата си репрезентация.
      За тях конвертирането към 32-битова двоична репрезентация е тривиално.
      За датите на създаване може да се използва целочислено Unix време (UNIX Epoch time)&mdash;това е
      напълно достатъчно, за да не се изгуби информация.
      Коментарите записваме като низ с нулев терминатор.
      Флагът за малка ревизия може да бъде представен като булева стойност.
      За ограничения върху статията се оказва, че има общо осем възможни стойности. Всяка от тях може да бъде
      съпоставена на число и записана по този начин.
    </p>

    <p>
      Обработката на авторите е малко по-сложна. За тях в глава "4.3 Изследване на данните" беше показано, че присъстват
      повторения в данните. Това може да бъде решено, като всяка възможна стойност се постави в множество, след това ѝ
      бъде съпоставен уникален числов индекс и накрая множеството автори и индексите бъдат записани отделно.
      Отново в глава "4.3 Изследване на данните" бяха идентифицирани и четири типа автори. Това поражда малка особеност
      при записването на множеството от автори в двоичен вид. Всеки от типовете се обработва и записва поотделно както
      следва:

      <ul>
        <li>
          <div><b>потребителски имена с числов идентификатор</b></div>
          Числовите идентификатори се като 32-битови числа, а потребителските имена като низ с нулев терминатор.
        </li><li>
          <div><b>IPv4 адреси</b> (например - "129.33.49.100")</div>
          Записват се като 32-битови числа.
        </li><li>
          <div><b>Интервали от IPv4 адреси</b> (например - "129.33.49.xxx")</div>
          Записват се като низове. Количеството им е толкова малко, че не оправдава записването им като число.
        </li><li>
          <div><b>Служебни низове</b> (например - "Conversion script")</div>
          Записват се като низове с нулев терминатор.
        </li>
      </ul>
    </p>

    <p>
      След тази обработка се получават следните размери за полета:
    </p>

    <ul>
      <li>
        <b>Ограничения върху статията</b>: 243&nbsp;426 байта
      </li>
      <li>
        <b>Идентификатор на статия</b>: 973&nbsp;704 байта
      </li>
      <li>
        <b>Идентификатор на ревизия</b>: 973&nbsp;704 байта
      </li>
      <li>
        <b>Дата на създаване на ревизия</b>: 1&nbsp;947&nbsp;408 байта
      </li>
      <li>
        <b>Автор на ревизия</b>: 2&nbsp;321&nbsp;692 байта
      </li>
      <li>
        <b>Флаг за малка ревизия</b>: 243&nbsp;426 байта
      </li>
      <li>
        <b>Коментар към ревизия</b>: 824&nbsp;129 байта
      </li>
    </ul>

    <!-- Total raw sizes: 46572 + 4782069 + 5842240 + 12171350 + 23850049 + 1581072 + 5582279 -->

    <p>
      Което дава общ размер от 7&nbsp;527&nbsp;489 байта. И компресия за изброените полета от под 14%.
    </p>

    <p>
      <div>Забележка:</div>
      За получаване на числата посочени по-горе, при записване на двоичната репрезентация се използват размери кратни
      на размера на един байт. Освен това не се правят опити за ентропийно кодиране на данните. По-нататъшни опити за
      компресия могат да доведат до размери под 3 MiB; т.е. компресия от около 5%.
    </p>

    <h3>7.2 Предварителна обработка на текст</h3>

    <p>
      След предварителната обработка на метаданните в XML набора от данни, остават за обработване заглавията и
      текстовете на статии. За всяка статия се конкатенират заглавието и текста ѝ. След това се заменят се всички
      служебни последователности от символи в XML със същинските символи, които кодират
      (напр. "<i>&amp;lt;</i>" се превръща в "<i>&lt;</i>", а "<i>&amp;amp;</i>" се превръща в "<i>&amp;</i>").
    </p>

    <p>
      Така се получават символни низове с общ размер 893&nbsp;208&nbsp;325 байта. Те се използват за формиране на
      категории, които се използват като свойства за обучаване на предсказващия модел, реализиран чрез рекурентни
      невронни мрежи.
    </p>
  </div>

  <div class="pages">
    <h2>8 Рекурентни невронни мрежи</h2>

    <p>
      През 1974 г. Уилям Литъл разглежда вид изкуствени невронни мрежи, вариант на невронни мрежи с право разпространение
      на сигнала, но със свойството да запазват някакво количество данни във вътрешно (скрито)
      състояние<a href="#reference-persistent-states-little"></a>. През 1982 г. Джон Хопфийлд изследва свойствата им за
      "генерализация, разпознаване, категоризация, коригиране на грешки и запомняне на времеви
      последователности"<a href="#reference-hopfield"></a> и популяризира техен частен случай като мрежи на Хопфийлд.
      В последствие, на базата на изследвания на Дейвид Румелхарт<a href="#reference-rumelhart"></a>, се формулира
      генералната идея за рекурентни невронни мрежи.
    </p>

    <p>
      През 1997 г. е предложен моделът LSTM<a href="#reference-lstm"></a>, който през следващите десетилетия
      получава множество подобрения и държи рекордите в широк диапазон от сфери, като например:
      <ul>
        <li>
          разпознаване на човешка реч<a href="#reference-lstm-speech-recognition-1"></a><a href="#reference-lstm-speech-recognition-2"></a><a href="#reference-lstm-speech-recognition-3"></a>;
        </li>
        <li>
          разпознаване на ръкописен текст<a href="#reference-lstm-handwritten-recognition-1"></a><a href="#reference-lstm-handwritten-recognition-2"></a> ;
        </li>
        <li>
          синтез на човешка реч<a href="#reference-lstm-tts"></a>;
        </li>
        <li>
          машинен превод<a href="#reference-lstm-machine-translation"></a>;
        </li>
        <li>
          ... и други<a href="#reference-lstm"></a>.
        </li>
      </ul>
      Чак в последните няколко години LSTM и вариантите му започват да отстъпват челните места в решенията на някои
      проблеми. Един от по-новите модели е трансформаторният (Transformer) модел<a href="#reference-transformers"></a>.
      При такъв модел се позволява обработването на текст в непоследователен ред&mdash;т.е. не е нужно думите
      (или символите) да се обработват от ляво надясно или от дясно наляво. Въпреки че този подход е обещаващ за
      обработка на естествени езици, негово приложение в тази дипломна работа няма да бъде търсено. Основна причина е
      по-лесното и елегантно имплементиране на алгоритми за компресия, когато с данните се работи в последователен ред.
    </p>

    <p>
      През 2014 г. е предложен моделът GRU<a href="#reference-lstm"></a>. Имплементацията му изисква по-малко параметри
      за същия брой неврони и така позволява по-бързо трениране. За някои задачи GRU се представя по-зле, но това
      често може да бъде компенсирано с повишаване на броя неврони, като бъде запазено преимуществото в скоростта му.
    </p>

    <p>
      В тази дипломната работа се разглеждат модели, използващи рекурентни невронни мрежи. В допълнение на рекурентните
      слоеве, се оценяват ползите от допълнителни слоеве на входа и изхода на моделите. Общият вид, който се изследва, е:
    </p>

    <ul>
      <li>
        <div><b>Слой за влагане (embedding layer)</b></div>
        <p style="margin-top: 0;">
          При кодиране на категории, с цел употребата им в невронна мрежа, стандартно се използва унитарен код
          (one-hot encoding). Това означава, че при брой на категориите <i>N</i>, на всяка категория може да се гледа
          като на базов вектор в пространство с размерност <i>N</i>. Слоят за влагане има за цел да съпостави тези
          вектори на вектори в пространство с размерност <i>M</i>, където <i>M≪N</i>. Така се позволява на следващите
          слоеве да използват много по-малко неврони и съответно да изискват много по-малко параметри. Целта е
          постигането на по-малки модели и по-бързо трениране<a href="#reference-word-embeddings-1"></a>.
        </p>

        <p>
          В практиката често се използват такива слоеве, които предварително са тренирани на огромен обем данни. Това
          от една страна позволява пестене на време при трениране; от друга&mdash;позволява постигането на добри
          резултати при употребата на относително малки набори от данни<a href="#reference-word-embeddings-2"></a>.
          В тази дипломна работа няма да се разчита на предварително трениран слой за влагане.
        </p>

        <p>
          Обикновено този подход се прилага при използването на думи като категории. В тази дипломна работа се изследва
          ползата от присъствието на такъв слой при работа със символни и n-грамни категории. Изследват се и резултатите,
          получени при различни размери на слоя.
        </p>
      </li>
      <li>
        <div><b>Рекурентни слоеве (recurrent layers)</b></div>
        <p style="margin-top: 0;">
          Един или повече слоеве с рекурентни неврони. Сравняват се LSTM и GRU слоеве. Изследват се ползите от различен
          брой слоеве, както и от различен брой неврони във всеки слой.
        </p>
      </li>
      <li>
        <div><b>Напълно свързан слой (dense layer)</b></div>
        <p style="margin-top: 0;">
          Често се използва в практиката при по-сложени модели. Позволява използването на по-малък брой неврони на
          изхода на последния рекурентен слой. Без такъв слой изходът на рекурентния слой трябва да бъде с размерност
          равна на броя категории.
        </p>

        <p>
          Размерът му не подлежи на директна параметризация, а е функция на броя категории в набора от данни и размера
          на последния рекурентен слой. Поради тази причина се изследва поведението на модела единствено със и без слоя.
        </p>
      </li>
    </ul>

    <h3>8.1 Класически подход и проблеми</h3>

    <p>
      Тренирането на рекурентна невронна мрежа не се различава съществено от тренирането на всяка друга невронна мрежа.
      Моделът притежава голям брой параметри, описващи теглата на връзките между невроните.
      На базата на голям набор от данни, се изчислява предварително избрана оценяваща функция при съществуващите
      параметри. След това, чрез метода на обратно разпространение на грешките (back propagation), се изчисляват нови
      стойности за параметрите.
    </p>

    <p>
      Като особеност при обучението на рекурентни невронни мрежи може да се посочи необходимостта от запазване на
      последователностите на свойствата при формиране на партиди от данни. Тази необходимост е продиктувана
      заради скритото (вътрешно) състояние на невронните слоеве, целящо да помогне за научаването на зависимости във
      въпросните последователности. Но единственото отражение, което това изискване оказва, е върху практическата
      реализация.
    </p>

    <p>
      В крайна сметка резултатът от обучението е голямо количество параметри във вид на числа с плаваща запетая.
      Трябва да се отбележи, че с нарастването на броя неврони, количеството на параметрите расте приблизително квадратично.
      Причината е, че параметрите отговарят на преходи между слоеве, а два слоя със съответно <i>n</i> и <i>m</i>
      неврона са свързани с <i>n&times;m</i> прехода. Което неизбежно води до модел, който е по-голям.
      А в глава "6 Предложен подход за компресия на наборите от данни <i>enwikX</i>" размерът на модела беше
      идентифициран като един от четири начина да се намали размера на компресирания файл.
    </p>

    <p>
      За да бъде добита представа за практическото отражение на размера на модела върху размера на резултатната
      компресия, могат да бъдат разгледани някои от водещите модели за обработка на естествени езици. Текущите най-добри
      модели, използващи рекурентни невронни мрежи, и тренирани върху <i>enwik9</i>, имат между 40 и 100 милиона
      параметъра<a href="#reference-nlp-progress"></a>. Ако се предположи, за параметрите се използват числа с плаваща
      запетая и единична точност, това би дало размер от 4 байта за всеки параметър. Това означава, че
      размерът на модела, който трябва да присъства в декомпресиращата програма, е между 160 MiB и 400 MiB. Дори без да
      се отчете допълнителната памет, нужна за ентропийно кодиране, това е много повече от целения резултат.
    </p>

    <p>
      Трябва да бъде търсен подход за намаляване на размера на модела.
    </p>

    <h3>8.2 Обработка на свойства (feature engineering)</h3>

    <p>
      Обработката на свойства е подход, необходим за много от алгоритмите за машинно самообучение, при който се изучава
      наборът от данни и на ръка се избират, изхвърлят или манипулират свойства, които се използват при обучението на модел.
      Този подход обикновено се заобиколя при невронните мрежи, като се заменя с автоматизираното извличане на свойства
      (feature extraction). Целта е да премахне човешката намеса и да се замени с автоматизиран процес.
      При автоматизираното извличане на свойства, на модела се дава максимално количество данни, като в самия модел се
      създават допълнителни слоеве, които да благоприятстват за обработката на въпросните свойства. Подходът е
      демонстриран като ефективен и е широко изучен<a href="#reference-automated-feature-extraction"></a>.
    </p>

    <p>
      Но добавянето на допълнителни слоеве неизбежно води до увеличаване на броя параметри на модела и размера му.
      За целите на тази дипломна работа, като алтернатива, се предлага полу-автоматизиран подход. Идеята е да се
      постигне ограничаване на размера на модела, като в същото време се запази известна гъвкавост на свойствата
      при обучение. Предлага се следното параметризиране на входните данни:
    </p>

    <ul>
      <li>
        <div><b>8.2.1 Размер на азбуката</b></div>
        <p style="margin-top: 0;">
          В глави "4.3.2 Възможни символи" и "4.3.3 Малки и големи букви" беше разгледано колко често се използват
          символите в набора от данни. Беше установено, че една малка част от символите съставляват огромната част от
          текста на статиите. Намаляването на броя символи води до намаляване на броя категории. Това от своя
          страна води до по-малко входни и изходни неврони. Което редуцира преходите между неврони и броя параметри,
          нужни на модела.
        </p>

        <p style="margin-top: 0;">
          Иначе казано&mdash;ограничаването на броя символи, използвани за обучаване на модела, позволява построяването
          на по-малък модел.
        </p>

        <p>
          Параметърът се изследва в глава "9.1 Брой на категориите"
        </p>
      </li>
      <li>
        <div><b>8.2.2 Размер на речник от под-думи</b></div>
        <p style="margin-top: 0;">
          Една възможност при формирането на категории е азбуката от символи да бъде допълнена с n-грами от символи по
          следния начин:
        </p>

        <ol>
          <li>В началото на всяка буква отговаря една категория.</li>
          <li>Низът от символи се разглежда като низ от категории.</li>
          <li>Намира се най-често срещаната двойка последователни категории и за тях се създава нова категория</li>
          <li>Срещанията на двойката най-често срещани последователни категории се заменят с новата категория</li>
          <li>Повтаря се стъпка 3 до достигане на предварително избран брой категории</li>
          <li>Списъкът от получени нови категории се записва в речник</li>
        </ol>

        <p>
          Така описаният алгоритъм е почти идентичен с алгоритъма Re-Pair<a href="#reference-re-pair"></a>. Единствената
          разлика е условието за край. Re-Pair продължава докато съществуват двойки, които се срещат повече от веднъж;
          а предложеният алгоритъм извършва фиксиран брой стъпки.
        </p>

        <p>
          Предложеният подход очевидно увеличава броя категории&mdash;нещо, което в предишната глава беше посочено като
          фактор за влошаване на компресията. Но от друга страна скъсява низа от категории и подобрява компресията. За
          откриването на подходящ баланс в броя на под-думи, чрез който да се извлекат ползи, са необходими експерименти.
          Те се провеждат в глава "9.1 Брой на категориите".
        </p>

        <p>
          <div>Забележка:</div>
          За получените n-грами от символи се избира термина "под-думи". Това отразява именуване, срещано в библиотеката
          <i>tensorflow data</i>.
        </p>
      </li>
    </ul>

    <p>
      <div>Забележка:</div>
      Тук бяха разгледани единствено свойства, произлизащи от последователността от символи в заглавията и текстовете на
      статиите. На решението с кои данни да бъде трениран модел може да се гледа като на обработка на свойства. В този
      смисъл глава "7 Предварителна обработка на данните" също е вид обработка на свойства.
    </p>

    <h3>8.3 Свръх-нагаждане (overfitting)</h3>

    <p>
      Свръх-нагаждането е състояние, в което предсказващи модели могат да се окажат след трениране.
      В това състояние те успяват да предсказват данни, с които са били тренирани, но губят предсказващата си способност
      за данни, които виждат за пръв път. Проблемът е добре изследван и съществуват много подходи за избягването му.
      Например: избиране на възможно най-прост модел; ограничаване на броя параметри на модела; ограничаване
      на броя итерации при трениране на модела; регуляризация; вкарване на шум във входните данни и
      други<a href="#reference-ml-bishop"></a><a href="#reference-avoid-overfitting"></a>.
      Но основен подход е разделянето на данните на тренировъчна и на валидационна части. Тренирането се осъществява
      върху тренировъчната част, а оценяването на модела&mdash;върху валидационната.
    </p>

    <p>
      Но избягването на свръх-нагаждане би имало смисъл, ако се тренира модел, чиято цял е предсказване на различни набори
      данни. В разглеждания случай, моделът се тренира върху конкретни данни и се прилага единствено върху тях.
      При нужда от компресия на друг набор данни, би следвало моделът да се тренира единствено върху него.
      Поради тази причина наборът от данни не се разделя на тренировъчна и валидационна част и не се правят опити за
      ограничаване на свръх-нагаждането. Още по-интересно е, че на свръх-нагаждането може да се гледа като
      научаване на входните данни наизуст<a href="#reference-overfitting-as-memorization"></a> и това да се окаже
      полезно за постигане на по-добра компресия<a href="#reference-overfitting-for-compression"></a>.
    </p>
  </div>

  <div class="pages">
    <h2>9 Провеждане на експерименти и анализ на резултатите</h2>

    <p>
      Експериментите обхващат период от около половин година&mdash;като в това време не се включва предварителната
      реализация на кода, нужен за експериментите. Експериментите се простират в 45 тетрадки, написани чрез софтуерния
      пакет Jupyter. Голямата част са осъществени на видео карта NVidia RTX 2060 SUPER, която предоставя хардуерно
      ускорение на операции с тензори. Изчислителната ѝ мощ се оценява на около 7 терафлопа при изчисления с единична
      точност.
    </p>

    <p>
      Самите експериментите разглеждат няколко различни параметъра, като в различни моменти се оценяват следните техни
      възможности или стойности:
    </p>

    <ul>
      <li>избор между GRU и LSTM рекурентни слоеве</li>
      <li>Различен брой слоеве: 2, 3, 4, 5</li>
      <li>Рекурентни слоеве с различен брой неврони: 256, 376, 512, 768, 1024, 1280</li>
      <li>Различни размери на слой за влагане: 16, 32, 64, 128, както и без слой за влагане</li>
      <li>Азбуки с размер: 72, 256</li>
      <li>Речници от под-думи с размери: 0, 128, 256, 512, 1024, 2048, 4096</li>
      <li>Наборите от данни: enwik8 и enwik9</li>
    </ul>

    <p>
      За пълното изчерпване са нужни 6720 експеримента. <!-- 2 * 4 * 6 * 5 * 2 * 7 * 2 -->
      Средното време за едно обучение варира много, тъй като размерите на модела и входните данни могат да се различават
      с порядъци. Но автора оценява средно време на около 48 часа. С наличния хардуер, изчерпателното изследване би
      отнело над 36 години.
    </p>

    <p>
      Любопитно е да се отбележи, че в началото експерименти се извършваха върху процесор AMD A10 7890K - четириядрен,
      осемнишков, с честота 4.1GHz и 8GiB оперативна памет. При преминаването към трениране на гореспоменатата видеокарта,
      беше установено ускорение в размер на около 300 (триста) пъти. Куриозно е, че върху първоначалния хардуер,
      изчерпването на всички комбинации би отнело над 11000 (единадесет хиляди) години. В същото време, за трениране
      на невронни мрежи в Google са достъпни тензорни процесори способни на изчислителна мощ над 100 петафлопа,
      разполагащи с 32 TiB памет<a href="#reference-google-tpu"></a>. Така пълното изчерпване може да се постигне в
      рамките на дни. Това означава, че проблемът е практически решим, но извън възможностите на автора.
    </p>

    <p>
      Поради непрактичността на изследването на хиперпараметрите чрез пълно изчерпване, изследването е силно ограничено.
      Като цяло се търсят инкрементални подобрения на параметрите, вместо методично да се изследват всички възможности.
      По-надолу в тази глава са представени някои от изводите, като ползите от тях са демонстрирани чрез единични опити.
      Тези единични опити са взети от множеството направени експерименти.
    </p>

    <h3>9.1 Брой на категориите</h3>
    <!-- This section is abridged -->

    <p>
      Непосредствено преди обучаването на модела се извършва превръщане на последователността от символи в
      последователност от категории. Тези категории са и единствените свойства, с които моделът се обучава.
      Този процес е описан по-подробно в глава "8.2 Обработка на свойства (feature engineering).
    </p>

    <div style="float: left; margin: 10px 30px 10px 0;">
      <img src="subwords-dictionary-size.png">
      <i style="display: block; padding-left: 30px;">(фиг. 10)</i>
    </div>

    <p>
      На <i>(фиг. 10)</i> по хоризонтала са изобразени разгледаните размери на речници от под-думи. По вертикала е
      изобразен размерът на файла, който се получава. Ясно се вижда намаляването на ползите при голям брой по-думи.
    </p>

    <p>
      В резултат на известно количество проведени изследвания, избраният размер на речник от под-думи е 183, а избраният
      размер на азбука&mdash;72. Това дава брой на категориите&mdash;255 и дължина на низа от
      категории&mdash;495&nbsp;237&nbsp;324 (почти двойно по-малко от броя символи).
    </p>

    <h3>9.2 LSTM срещу GRU</h3>
    <!-- Базирано на тетрадка: 40. train-with-even-more-simplified-dataset -->

    <p>
      За сравнение между SLTM и RNN са избрани модели с един слой за влагане (с размер 16), два рекурентни слоя
      (с размери по 1024) и един напълно свързан слой на изхода. Използва се азбука от 72 символа и не присъства речник
      от под-думи.
    </p>

    <div style="display: inline-block; width: 100%;">
      <div style="float: left;">
        <pre style="font-size: 0.45rem">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
masking_1 (Masking)          (256, 256)                0
_________________________________________________________________
embedding_1 (Embedding)      (256, 256, 16)            1152
_________________________________________________________________
lstm_2 (LSTM)                (256, 256, 1024)          4263936
_________________________________________________________________
lstm_3 (LSTM)                (256, 256, 1024)          8392704
_________________________________________________________________
dense_1 (Dense)              (256, 256, 72)            73800
=================================================================
Total params: 12,731,592
Trainable params: 12,731,592
Non-trainable params: 0
        </pre>
        <i style="display: block; padding-left: 10px;">(фиг. 11)</i>
      </div>

      <div style="float: right;">
        <pre style="font-size: 0.45rem;">
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
masking (Masking)            (256, 256)                0
_________________________________________________________________
embedding (Embedding)        (256, 256, 16)            1152
_________________________________________________________________
gru (GRU)                    (256, 256, 1024)          3201024
_________________________________________________________________
gru_1 (GRU)                  (256, 256, 1024)          6297600
_________________________________________________________________
dense (Dense)                (256, 256, 72)            73800
=================================================================
Total params: 9,573,576
Trainable params: 9,573,576
Non-trainable params: 0
        </pre>
        <i style="display: block; padding-left: 20px;">(фиг. 12)</i>
      </div>
    </div>

    <p>
      <div>Забележка 1:</div>
      Числата (256, 256), които са видими на <i>фиг. 11</i> и <i>фиг. 12</i> отговарят на размера на партидите от данни,
      с които е трениран моделът.
    </p>

    <p>
      <div>Забележка 2:</div>
      Маскиращият слой, видим на <i>фиг. 11</i> и <i>фиг. 12</i> е имплементационен детайл свързан с подравняването на
      статии с различни дължини. Може да бъде забелязано, че той не притежава параметри за обучение.
    </p>

    <p>
      Най-очевидната разлика е броят параметри, нужен за всеки от моделите. При LSTM&mdash;12&nbsp;731&nbsp;592, а при
      GRU&mdash;9&nbsp;573&nbsp;576. Тази разлика директно повлиява върху скоростта на обучение.
    </p>

    <div style="float: left; margin: 10px 30px 10px 0;">
      <img src="lstm-vs-gru-in-iterations.png">
      <i style="display: block; padding-left: 30px; padding-bottom: 30px;">(фиг. 13)</i>
      <img src="lstm-vs-gru-in-hours.png">
      <i style="display: block; padding-left: 30px;">(фиг. 14)</i>
    </div>

    <p>
      Сходимостта при тренирането може да бъде видяна на <i>фиг. 13</i> и <i>фиг. 14</i>. В двете по вертикала е
      оценяващата функция. По хоризонтала на <i>фиг. 13</i> е епохата от обучението, а на <i>фиг. 14</i> е времето
      изминало от началото на обучението (в часове). Със син цвят обозначен LSTM, а с оранжев&mdash;GRU.
    </p>

    <p>
      На <i>фиг. 13</i> и <i>фиг. 14</i> се вижда, че за изпълнението на 4 епохи, на LSTM са нужни приблизително 15
      часа, а на GRU&mdash;приблизително 12. За едно и също време GRU успява да достигне малко по-добри резултати,
      но за един и същ брой епохи, LSTM достига до по-добрия резултат.
    </p>

    <p>
      При всички проведени експерименти резултатите наподобяват избрания пример. Като цяло е трудно еднозначно да се
      каже коя архитектура произвежда по-добри модели. Но със сигурност може да се твърди, че GRU произвежда по-малки
      модели. Поради спецификата на поставените цели, GRU е по-често използван за тази дипломна работа.
    </p>

    <h3>9.3 Способност за наизустяване</h3>

    <div style="float: left; margin: 10px 30px 10px 0;">
      <img src="memorisation-example.png">
      <i style="display: block; padding-left: 30px; padding-bottom: 30px;">(фиг. 15)</i>
    </div>

    <p>
      В конкретен момент от обучението беше забелязано интересно поведение на информационната ентропия за статиите.
      След една от епохите, максималната ентропийната компресия изглеждаше по начина изобразен на <i>фиг. 15</i>.
      По хоризонтала е изобразен индексът на статията в набора от данни, като са взети единствено статии с индекс
      кратен на 1000. По вертикала е изобразена максималната ентропийна компресия.
    </p>

    <p>
      Разглежданият модел използва 15&nbsp;611&nbsp;775 параметъра, което го поставя сред средно големите модели,
      изследвани в рамките на дипломната работа.
    </p>

    <p>
      Прави впечатление дълга последователност от статии с много добра компресия. Един пример за такава статия,
      достигаща компресия от 3.61%, е:
    </p>

    <pre style="font-size: 0.6rem;">
      <code class="language-wiki">
'''edwards township''' is a township located in [[kandiyohi county, minnesota]].  as of the [[2000]] census, the township had a total population of 304.

== geography ==
according to the [[united states census bureau]], the township has a total area of 92.6 [[square kilometer|km&sup2;]] (35.7 [[square mile|mi&sup2;]]).  91.6 km&sup2; (35.4 mi&sup2;) of it is land and 1.0 km&sup2; (0.4 mi&sup2;) of it is water.  the total area is 1.04% water.

== demographics ==
as of the [[census]][[geographic references#2|<sup>2</sup>]] of [[2000]], there are 304 people, 101 households, and 84 families residing in the township.  the [[population density]] is 3.3/km&sup2; (8.6/mi&sup2;).  there are 106 housing units at an average density of 1.2/km&sup2; (3.0/mi&sup2;).  the racial makeup of the township is 97.37% [[white (u.s. census)|white]], 1.64% [[african american (u.s. census)|african american]], 0.33% [[native american (u.s. census)|native american]], 0.00% [[asian (u.s. census)|asian]], 0.33% [[pacific islander (u.s. census)|pacific islander]], 0.33% from [[race (u.s. census)|other races]], and 0.00% from two or more races.  2.30% of the population are [[hispanic (u.s. census)|hispanic]] or [[latino (u.s. census)|latino]] of any race.

there are 101 households out of which 36.6% have children under the age of 18 living with them, 78.2% are [[marriage|married couples]] living together, 2.0% have a female householder with no husband present, and 16.8% are non-families. 12.9% of all households are made up of individuals and 6.9% have someone living alone who is 65 years of age or older.  the average household size is 2.98 and the average family size is 3.32.

in the township the population is spread out with 29.3% under the age of 18, 9.2% from 18 to 24, 25.3% from 25 to 44, 24.0% from 45 to 64, and 12.2% who are 65 years of age or older.  the median age is 37 years.  for every 100 females there are 109.7 males.  for every 100 females age 18 and over, there are 117.2 males.

the median income for a household in the township is $44,063, and the median income for a family is $48,125. males have a median income of $26,719 versus $21,375 for females. the [[per capita income]] for the township is $16,256.  5.7% of the population and 5.5% of families are below the [[poverty line]].  out of the total population, 7.1% of those under the age of 18 and 16.7% of those 65 and older are living below the poverty line.
[[category:kandiyohi county, minnesota]]
[[category:townships in minnesota]]
      </code>
    </pre>

    <p>
      Оказва се, че на определена позиция в набора от данни има поредица еднотипни статии. Изглежда статиите са
      автоматично генерирани от статистическа информация за населени места в САЩ. И в конкретната епоха от обучението
      моделът е достигнал параметри на обучението, с които успява да предвижда подобни статии ужасно добре. Авторът
      на дипломната работа интерпретира това като способност на модела за "наизустяване" на общия вид на тези статии.
    </p>

    <h3>9.4 Способност за предсказване</h3>
    <!-- Базирано на тетрадка: 46. demonstrate-predictions.ipynb -->

    <p>
      Интересно е да се постигне някакъв вид интроспекция върху предсказващите модели, които се построяват при
      експериментите. Но при невронните мрежи е изключително трудно да се обяснят модели. Това е известно като "проблема
      на интерпретируемостта"<a href="#reference-black-box"></a>. Невронните мрежи са черни кутии, и е възможен
      единствено косвен анализ на базата на резултатите от работата им.
    </p>

    <p>
      Въпреки че това е ограничаващо, все пак не е излишно да се разгледат такива резултати. За целта може да бъде взет
      обучен модел, да му се дадат начални данни и да се разгледа какво предсказва той.
    </p>

    <div style="float: left; margin: 10px 30px 10px 0;">
      <pre style="font-size: 0.45rem;">
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
masking (Masking)            (384, 128)                0
_________________________________________________________________
embedding (Embedding)        (384, 128, 128)           32640
_________________________________________________________________
gru (GRU)                    (384, 128, 1280)          5414400
_________________________________________________________________
gru_1 (GRU)                  (384, 128, 1280)          9838080
_________________________________________________________________
dense (Dense)                (384, 128, 255)           326655
=================================================================
Total params: 15,611,775
Trainable params: 15,611,775
Non-trainable params: 0
      </pre>
      <i style="display: block; padding-left: 20px;">(фиг. 16)</i>
    </div>

    <p>
      Структурата на такъв примерен модел е описан на <i>фиг. 16</i>. Невронната мрежа е обучена е върху <i>enwik9</i> в
      продължение на 5 епохи в рамките на около 19 часа. Достигнатата от невронната мрежа компресия е приблизително 22%
      (в числото не е включен размерът на модела).
    </p>

    <p>
      На модела се дава началният низ "<i>=Algorithm=\n\n</i>". След това 500 пъти се поискват предсказания за следващи
      категории. На всяка стъпка, измежду най-вероятните няколко категории, една произволна се избира и се фиксира като
      следваща. По този начин напълно се заобикаля ентропийната компресия и се получава косвена представа за
      модела, изграден от невронната мрежа. По този начин невронната мрежа генерира следния текст:
    </p>

    <pre style="font-size: 0.6rem; white-space: pre-wrap; word-break: break-word;">
the [[complicity]]-cursive communicy computated, [[alberry]] and [[albert gottfree]], was bothy in the satelite, as the ''goal'', and the governer was a govern-born-[[alberta]], and then golden home, [[san jana]] annotation commissioners had been used to describe a sentance for the senate, and also hosts a goodinghol [[saltire governang]].
althre sailors was bought [[johnny bertha]] ([[1955]]-), and he receptorde hosted their owner of the ''[[hote and the goodwin committed took]]''.]]
==see al seeming as militias
 military secretaries
==

== reliabing, journeys==== ==


[[californ]]

==see links
==
[[catilina]]n mississites - januari jansen,

[[jangle jang]]X
[[bXlinger]]--
X
==see====



==evening compan=====X ==X========
* X
== recordes of java -==X==X

[[ivan X X]]
====
[[johanan krajXr]]
*[[karXn kXk]]-[[karl majerka]]
* [[kief jasmi]]-- [[jasper karassen]],[[kase
    </pre>


    <p>
      Някои изводи, които могат да се направят са:
    </p>

    <ul>
      <li>
        Без да разполага с речник от цели думи, невронната мрежа е достигнала до морфологичен модел на английския език.
        За откриването на грешни думи е нужно читателят да се вгледа.
      </li>
      <li>
        Невронната мрежа е достигнала до синтактичен модел на английския език. На няколко места се срещат поредици думи,
        които могат да се интерпретират като последователности от "подлог-сказуемо-допълнение". Присъстват няколко
        различни граматически времена (например минало перфектно пасивно), които изглеждат правилно построени.
      </li>
      <li>
        Присъства пунктуация, която изглежда почти разумно.
      </li>
      <li>
        Има почти валиден код на маркиращия език Wikitext. Забелязва се списък от подточки (звезда в началото на реда),
        заглавия на секции (обградени със символа за равенство), както и множество връзки към други статии (обградени с
        квадратни скоби).
      </li>
      <li>
        Зададеният контекст&mdash;"<i>=Algorithm=\n\n</i>"&mdash;е напълно игнориран.
      </li>
      <li>
        В текста отсъства каквато и да е семантика.
      </li>
    </ul>

    <h3>9.5 Статистика на предсказанията</h3>

    <p>
      В глава "9.4 Способност за предсказване" беше изследвана способността на модела да предсказва, абстрахирайки се от
      реалните данни. В тази глава ще бъде изследвана способността за предсказване, но в контекста на конкретна статия.
      За целта e избран предварително обучен модел със структура, описана на <i>фиг. 17</i>.
    </p>

    <div style="float: left; margin: 10px 30px 10px 0;">
      <pre style="font-size: 0.45rem;">
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
masking (Masking)            (384, 128)                0
_________________________________________________________________
embedding (Embedding)        (384, 128, 128)           32640
_________________________________________________________________
gru (GRU)                    (384, 128, 1280)          5414400
_________________________________________________________________
gru_1 (GRU)                  (384, 128, 1280)          9838080
_________________________________________________________________
dense (Dense)                (384, 128, 255)           326655
=================================================================
Total params: 15,611,775
Trainable params: 15,611,775
Non-trainable params: 0
      </pre>
      <i style="display: block; padding-left: 20px;">(фиг. 17)</i>
    </div>

    <p>
      Моделът е обучен върху азбука от 72 букви и речник от 183 под-думи. Достига се компресия от приблизително 21%.
      Тренирането е извършено върху <i>enwik8</i> в продължение на 115 епохи и около 13 часа.
    </p>

    <div style="clear:both;"></div>

    <p>
      Избрана е статия с дължина 1653 символа, след предварителната обработка. След кодирането с речника от под-думи,
      дължината на статията е 1101 категории. Съдържанието на статията, след предварителната обработка, е:
    </p>

    <pre style="font-size: 0.6rem;">
      <code class="language-wiki">
=bernoulli's inequality=
in [[mathematics]], '''bernoulli's inequality''' is an [[inequality]] that approximates [[exponentiation]]s of 1 X <i>x</i>.

the inequality states that
:<math>(1 X x)Xr \geq 1 X rx\!</math>
for every [[integer]] <i>r</i> X 0 and every [[real number]] <i>x</i> X X1. if the exponent <i>r</i> is [[even number|even]], then the inequality is valid for ''all'' real numbers <i>x</i>. the strict version of the inequality reads
:<math>(1 X x)Xr > 1 X rx\!</math>
for every integer <i>r</i> X 2 and every real number <i>x</i> X X1 with <i>x</i> X 0.

bernoulli's inequality is often used as the crucial step in the [[proof (math)|proof]] of other inequalities. it can itself be proved using [[mathematical induction]].

the exponent <i>r</i> can be generalized to an arbitrary real number as follows: if <i>x</i> > X1, then
:<math>(1 X x)Xr \geq 1 X rx\!</math>
for <i>r</i> X 0 or <i>r</i> X 1, and
:<math>(1 X x)Xr \leq 1 X rx\!</math>
for 0 X <i>r</i> X 1.
this generalization can be proved by comparing [[derivative]]s.
again, the strict versions of these inequalities require <i>x</i> X 0 and <i>r</i> X 0, 1.

== related inequalities ==
the following inequality estimates the <i>r</i>-th power of 1 X <i>x</i> from the other side. for any real numbers <i>x</i>, <i>r</i> > 0, one has
:<math>(1 X x)Xr < eX{rx},\!</math>
where <i>e</i> = [[e (number)|2.718...]].
this may be proved using the inequality (1 X 1/<i>k</i>)<sup><i>k</i></sup> < <i>e</i>.

[[category:inequalities]]

[[de:bernoullische ungleichung]]
[[fr:inégalité de bernoulli]]
[[it:diseguaglianza di bernoulli]]
[[pl:nierXwnoX bernoulliego]]
[[ru:X X]]
[[zh:X]]
      </code>
    </pre>

    <p>
      От гледната точка на модела, статията е представена като последователност от категории, всяка от които е на една
      измежду 1101 позиции. За всяка позиция може да бъде разгледана реалната категория, както и предсказанието на модела.
      Предсказанията представляват вектор от 255 вероятности, чиято сума е 1. Ако тези вероятности бъдат пренаредени
      в низходящ ред, можем да гледаме на тях като функция на плътността на някакво неизвестно дискретно статистическо
      разпределение. На <i>фиг. 18</i> е изобразено точно това, разглеждайки предсказанията за първата позиция.
      На <i>фиг. 19</i> е изобразена съответстващата му функция на разпределение.
    </p>

    <div style="display: inline-flex; margin: 20px 0;">
      <div style="float: left;">
        <img src="prediction-pdf-position-0.png">
        <i style="display: block; padding-left: 30px;">(фиг. 18)</i>
      </div>
      <div style="float: left">
        <img src="prediction-cdf-position-0.png">
        <i style="display: block; padding-left: 30px;">(фиг. 19)</i>
      </div>
    </div>

    <p>
      Моментално прави впечатление приликата със зета разпределение (нормализирана версия на закона на Ципф).
      Логично е да се провери, дали това поведение важи за други позиции. На <i>фиг. 20</i> с различни цветове са
      изобразени функциите на разпределение позиция <i>i</i>, където i ∈ [1; 10]. А на <i>фиг. 21</i>&mdash; за позиция
      <i>i</i>, където i ∈ [11; 20]
    </p>

    <div style="display: inline-flex; margin: 20px 0;">
      <div style="float: left;">
        <img src="prediction-cdf-positions-0-10.png">
        <i style="display: block; padding-left: 30px;">(фиг. 20)</i>
      </div>
      <div style="float: left">
        <img src="prediction-cdf-positions-10-20.png">
        <i style="display: block; padding-left: 30px;">(фиг. 21)</i>
      </div>
    </div>

    <p>
      Изглежда свойството се запазва. В глава "4.3.2 Възможни символи" беше демонстрирано графично, че законът на Ципф
      важи за първите няколко десетки символа&mdash;а този модел е обучен единствено с тях. По всичко личи, че
      невронната мрежа се обучава да доближава статистическа дистрибуция.
    </p>

    <p>
      <div>Забележка 1</div>
      Важно е да се отбележи, че моделът не просто е научил разпределението на глобално ниво. Предсказанията, които
      дава, зависят от контекста. В реда на нещата е моделът да предскаже зета разпределение за конкретна позиция, в
      което най-вероятният символ е много малко вероятен на глобално ниво.
    </p>

    <p>
      <div>Забележка 2</div>
      Интересно е да се провери, дали такава статистическа зависимост наистина съществува във набора от данни&mdash;не
      само на глобално ниво, а и при условие предшестващи символи.
    </p>

    <p>
      <div>Забележка 3</div>
      С такова поведение на практика е невъзможно за модела да даде оценки за два символи, които са едновременно
      високи и близки. Това, обаче, би било възможно за ансамблов метод, използващ няколко рекурентни невронни мрежи.
      Това дава обосновка и за по-добрите резултати, постигани от алгоритми, използващи компресия със смесване на контексти.
    </p>

    <p>
      .......
    </p>

    На <i>(фиг. 19)</i> са изобразени

    <div style="display: inline-flex;">
      <div style="float: left;">
        <img src="correct-prediction-chance-over-time.png">
        <i style="display: block; padding-left: 30px;">(фиг. 22)</i>
      </div>
    </div>

    <div>
      <img src="correct-prediction-chance-sorted.png">
      <img src="arcsine-cdf.png">
      <img src="arcsine-pdf.png">
    </div>

    <h3>9.6 Брой и размер на рекурентните слоеве</h3>
    <!-- This section is abridged -->

    <p>
      След множество проведени експерименти, се достигат следните изводи:
    </p>

    <div style="float: left; margin: 10px 30px 10px 0;">
      <pre style="font-size: 0.45rem;">
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
masking (Masking)            (384, 128)                0
_________________________________________________________________
embedding (Embedding)        (384, 128, 128)           32640
_________________________________________________________________
gru (GRU)                    (384, 128, 1280)          5414400
_________________________________________________________________
gru_1 (GRU)                  (384, 128, 1280)          9838080
_________________________________________________________________
dense (Dense)                (384, 128, 255)           326655
=================================================================
Total params: 15,611,775
Trainable params: 15,611,775
Non-trainable params: 0
      </pre>
      <i style="display: block; padding-left: 20px;">(фиг. 26)</i>
    </div>

    <ul>
      <li>Слой за влагане на входа е нужен; добър размер за него е 64 или 128</li>
      <li>Напълно свързан слой на изхода е нужен</li>
      <li>Един рекурентен слой не дава добри резултати</li>
      <li>Повече от два рекурентни слоя не водят до подобрения</li>
      <li>Размери от 1024 или 1280 работят добре за <i>enwik9</i></li>
    </ul>

    <p style="clear: both;">
      На <i>(фиг. 26)</i> е описан моделът, избран като финален резултат от дипломната работа. Компресията, която
      моделът достига е 22%. Използват се 15&nbsp;611&nbsp;775 параметъра. Финалните размери са:
    </p>

    <ul>
      <li>Ентропийната компресия, която този модел достига е 22%. Размерът е 187 MiB</li>
      <li>Размерът на модела при единична точност на числата с плаваща запетая е 59 MiB</li>
      <li>Размерът на метаданните, подлежащи на предварителна обработка, е 7 MiB</li>
      <li>Обслужващият C++ код е под 200 KiB</li>
    </ul>

    <p>
      Компресията имплементирана по този начин достига размер от 253 MiB.
      Най-добрият постигнат резултат е 111 MiB (от алгоритъма phda9v1.8).
      При максимална компресия, zip достига до размер 295 MiB.
    </p>

  </div>

  <!--div class="pages">
    <h2>10 Ентропийно кодиране</h2>

    <h3>10.1 Кодиране на Хъфман</h3>

    <h3>10.2 Аритметично кодиране</h3>
  </div-->

  <div class="pages">
    <h2>10 Методи и материали</h2>

    <p>
      Текстът на дипломанта работа е написан на HTML/CSS.
      За синтактичното оцветяване е използвана библиотеката PrismJS<!-- https://prismjs.com/ --> и цветовата ѝ схема
      prism-vs.<!-- https://github.com/PrismJS/prism-themes/blob/master/themes/prism-vs.css -->
      За математически формули е използван TeX, който бива изобразен като стандартен MathML.
      За разширена MathML съвместимост, както и за поддръжката на подмножество от езика TeX, се използва библиотеката MathJax.
    </p>

    <p>
      Всички експерименти са проведени в Python 3.7 с помощта на <i>Jupyter</i> 1.0.0.
      За общи изчисления е използван <i>numpy</i> 1.18.1.
      За изследване на статистически разпределения е използван <i>scipy</i> 1.4.1.
      За графики е използван <i>matplotlib</i> 3.2.0.
      За получаване на речника от под-думи, описан в "9.1 Брой на категориите" се използва класът
      <i>SubwordTextEncoder</i> от библиотеката <i>tensorflow-datasets</i> 3.1.0.
      За експерименти с невронни мрежи е използван <i>Tensorflow</i> - версии 2.1 и 2.2.
      Като абстракция от високо ниво над <i>Tensorflow</i> се използва <i>Keras</i>.
    </p>

    <p>
      Огромната част от експериментите в <i>Tensorflow</i> са проведени върху видео карта NVidia GeForce RTX 2060 SUPER,
      разполагаща с 8GiB оперативна памет и 272 тензорни ядра.
    </p>

    <p>
      Правилата на Hutter Prize търпят известни ревизии във времето. В момента на започване на тази дипломна работа,
      в правилата се включва единствено размера на декомпресиращата програма. По тази причина за първоначалния код,
      имплементиращ "7 Предварителна обработка на данните", е избран езика Java и библиотеките <i>xjc</i> и <i>jaxb</i>.
      Изборът е естествен, поради удобството и широкия набор от инструменти за работата с XML и XML схеми, предоставени
      от Java. Но в хода на дипломната работа, в правилата се включва и размерът на компресиращата програма.
      Така се стига до текущия вид на правилата, описан в глава "5 Hutter Prize". С втората ревизия на правилата,
      използването на Java се превръща в пречка, заради големия обем на получените изпълними файлове и неяснотите около
      дистрибутиране на файловете асоциирани с Java виртуалната машина.
    </p>

    <p>
      Така във финалната ревизия, за имплементациите на компресиращата и декомпресираща програми, е използван C++.
      Използвани са известно количество конструкции от C++17. Кодът е компилиран върху Microsoft Visual Studio 2019,
      но е достатъчно преносим, за да работи на всеки друг C++17 компилатор.
    </p>

    <p>
      За обработка на XML е използвана библиотеката <i>libstudxml</i>. <!--https://www.codesynthesis.com/projects/libstudxml/-->
      Библиотеката е сметната за подходяща заради способността ѝ да обработва невалидни XML документи. Това се
      налага заради липсващи затварящи тагове&mdash;особеност описана в глава "4.1 MediaWiki page export format".
      Друго предимство е, че библиотеката е достъпна под MIT лиценз. Това контрастира на конкурентите ѝ, които
      използват относително овързващи варианти на GPL лиценза.
    </p>
  </div>

  <div class="pages">
    <h2>11 Заключение</h2>

    <p>
      Търсенето на възможно най-добра компресия за целите на изкуствения интелект се мотивира от формализацията на
      интелект, представена от Чарлз Хътър в книгата „Universal Artificial Intelligence. Sequential Decisions
      Based on Algorithmic Probability“.
    </p>

    <p>
      В следващите секции се обобщава подход за компресия, предложен от тази дипломна работа.
    </p>

    <h3>12.1 Резултати и дискусии</h3>

    <p>
      В дипломната работа беше предложен подход за компресия, използващ рекурентна невронна мрежа като предсказващ модел.
      Бяха описани потенциалните проблеми породени от броя параметри и размера на модела. Бяха дадени предложения
      за избягването на въпросните проблеми. На базата на тези идеи беше изградена основа, позволяваща по-задълбочено
      изследване на подобни модели.
    </p>

    <p>
      Върху тази основа, след изследване на параметрите на няколко сходни модела, беше построен пример за компресия на
      набора от данни <i>enwik9</i>, който се представя по-добре от широко използвани компресии като zip/gzip/bzip/bzip2/7zip;
      но не успява да надвиши най-добрата компресия, постигната от изследванията в областта.
    </p>

    </p>

    <h3>12.2 Бъдещи цели</h3>

    <p>
      <div><b>Статистически анализ на полученият модел</b></div>
      В глава "9.5 Статистика на предсказанията" се показват някои интересни поведения на обучените модели. Те могат да
      послужат като основа на по-обстоен статистически анализ, който да подскаже подходи за подобряване на компресията.
      Интересно е и дали на базата на силни статистически допускания, е възможно да се направи оценка за
      максималната компресия, без да се обучава невронната мрежа на практика. Това би облекчило нуждата от изследване на
      пространството от хиперпараметри.
    </p>

    <p>
      <div><b>Подобрения на модела</b></div>
      По аналог на компресията със смесване на контексти, е интересно да се изследва възможността за използване на
      множество паралелни малки рекурентни невронни слоя, вместо един голям. Те биха могли да действат като ансамблов
      метод за предсказване. Освен това биха позволили смесването на различни видове рекурентни слоеве (LSTM, GRU).
      Любопитна е възможността за смесване с трансформаторен модел или със скрит модел на Марков.
    </p>

    <p>
      <div><b>Квантизация на параметрите на модела</b></div>
      Интересна възможност, предоставена от <i>Tensorflow</i> е квантизацията на параметри. Тя позволява превръщането на
      аритметиката с числа с плаваща запетая в аритметика с 8-битови цели числа. Това е свързано с известно влошаване на
      поведението на модела, но за числа с единична точност води до четворно намаляване на нужната памет. Това потенциално
      би довело до подобряване на крайния коефициент на компресия.
    </p>

    <p>
      <div><b>Използване на повече свойства при обучение на модела</b></div>
      Моделите могат да се възползват от повече свойства. В тази дипломна работа, за получаване на свойства, се
      използват единствено текстът и заглавията на статиите. Може да бъдат изследвани ползите от използване на
      автори на статии, време на създаване, коментари, и други полета с метаданни.
      <br>
      Възможно е и използването на връзките между статиите като свойства за трениране. За целта може да се използва
      матрицата на съседство, определена от връзките, като ѝ се приложи влагане с цел намаляване на размерността на
      получените свойства.
    </p>

    <p>
      <div><b>Изчерпателен анализ на хиперпараметрите</b></div>
      Може да бъде изследвано пълното пространство от параметри, описано в глава "9 Провеждане на експерименти и анализ
      на резултатите". За целта е необходим достъп до повече хардуерни ресурси за обучаване на невронни мрежи. Може да
      бъде постигнат известен успех и с изследването само на част от пространството, чрез употребата на по-малки модели
      и използването на модерни формати за числа с плаваща запетая като <i>bfloat16</i>.
    </p>

    <p>
      <div><b>Автоматично генериране на код по XML схема</b></div>
      С цел по-добра автоматизация и за съвместимост с повече формати на входни данни, може да се опита подход с
      автоматично генериране на код по XML схема.
      Така ръчният процес за намаляване на обема на данните, описан в глава "7 Предварителна обработка на данните", може
      да бъде автоматизиран.
    </p>

    <p>
      <div><b>Подобряване на компресията при предварителна обработка</b></div>
      Описаният в глава "7 Предварителна обработка на данните" процес за намаляване на обема на данните може да бъде
      подобрен. При записване на двоичната репрезентация се използват размери кратни на размера на един байт.
      Използването на битови репрезентации неизбежно би довело до по-добри резултати. Интересна е възможността за
      прилагане на ентропийна компресия за тези данни.
    </p>

    <!-- p>
      <div><b>TODO: Подобряване на аритметичното кодиране</b></div>
    </p -->
  </div>

  <div class="pages references">
    <h2>13 Библиография</h2>
    <ol>
      <li id="reference-turing-machine">
        Turing, Alan Mathison (July 1936). "On computable numbers, with an application to the Entscheidungsproblem". J. of Math, 58 (5): 345-363.
      </li>
      <li id="reference-turing-test">
        Turing, Alan Mathison (July 1950). "Computing Machinery and Intelligence". Computing Machinery and Intelligence. Mind 49: 433-460.
      </li>
      <li id="reference-turing-provocation">
        Gandy, R. (1996). "Human versus mechanical intelligence". Machines and Thought: The Legacy of Alan Turing. Clarendon: 125-136.
      </li>
      <li id="reference-the-essential-turing">
        Turing A. M., Copeland, B. J. (Ed.). (2004). The Essential Turing. Clarendon Press.
      </li>
      <li id="reference-kolmogorov-introduction">
        Li, M., Vitányi, P. (2008). An introduction to Kolmogorov complexity and its applications (Vol. 3). New York: Springer.
      </li>
      <li id="reference-shannon-mccarthy-objection">
        Shannon, C. E. (ed.), McCarthy, J (ed.). (1956). Automata studies. New Jersey: Princeton University Press.
      </li>
      <li id="reference-hume-habit">
        Henderson, L. (2018). The problem of induction: https://plato.stanford.edu/entries/induction-problem/
      </li>
      <li id="reference-coding-theorem">
        Левин, Л. А. (1974). Законы сохранения (невозрастания) информации и вопросы обоснования теории вероятностей. Проблемы передачи информации, 10 (3), 30-35.
      </li>
      <li id="reference-logical-probability">
        Carnap, R., 1950, Logical Foundations of Probability, Chicago: University of Chicago Press.
      </li>
      <li id="reference-carnap-kaput">
        Hájek, A. (2002). Interpretations of probability: https://plato.stanford.edu/entries/probability-interpret/index.html#LogPro
      </li>
      <li id="reference-solomonoff-report">
        Solomonoff, R. J. (1960, November). A preliminary report on a general theory of inductive inference. United States Air Force, Office of Scientific Research.
      </li>
      <li id="reference-solomonoff-inductive-1">
        Solomonoff, R. J. (1964). A formal theory of inductive inference. Part I. Information and control, 7(1), 1-22.
      </li>
      <li id="reference-solomonoff-inductive-2">
        Solomonoff, R. J. (1964). A formal theory of inductive inference. Part II. Information and control, 7(2), 224-254.
      </li>
      <li id="reference-chaitin-information-theory">
        Chaitin, G. J. (1975). A theory of program size formally identical to information theory. Journal of the ACM (JACM), 22(3), 329-340.
      </li>
      <li id="reference-employment-test">
        Nilsson, Nils John (WINTER 2005). "Human-Level Artificial Intelligence? Be Serious!". AI MAGAZINE
      </li>
      <li id="reference-robot-college-test">
        Goertzel, Ben (5 September 2012). "What counts as a conscious thinking machine?". NewScientist Magazine issue 2881;
        https://www.newscientist.com/article/mg21528813-600-what-counts-as-a-conscious-thinking-machine/
      </li>
      <li id="reference-mc-carthy-on-intelligence">
        McCarthy, John (12 November 12 2007). "WHAT IS ARTIFICIAL INTELLIGENCE?".
        http://www-formal.stanford.edu/jmc/whatisai/node1.html
      </li>
      <li id="reference-hutter-ai-theory">
        Hutter, Marcus (September 2001). "Towards a Universal Theory of Artificial Intelligence based on Algorithmic Probability and Sequential Decisions",
        Lecture Notes in Artificial Intelligence (LNAI 2167), Proc. 12th European Conf. on Machine Learning, ECML (2001) 226--238
      </li>
      <li id="reference-mahoney-on-compression">
        Mahoney, Matt (20 August 2006). "Rationale for a Large Text Compression Benchmark",
        https://cs.fit.edu/~mmahoney/compression/rationale.html
      </li>
      <li id="reference-hutter-prize">
        Hutter, Marcus (March 2006) "50'000€ Prize for Compressing Human Knowledge",
        http://prize.hutter1.net/
      </li>
      <li id="reference-hutter-incomputablility">
        Jan Leike and Marcus Hutter (20 October 2015). "On the Computability of AIXI". UAI 2015. arXiv:1510.05572
      </li>
      <li id="reference-shannon-human-baseline">
        Shannon, Cluade E. (15 September 1950). “Prediction and Entropy of Printed English”, Bell Sys. Tech. J (3) p. 50-64, 1950.
      </li>
      <li id="reference-overfitting-as-memorization">
        Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, Dawn Song. (22 Feb 2018). "The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks".
        arXiv:1802.08232
      </li>
      <li id="reference-overfitting-for-compression">
        Egawa, Hiroki & Shibata, Yuichiro. (January 2019). "Storing and Compressing Video into Neural Networks by Overfitting".
        10.1007/978-3-319-93659-8_56.
      </li>
      <li id="reference-unreasonable-rnn-effectiveness">
        Karpathy, Andrej (21 May 2015). "The Unreasonable Effectiveness of Recurrent Neural Networks"
        http://karpathy.github.io/2015/05/21/rnn-effectiveness/
      </li>
      <li id="reference-jim-bowery-c-prize">
        Bowery, Jim (May 2005) "The C-Prize",
        https://groups.google.com/forum/#!topic/comp.compression/JHxrwaMMkv0
      </li>
      <li id="reference-media-wiki-xml-xsd">
        Wikimedia Foundation. MediaWiki's page export format XML schema
        https://www.mediawiki.org/xml/export-0.10.xsd
      </li>
      <li id="reference-data-exploration">
        Stratos Idreos, Olga Papaemmonouil, Surajit Chaudhuri. "Overview of Data Exploration Techniques". FOSTER Open Science.
        https://www.fosteropenscience.eu/sites/default/files/pdf/2933.pdf
      </li>
      <li id="reference-zipfs-law">
        Powers, David M W (1998). "Applications and explanations of Zipf's law". Association for Computational Linguistics: 151–160.
      </li>
      <li id="reference-nncp">
        Bellard, Fabrice (4 May 2019). "Lossless Data Compression with Neural Networks". https://bellard.org/nncp/nncp.pdf
      </li>
      <li id="reference-cmix">
        Knoll, Byron (1 August 2019). CMIX version 18, http://www.byronknoll.com/cmix.html.
      </li>
      <li id="reference-compression-benchmark">
        Mahoney, Matt (9 Mar. 2020). "Large Text Compression Benchmark".
        http://mattmahoney.net/dc/text.html
      </li>
      <li id="reference-context-mixing">
        Mahoney, M. (2005), "Adaptive Weighing of Context Models for Lossless Data Compression", Florida Tech. Technical Report CS-2005-16
      </li>
      <li id="reference-shannon-coding">
        Shannon, Claude E. (July 1948). "A Mathematical Theory of Communication".
        Bell System Technical Journal. 27 (3): 379–423. doi:10.1002/j.1538-7305.1948.tb01338.x. hdl:11858/00-001M-0000-002C-4314-2.
      </li>
      <li id="reference-shannon-fano-coding">
        Fano, R.M. (1949). "The transmission of information".
        Technical Report No. 65. Cambridge (Mass.), USA: Research Laboratory of Electronics at MIT.
      </li>
      <li id="reference-huffman-coding">
        Huffman, D. (1952). "A Method for the Construction of Minimum-Redundancy Codes".
        Proceedings of the IRE. 40 (9): 1098–1101. doi:10.1109/JRPROC.1952.273898.
      </li>
      <li id="reference-arithmetic-coding-1">
        Rissanen, J.J.; Langdon G.G., Jr (March 1979). "Arithmetic coding". IBM Journal of Research and Development. 23 (2): 149–162. doi:10.1147/rd.232.0149
      </li>
      <li id="reference-arithmetic-coding-2">
        Rubin, F (Nov 1979). "Arithmetic stream coding using fixed precision registers".
        IEEE Trans. If. Theory IT-25, 6 (Nov. 1979), 672-675.
      </li>
      <li id="reference-range-coding">
        Nigel G., Martin N., (July 1979). "Range encoding: An algorithm for removing redundancy from a digitized message",
        Video & Data Recording Conference, Southampton, UK, July 24–27, 1979.
      </li>
      <li id="reference-lstm">
        Sepp Hochreiter; Jürgen Schmidhuber (15 November 1997). "Long short-term memory".
        Neural Computation. 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735. PMID 9377276.
      </li>
      <li id="reference-gru">
        Cho, Kyunghyun; van Merrienboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua (2014).
        "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation". arXiv:1406.1078
      </li>
      <li id="reference-bwt">
        M. Burrows and D.J. Wheeler, (10 May 1994). "A Block-sorting Lossless Data Compression Algorithm",
        Technical Report 124, Digital Equipment Corporation
      </li>
      <li id="reference-bwt-theory-and-practice">
        Manzini, Giovanni (18 August 1999). "The Burrows-Wheeler Transform: Theory and Practice".
        Mathematical Foundations of Computer Science 1999: 24th International Symposium, MFCS'99 Szklarska Poreba, Poland, September 6-10, 1999 Proceedings. Springer Science & Business Media.
      </li>
      <li id="reference-persistent-states-little">
        Little, W. A. (1974). "The existence of persistent states in the brain". Math. Biosci., 19, 101-120.
      </li>
      <li id="reference-hopfield">
        Hopfield, J. J. (1982). "Neural Networks and Physical Systems with Emergent Collective Computational abilities".
        Proc. Natl. Acad. Sci. USA, 79, 2554-2558.
      </li>
      <li id="reference-rumelhart">
        Williams, Ronald J.; Hinton, Geoffrey E.; Rumelhart, David E. (October 1986). "Learning representations by back-propagating errors".
        Nature. 323 (6088): 533–536.
      </li>
      <li id="reference-lstm-speech-recognition-1">
        Fernández, Santiago; Graves, Alex; Schmidhuber, Jürgen (2007). "An Application of Recurrent Neural Networks to Discriminative Keyword Spotting".
        Proceedings of the 17th International Conference on Artificial Neural Networks. ICANN'07. Berlin, Heidelberg: Springer-Verlag. pp. 220–229. ISBN 978-3-540-74693-5.
      </li>
      <li id="reference-lstm-speech-recognition-2">
        Sak, Haşim; Senior, Andrew; Beaufays, Françoise (2014). "Long Short-Term Memory recurrent neural network architectures for large scale acoustic modeling"
      </li>
      <li id="reference-lstm-speech-recognition-3">
        Li, Xiangang; Wu, Xihong (15 October 2014). "Constructing Long Short-Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition".
        arXiv:1410.4281
      </li>
      <li id="reference-lstm-handwritten-recognition-1">
        Schmidhuber, Jürgen (January 2015). "Deep Learning in Neural Networks: An Overview".
        Neural Networks. 61: 85–117. arXiv:1404.7828. doi:10.1016/j.neunet.2014.09.003. PMID 25462637.
      </li>
      <li id="reference-lstm-handwritten-recognition-2">
        Graves, Alex; Schmidhuber, Jürgen (2009). Bengio, Yoshua; Schuurmans, Dale; Lafferty, John; Williams, Chris editor-K. I.; Culotta, Aron (eds.).
        "Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks". Neural Information Processing Systems (NIPS) Foundation: 545–552.
      </li>
      <li id="reference-lstm-tts">
        Fan, Bo; Wang, Lijuan; Soong, Frank K.; Xie, Lei (2015) "Photo-Real Talking Head with Deep Bidirectional LSTM", in Proceedings of ICASSP 2015
      </li>
      <li id="reference-lstm-machine-translation">
        Sutskever, Ilya; Vinyals, Oriol; Le, Quoc V. (2014). "Sequence to Sequence Learning with Neural Networks" (PDF). Electronic Proceedings of the Neural Information Processing Systems Conference. 27: 5346. arXiv:1409.3215. Bibcode:2014arXiv1409.3215S.
      </li>
      <li id="reference-transformers">
        Polosukhin, Illia; Kaiser, Lukasz; Gomez, Aidan N.; Jones, Llion; Uszkoreit, Jakob; Parmar, Niki; Shazeer, Noam; Vaswani, Ashish (2017-06-12).
        "Attention Is All You Need". arXiv:1706.03762
      </li>
      <li id="reference-word-embeddings-1">
        Sahlgren, Magnus (30 September 2015). "A brief history of word embeddings"
        https://www.linkedin.com/pulse/brief-history-word-embeddings-some-clarifications-magnus-sahlgren/
      </li>
      <li id="reference-word-embeddings-2">
        Mikolov, Tomas; Sutskever, Ilya; Chen, Kai; Corrado, Greg; Dean, Jeffrey (2013). "Distributed Representations of Words and Phrases and their Compositionality".
        arXiv:1310.4546
      </li>
      <li id="reference-nlp-progress">
        Ruder, Sebastian (2020). "Tracking Progress in Natural Language Processing - Language modeling - Character Level Models".
        http://nlpprogress.com/english/language_modeling.html#hutter-prize
      </li>
      <li id="reference-automated-feature-extraction">
        Zeiler, Matthew D; Fergus, Rob (12 Nov 2013). "Visualizing and Understanding Convolutional Networks" arXiv:1311.2901
      </li>
      <li id="reference-re-pair">
        Larsson, N. J., & Moffat, A. (2000). "Off-line dictionary-based compression". Proceedings of the IEEE, 88(11), 1722-1732.
      </li>
      <li id="reference-ml-bishop">
        Bishop, Christopher M (2006). "Pattern Recognition and Machine Learning". Springer, ISBN: 9780387310732
      </li>
      <li id="reference-avoid-overfitting">
        Shaeke Salman, Xiuwen Liu (19 Jan 2019). "Overfitting Mechanism and Avoidance in Deep Neural Networks". arXiv:1901.06566v1
      </li>
      <li id="reference-google-tpu">
        Cloud TPU v3 Pod, Google, https://cloud.google.com/tpu#cloud-tpu-v3-pod
      </li>
      <li id="reference-black-box">
        Gilpin, Leilani H.; Bau, David; Yuan, Ben Z.; Bajwa, Ayesha; Specter, Michael; Kagal, Lalana (31 May 2018).
        "Explaining Explanations: An Overview of Interpretability of Machine Learning". arXiv:1806.00069
      </li>
    </ol>
  </div>
</body>
</html>
