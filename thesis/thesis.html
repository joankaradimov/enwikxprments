<head>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.20.0/themes/prism.min.css" rel="stylesheet" />
  <link href="prism-vs.css" rel="stylesheet" />
  <style>
    * { box-sizing: border-box; }

    p { text-align: justify; }

    html { font-size: 18px; }
    h1 { font-size: 3rem; text-align: center; }
    h2 { font-size: 2rem; text-align: center; }
    h3 { font-size: 1.8rem; }
    h3 { font-size: 1.5rem; }
    sup { font-size: .5rem; }

    @page {
      size: A4;
      padding: 2cm;
      margin: 3cm auto;
    }

    .pages {
      width: 21cm;
      min-height: 29.7cm;
      padding: 2cm;
      margin: 1cm auto;
      border: 1px #D3D3D3 solid;
      border-radius: 3px;
    }

    @media print {
      html {
        font-size: 18px;
      }

      .pages {
        padding: 0 2cm;
        margin: 0 auto;
        border: initial;
        border-radius: initial;
        width: initial;
        min-height: initial;
        box-shadow: initial;
        background: initial;
        page-break-after: always;
      }
    }

    .contents ol { counter-reset: item; }
    .contents ol > li { display: block; }
    .contents ol > li:before {
      content: counters(item, ".") " ";
      counter-increment: item;
    }

    .references ol { counter-reset: item; font-size: 12px; padding-left: 0; }
    .references ol > li { display: block; padding-left: 30px;}
    .references ol > li:before {
      content: "[" counters(item, '') "]";
      counter-increment: item;
      margin-left: -25px;
      min-width: 22px;
      display: inline-block;
    }

    .dataset-fields li {
      padding-bottom: 15px;
    }

    pre[class*="language-"] {
      border: none;
      overflow: hidden;
    }

    code[class*="language-"] {
      white-space: pre-wrap;
      word-break: break-word;
    }
  </style>

  <script>
MathJax = {
  options: {
    renderActions: {
      find: [10, function (doc) {
        for (var node of document.querySelectorAll('script[type^="math/tex"]')) {
          var display = !!node.type.match(/; *mode=display/);
          var math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
          var text = document.createTextNode('');
          node.parentNode.replaceChild(text, node);
          math.start = {node: text, delim: '', n: 0};
          math.end = {node: text, delim: '', n: 0};
          doc.math.push(math);
        }
      }, '']
    }
  }
};
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.20.0/prism.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.20.0/plugins/autoloader/prism-autoloader.min.js"></script>
</head>
<body>
  <div class="pages" style="text-align: center;">
    <h1>
      Компресиране на някои представяния на знание с приложение на рекурентни невронни мрежи
    </h1>
    <div>ТОДО: дата, име, ръководител, катедра(?)</div>
    <i>
      <div>joan.karadimov@gmail.com</div>
      <div>Факултет по математика и информатика</div>
      <div>Софийски Университет</div>
    </i>

    <div style="font-weight: bold;">Абстракт</div>
    <div>TODO</div>
  </div>

  <div class="pages">&nbsp;</div>

  <div class="pages">
    <h2>1. Въведение</h2>

    <p>
      Постигането на силен изкуствен интелект и обща интелигентност е една от задачите, с които изкуственият
      интелект (ИИ) се сблъсква още от създаването си. Въпреки относително бурното развитие на ИИ през последните
      години, прогресът в областта на общата интелигентност за момента е слаб. От части това може да се обясни с
      хардуерни ограничения, лимитиращи способността ни да достигаме смислени практически резултати. От друга страна -
      преди въобще да се търси смислен резултат в контекста на силния изкуствен интелект, трябва да бъде дефиниран
      начин за проверка и тестване на въпросния резултат.
    </p>

    <p>
      Добре известен е тестът на Тюринг <a href="#reference-turing-test"><sup>[x]</sup></a>. Но още в първата глава
      "The Imitation Game" на основополагащата си статия, Тюринг отказва да се ангажира с дефиниция на интелект, и
      вместо това избира да търси алтернативен въпрос. Подобен, но по-малко известен, е тестът на работното място
      (The Employment Test)<a href="#reference-employment-test"><sup>[x]</sup></a>, формулиран от Нилс Нилсън.
      Целта е да се провери дали агент може да се справи поне толкова
      добре колкото човек в служба, съществена за икономиката. Тестът на студента-робот (The Robot College Student
      Test)<a href="#reference-robot-college-test"><sup>[x]</sup></a> на Бен Гьорцел цели да провери способността на агент
      успешно да премине и завърши пълен университетски курс.
    </p>

    <p>
      Множество други автори предлагат подобни тестове. И въпреки че много от тях представят интересни перспективи,
      обединяващо е, че никой не предлага строга формализация или подход към решаването на проблема. Иначе казано -
      изброените тестове дават крайна цел, но не и явна функция, която да оптимизираме, за да стигнем до силен
      изкуствен интелект. За такова начинание, за начало, е нужна общоприета дефиниция на интелект. По темата,
      Джон МакКарти казва: "Проблемът е, че като цяло, все още не можем да характеризираме кои изчислителни процедури да
      наречем интелигентни"<a href="#reference-mc-carthy-on-intelligence"><sup>[x]</sup></a>. Липсата на такава
      общоприета и твърда дефиниция на интелект оставя място за интерпретация, но и създава интересни възможности за
      изследвания.
    </p>

    <h3>Математическо моделиране на интелекта</h3>

    <p>
      В книгата си Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability, в търсене на
      числен модел на интелекта, Маркъс Хътър предлага формализма на универсалния алгоритмичен агент&mdash;AIXI.
      AIXI е агент за обучение с утвърждение, чиято цел е на базата на предишни свои действия, както и на
      взаимодействие с дадена среда, описана чрез машина на Тюринг&mdash;да максимизира целева награда, оценена с число.
      Хътър доказва, че оптималното поведение на такъв агент е да предполага, че средата, с която взаимодейства,
      е описана от най-кратката възможна програма<a href="#reference-hutter-ai-theory"><sup>[x]</sup></a>.
    </p>

    <p>
      Агентът не разполага с функцията, описваща средата в явен вид. Така неговата цел става построяването на най-малък
      възможен модел на въпросната среда, базирайки се на взаимодействието си с нея. Тъй като такъв модел би бил описан
      с програма, целта на агента, на практика, е да компресира наблюденията върху средата си до най-кратката такава
      програма<a href="#reference-mahoney-on-compression"><sup>[x]</sup></a>.
    </p>

    <div style="font-style: italic; display: inline-block; margin-bottom: 12px;">
      <div style="float: left; width: 38%; margin: 0 3%;">
        [...] being able to compress well is closely related to acting intelligently, thus reducing the slippery concept
        of intelligence to hard file size numbers. In order to compress data, one has to find regularities in them, which
        is intrinsically difficult.
      </div>

      <div style="float: right; width: 50%; margin: 0 3%;">
        [...] способността да се компресира добре е тясно свързана с възможността да действаш интелигентното. Така
        неясната идея за интелигентност може да се сведе до размери на файлове и числа. А за да се компресират данни, е
        нужно да се намерят закономерности в тях, което е трудно.
      </div>
    </div>
    <div style="text-align: right; font-style: italic; margin-bottom: 12px;">
      Маркъс Хътър<a href="#reference-hutter-prize"><sup>[x]</sup></a>
    </div>

    <p>
      Разглеждането на компресия като еквивалентна на интелект предоставя мощен инструмент. Разполагайки с набор от
      данни, способността за компресия на данните дава числова мярка на интелекта.
    </p>

    <h3>Цел на дипломната работа</h3>

    <p>
      През 2006-та година, Маркъс Хътър стартира състезанието
      Hutter Prize(todo: превод)<a href="#reference-hutter-prize"><sup>[x]</sup></a>&mdash;практическа задача за компресия на човешко знание.
      Целта на тази дипломна работа е да очертае подход за компресия, съвместим с изискването на състезанието. Това се постига,
      като се строи модел за предвиждане на данни, чрез използването на актуални изследвания и технологии.
      Изследването набляга на прилагането на рекурентни невронни мрежи за строенето на модела&mdash;и
      по-специално&mdash;LSTM и GRU. Тези архитектури са показали своята ефективност в предвиждането на последователности
      от данни<a href="#reference-unreasonable-rnn-effectiveness"><sup>[x]</sup></a>.
    </p>

    <p>
      Построените предвиждащи модели дават оценки на вероятностите за възможни следващи символи в набора от данни.
      Компресия се постига чрез ентропийно кодиране, като на символи с по-висока вероятност се
      съпоставят по-кратки последователности от битове, а на по-малко вероятни&mdash;по-дълги. Разглеждат се кодиране
      на Хъфман и аритметично кодиране, както и отражението им върху feature engineering-а [todo: превод], който прилагаме.
    </p>

    <p>
      Интересен е въпросът&mdash;ако се сравняват алгоритми за компресия&mdash;какъв набор от данни да бъде избран.
      Очевиден отговор е "цялото човешко знание". Но такъв отговор няма добра практическа стойност.
      Нужна е някаква текстова репрезентация на възможно най-голяма част от човешкото знание.
      Като стандартен benchmark (todo: превод) Джим Бауъри предлага текстовото съдържание на
      Wikipedia<a href="#reference-jim-bowery-c-prize"><sup>[x]</sup></a>. Тя е един добър кандидат, тъй като
      притежава съществен обем. От друга страна е подходяща заради отвореността и лесния достъп до данните ѝ.
      Хътър подкрепя идеята и решава да използва Wikipedia за целите на Hutter Prize(todo: превод).
    </p>

    <p>
      Това е и практическият проблем, който разглеждаме&mdash;постигането на добра компресия на данните в Wikipedia.
      В частност&mdash;върху набори от данни наречени <i>enwikX</i>, където X ∈ {5, 6, 7, 8, 9} и съдържа първите
      10<sup>X</sup> байта от XML репрезентация на данните в Wikipedia.
    </p>

    <p>
      Естествен въпрос е какво означава "добра" компресия. Често използвани мерки са "по-добра от съществуващия машинен подход",
      както и "по-добра от човек". В контекста на силния изкуствен интелект, второто представлява по-голям интерес.
      Изследвайки ентропия и компресия, Клод Шанън провежда експерименти с хора, опитвайки се да оцени способността
      им да служат като модел за предвиждане на текст. Използвайки 27 символа - буквите от латиницата в английския,
      допълнени със символ за интервал - Шанън оценява способността на хората на еквивалентна на компресия в
      границите между 0.6 и 1.3 бита за символ<a href="#reference-shannon-human-baseline"><sup>[x]</sup></a>.
      Махони провежда симулации с конкретни модели и оценява същата мярка на около 1 бит за символ, като добавянето на
      пунктуация и разграничаването на големи/малки букви вдига мярката до 1.25 бита за
      символ<a href="#reference-mahoney-on-compression"><sup>[x]</sup></a>.
    </p>

    <p>
      Необичайно в случая е, че избягването на свръх-нагаждането (overfitting) няма да бъде цел. Интересно е, че за
      компресия чрез невронни мрежи може дори да бъде полезно<a href="#reference-overfitting for compression"><sup>[x]</sup></a>.
      Друго нехарактерно е feature engineering-а [todo: превод], който се прилага върху данните.
      Този подход, необходим за много от алгоритмите за машинно самообучение, обикновено се заобиколя при невронните мрежи.
      Но в случая feature engineering-ът се налага и има паралели с някои подходи при класически алгоритми за компресия.
    </p>

    <h3>План на целите</h3>

    <h4>Изследване на данните</h4>

    <p>
      В глава "4 Наборите от данни enwikX" се изследват наборите от данни. Първоначално данните се разгледат на високо
      ниво, като се обръща внимание на присъствието на два формални езика и един естествен (английски) език, вложени
      един в друг. В последствие се обръща внимание структурата и конкретни полета с метаданни. Търсят се възможности за
      опростяване на потенциалните тренировъчни данни. Изследват се и детайли, които биха или полезни на етап трениране на модел.
    </p>

    <h4>Формулиране на подход</h4>

    <p>
      В глава "6 Предложен подход за компресия на наборите от данни enwikX" се формулира груб подход за компресия,
      като имплементационните детайли се оставят за по-късно. Обръща се внимание на начина за постигане на компресия,
      както и на възстановяването на оригиналните данни при декомпресия.
    </p>

    <h4>Предварителна обработка на данните</h4>

    <p>
      В глава "6.1 Предварителна обработка на данните" се описва обработката на XML данните и отделянето на метаданните
      от същинското текстово съдържание на статиите. Разглежда се наивна стратегия за компресиране на метаданните, която
      не използва подходи от машинното самообучение и изкуствения интелект, но притежава интересни паралели с
      преобразуванието на Бъроуз-Уилър. Текстовото съдържание се интерпретира като последователност от категории.
      Разглеждат се начини за намаляване на броя на категориите и за съкращаване на последователността от категории.
    </p>

    <h4>Моделиране с невронни мрежи</h4>

    <p>
      В глава "6.2 Рекурентни невронни мрежи" се разглежда способността на рекурентни невронни мрежи да служат
      като модели за предвиждане на последователности от данни. Представят се резултати от други изследвания и се описва
      общата им структура, таксономията им, и развитието им във времето. Мотивира се избора на разглежданите архитектури
      и се дават някои алтернативи. Дефинират се свръх-нагаждане (overfitting) и Feature Engineering и се описват начините, по които
      употребата им в предложения подход се различава от традиционните им приложения.
    </p>

    <h4>Имплементации и изследване на ентропийни кодирания</h4>

    <p>
      В глава "6.3 Ентропийно кодиране" се сравняват асимптотичните сложности и някои практически
      проблеми при две от най-разпространените ентропийни кодировки. Сравнява се представянето им във времето и се
      демонстрират идеи за оптимизации.
    </p>

    <h4>Експерименти</h4>

    <p>
      В глава "7 Експерименти" се разглежда избора на конкретна архитектура на рекурентни невронни мрежи. Изследват се
      хиперпараметрите на въпросните невронни мрежи - брой слоеве, размер на embedding (todo: превод) слой, размер на
      рекурентен (todo: превод) слой, както и по-общите хиперпараметри - размер на азбуката от символи и брой на n-грами
      (todo: превод), които се разглеждат.
    </p>
  </div>

  <div class="pages contents">
    <h2>2. Съдържание</h2>
    <ol>
      <li>
        Въведение
        <ol>
          <li>Математическо моделиране на интелекта</li>
          <li>Цел на дипломната работа</li>
          <li>План на целите</li>
        </ol>
      </li>
      <li>Съдържание</li>
      <li>
        Компресията като мярка на интелект
        <ol>
          <li>Ентропия на Шанън и оптималната компресия</li>
          <li>Сложност по Колмогоров и универсална индукция на Соломонов</li>
          <li>AIXI и AIXItl</li>
          <li>Теорема за най-бърз и най-кратък алгоритъм</li>
        </ol>
      </li>
      <li>
        Наборите от данни <i>enwikX</i>
        <ol>
          <li>MediaWiki page export format</li>
          <li>Wikitext</li>
          <li>Изследване на данните</li>
        </ol>
      </li>
      <li>
        Hutter Prize (todo: превод)
        <ol>
          <li>Текущи най-добри резултати</li>
          <li>Забележки относно правилата</li>
        </ol>
      </li>
      <li>
        Предложен подход за компресия на наборите от данни <i>enwikX</i>
        <ol>
          <li>Предварителна обработка на данните</li>
          <li>
            Рекурентни невронни мрежи
            <ol>
              <li>Класически подход и проблеми</li>
              <li>Свръх-нагаждане (overfitting)</li>
              <li>Feature Engineering</li>
            </ol>
          </li>
          <li>
            Ентропийно кодиране
            <ol>
              <li>Кодиране на Хъфман</li>
              <li>Аритметично кодиране</li>
            </ol>
          </li>
        </ol>
      </li>
      <li>
        Експерименти
        <ol>
          <li>LSTM срещу GRU</li>
          <li>Размер на азбуката</li>
          <li>Размер на речник от под-думи</li>
          <li>Брой и размер на рекурентните слоеве</li>
        </ol>
      </li>
      <li>Методи и материали</li>
      <li>Резултати и дискусии</li>
      <li>Заключение</li>
    </ol>
  </div>

  <div class="pages">

    <h2>Компресията като мярка на интелект</h2>

    <h3>Ентропия на Шанън и оптималната компресия</h3>

    <p>
      <script type="math/tex; mode=display">H(X) = -\sum_i P_X(x_i) \log_b{P_X(x_i)}</script>
    </p>

    <div>ЧЕРНОВА</div>
    <i>
    <p>
      Как се третира предвиждащ модел, който просто знае тестовия набор от данни наизуст? Тогава,
      изчислявайки информационната ентропия на този набор от данни, ще се получи нулева ентропия. Но за произволен друг
      набор от данни, ентропията би била не-нулева и висока. На практика това означава, че съществува алгоритъм за
      компресия, произвеждащ най-добрата възможна компресия (0 бита) за нашия набор от данни, но даващ много лоша
      компресия за всичко друго.
    </p>

    <p>
      На пръв поглед този проблем може да се избегне, ако вместо един набор от данни, оценяваме и втори. Но тогава,
      можем просто да вземем модел, който знае двата набора от данни наизуст. Тогава компресията ще работи почти идеално
      за тях, но ще се проваля за всичко друго.
    </p>
    </i>

    <h3>Сложност по Колмогоров и универсална индукция на Соломонов</h3>

    [todo: да спомена, че не съществува оптимална не-ентропийна компресия]

    <h3>AIXI и AIXItl</h3>

    <!--p>
      Формализмът е
      обвързан с идеите за универсална индукция на Рей Соломонов и за сложност по Колмогоров. Но Хътър отбелязва, че
      въпреки възможността да се счете AIXI за формална дефиниция на математическо решение на ИИ, такова решение не е
      практично заради неизчислимостта му<a href="#reference-hutter-incomputablility"><sup>[x]</sup></a>.
    </p>

    <p>
      Като стъпка в посоката на практическа теория на ИИ Хътър предлага AIXItl &mdash; изчислим модел на AIXI, разглеждащ
      програми ограничени по размер и време за изчисление. Това се допълва от представената от него теорема за най-бърз
      и най-кратък алгоритъм<a href="#"><sup>[todo: цитат - страница 219]</sup></a>, която грубо казано твърди, че
      съществува единствена програма, която е най-бързата и най-кратката. Иначе казано - има единствена програма, която
      води до най-добро приближение на AIXItl до AIXI.
      [todo: малко повече пояснения, може би? Или мястото им не е в увода...]
    </p>

    <p>
      Добрите модели се описват от по-кратки програми. За по-кратка програма, може да се каже че е компресирана.
      В такъв случай може да се твърди, че демонстрирането на възможност за добро компресиране е тясно свързано с
      построяването на добър предвиждащ модел.
      От друга страна, разбирането на входните данни, изискващо интелект, позволява да се построи добър предвиждащ модел за данните.
      Отворен е въпросът, дали построяването на добър модел предполага интелект. Хътър хипотетизира, че двете са еквивалентни [todo: citation needed].
      Базирайки се на това, може да се твърди, че описването на модел в по-кратка (компресирана) програма е еквивалентно на интелект.
    </p-->

    <h3>Теорема за най-бърз и най-кратък алгоритъм</h3>
  </div>

  <div class="pages">
    <h2>Наборите от данни <i>enwikX</i></h2>

    <p>
      Съществуват няколко набора от данни, носещи подобните имена: <i>enwik6</i>, <i>enwik7</i>, <i>enwik8</i>,
      <i>enwik9</i>, и <i>enwik10</i>. Общият вид на имената е enwikX, където X ∈ {5, 6, 7, 8, 9}. Всеки от тях
      представлява първите 10<sup>X</sup> байта от една определена текстова репрезентация на свободно достъпната
      енциклопедия Wikipedia. Данните са взети единствено от англоезичната версия.
    </p>

    <p>
      Въпросните набори от данни са широко използвани в
      изследвания в областта на ИИ, и по-конкретно - в областите на моделиране и синтез на естествени езици. Честият
      избор на набори от данни базирани на Wikipedia при тези изследвания е продиктуван от няколко фактора:
      <ul>
        <li>
          Данните са достъпни под два лиценза - "GNU Free Documentation License" и "Creative Commons Attribution-ShareAlike".
          И двата позволяващи употребата и разпространението на данните с много малко ограничения.
        </li>
        <li>
          Данните имат относително високото качество, често с множество автори и редактори за всяка от статиите.
        </li>
        <li>
          Данните са взети от файл, чийто оригинален размер е 4.8 GiB - над 2 500 000 страници текст, при 2000
          символа на страница - безспорно значително количество текст.
        </li>
      </ul>
    </p>

    <p>
      Трябва да се отбележи, че данните са във вида, в който са съществували в Wikipedia на 3 март 2006. Към момента
      обемът на Wikipedia е над 4 пъти по-голям. Разумно е и да спекулираме, че качеството на статиите се е повишило.
      Но въпреки това, enwikX не се обновяват &ndash; това гарантира консистентност на изследванията във времето.
    </p>

    <h3>MediaWiki page export format</h3>

    <p>
      Преди започване на изследване на данните, е добре да се разгледа репрезентацията им. Вече беше споменато, че
      наборът от данни е текстов, но не беше уточнен конкретен формат и съдържание. Те ще бъдат предмет на интерес
      занапред в дипломната работа, и заслужава да бъдат разгледани по-подробно. Първите 15 реда могат да бъдат получени с Unix
      командата "<i>head -n 15 enwik9</i>":

      <pre>
        <code class="language-xml">
&lt;mediawiki xmlns=&quot;http://www.mediawiki.org/xml/export-0.3/&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd&quot; version=&quot;0.3&quot; xml:lang=&quot;en&quot;&gt;
&lt;siteinfo&gt;
  &lt;sitename&gt;Wikipedia&lt;/sitename&gt;
  &lt;base&gt;http://en.wikipedia.org/wiki/Main_Page&lt;/base&gt;
  &lt;generator&gt;MediaWiki 1.6alpha&lt;/generator&gt;
  &lt;case&gt;first-letter&lt;/case&gt;
    &lt;namespaces&gt;
    &lt;namespace key=&quot;-2&quot;&gt;Media&lt;/namespace&gt;
    &lt;namespace key=&quot;-1&quot;&gt;Special&lt;/namespace&gt;
    &lt;namespace key=&quot;0&quot; /&gt;
    &lt;namespace key=&quot;1&quot;&gt;Talk&lt;/namespace&gt;
    &lt;namespace key=&quot;2&quot;&gt;User&lt;/namespace&gt;
    &lt;namespace key=&quot;3&quot;&gt;User talk&lt;/namespace&gt;
    &lt;namespace key=&quot;4&quot;&gt;Wikipedia&lt;/namespace&gt;
    &lt;namespace key=&quot;5&quot;&gt;Wikipedia talk&lt;/namespace&gt;
        </code>
      </pre>

      Моментално е видно, че става въпрос за XML-базиран формат. Форматът се нарича <i>MediaWiki page export format</i>
      (todo: превод в допълнение на официалното английско име на формата?), описан е в XML
      схема<a href="#reference-media-wiki-xml-xsd"><sup>[x]</sup></a>, и съдържа статии, метаданни за статиите,
      както и общи метаданни. Откъсът по-горе описва общи метаданни, и по-специално в случая &ndash; името на сайта,
      адреса му, версията на използвания формат, както и част от основните секции в Wikipedia.
    </p>

    <p>
      Трябва да бъде отбелязано, че никой от <i>enwikX</i> наборите от данни не представлява валиден XML документ.
      Причината е, че всеки от тях съдържа определено количество данни, започващи от началото на валиден документ
      от тип MediaWiki page export format. Взимането на съдържание единствено от началото и изрязването му от края,
      води до незатворени XML етикети. Това от своя страна неизбежно води до споменатата липса на валидност.
    </p>

    <p>
      По-напред в съдържанието се намират данни и за самите статии. Например - редове от 120 до 140 могат да бъдат
      разгледани чрез "<i>head -n 120 enwik9 | tail -n 20</i>"

      <pre>
        <code class="language-xml">
&lt;/revision&gt;
&lt;/page&gt;
&lt;page&gt;
  &lt;title&gt;AdA&lt;/title&gt;
  &lt;id&gt;11&lt;/id&gt;
  &lt;revision&gt;
    &lt;id&gt;15898946&lt;/id&gt;
    &lt;timestamp&gt;2002-09-22T16:02:58Z&lt;/timestamp&gt;
    &lt;contributor&gt;
      &lt;username&gt;Andre Engels&lt;/username&gt;
      &lt;id&gt;300&lt;/id&gt;
    &lt;/contributor&gt;
    &lt;minor /&gt;
    &lt;text xml:space=&quot;preserve&quot;&gt;#REDIRECT [[Ada programming language]]&lt;/text&gt;
  &lt;/revision&gt;
&lt;/page&gt;
&lt;page&gt;
  &lt;title&gt;Anarchism&lt;/title&gt;
  &lt;id&gt;12&lt;/id&gt;
  &lt;revision&gt;
        </code>
      </pre>

      Това е статия за програмния език Ада. Като метаданни могат да бъдат открити името на статията и уникалния ѝ числов
      идентификатор. Прави впечатление XML етикета '&lt;revision&gt;'. Причината за съществуването му е, че Wikipedia
      поддържа история от редакциите на всяка статия, наричани ревизии. В <i>enwikX</i> наборите от данни,
      винаги присъства единствено последната ревизия. Това е и мястото, на което могат да бъдат открити някои от
      най-важните данни - уникален числов идентификатор на ревизията, време на създаване, автор, индикация дали
      редакцията е малка, коментар (липсващ тук), и текст на статията.
    </p>

    <h3>Wikitext</h3>

    <p>
      В примера по-горе, текстът на примерната статия е "#REDIRECT [[Ada programming language]]". Очевидно пунктуацията
      и специалните символи са повече, отколкото бихме очаквали, от текст на естествен език. Причината е, че текстът е
      във формат Wikitext &ndash; markup (todo: превод) език специално създаден за целите на Wikipedia. (todo: да разкажа за markup езици и за подобни формати, може би?)
    </p>

    <p>
      Форматът позволява декларирането на секции, подсекции, връзки към други статии и техни секции, автоматично
      пренасочване към други статии и техни секции, влагане на изображения, списъци с подточки, таблици и други.
      Между редове 135 и 140, например, може да бъде открито:
    </p>

      <pre>
        <code class="language-wiki">
          The word '''anarchism''' is [[etymology|derived from]] the [[Greek language|Greek]] ''[[Wiktionary:&amp;amp;#945;&amp;amp;#957;&amp;amp;#945;&amp;amp;#961;&amp;amp;#967;&amp;amp;#943;&amp;amp;#945;|&amp;amp;#945;&amp;amp;#957;&amp;amp;#945;&amp;amp;#961;&amp;amp;#967;&amp;amp;#943;&amp;amp;#945;]]'' (&amp;quot;without [[archon]]s (ruler, chief, king)&amp;quot;). Anarchism as a [[political philosophy]], is the belief that ''rulers'' are unnecessary and should be abolished, although there are differing interpretations of what this means. Anarchism also refers to related [[social movement]]s) that advocate the elimination of authoritarian institutions, particularly the [[state]].&amp;lt;ref&amp;gt;[http://en.wikiquote.org/wiki/Definitions_of_anarchism Definitions of anarchism] on Wikiquote, accessed 2006&amp;lt;/ref&amp;gt; The word &amp;quot;[[anarchy]],&amp;quot; as most anarchists use it, does not imply [[chaos]], [[nihilism]], or [[anomie]], but rather a harmonious [[anti-authoritarian]] society. In place of what are regarded as authoritarian political structures and coercive economic institutions, anarchists advocate social relations based upon [[voluntary association]] of autonomous individuals, [[mutual aid]], and [[self-governance]].

          While anarchism is most easily defined by what it is against, anarchists also offer positive visions of what they believe to be a truly free society. However, ideas about how an anarchist society might work vary considerably, especially with respect to economics; there is also disagreement about how a free society might be brought about.
        </code>
      </pre>

    <p>
      Откъсът съдържа примери за връзки към други статии и техни секции (оградени с двойни квадратни скоби), удебелен
      текст (тройки кавички) и други. Присъствието на толкова много специални символи, поставя под въпрос
      съпоставимостта с базовата (todo: превод на baseline) оценка от между 0.6 и 1.3 бита за символ, която Шанън дава.
      Оценката, както вече беше споменато, е за азбука от 27 символа, която игнорира числа, пунктуация, големи и малки букви
      и други възможно специални или служебни символи. Това поставя въпроса - какво може да бъде видяно в данните?
    </p>

    <h3>Изследване на данните</h3>

    <p>
      Изследването на данните е стандартен подход и цели първоначално запознаване с данните и постигане на определено
      ниво на разбиране за предметната област. Търсят се проблеми в данните, обръща се внимание на обема им, на
      присъствието на каквито и да е зависимости, статистически разпределения или скрита структура. Идентифицират се и
      части от тях, които биха представлявали интерес при по-обстойно изследване. Всичко това се постига с поредица
      кратки програми и с помощта на визуални репрезентации<a href="#reference-data-exploration"><sup>[x]</sup></a>.
    </p>

    <p>
      Разглеждайки <i>enwikX</i> наборите от данни, както и тяхната XML схема, може да се достигне до списък от имена на
      полета с метаданни. В търсенето на подход за компресия, от значение са типовете на въпросните метаданни. Чрез
      кратка помощна програма, са изчислени и приложени сумарните размерите на данните, разбити по поле. В размера са
      включени XML етикети, XML атрибути, както и водещите интервали преди тях.
    </p>

    <ul class="dataset-fields">
      <li>
        <b>Общи метаданни</b>:<br>
        Метаданни, които не са конкретни за определена статия, а общи за набора от данни.<br>
        Размер в байтове: 1403
      </li>
      <li>
        <b>Заглавие на статия</b><br>
        Текстов низ, съдържащ заглавието<br>
        Размер в байтове: 9221250
      </li>
      <li>
        <b>Ограничения върху статията</b><br>
        Текстов низ в специфичен формат, описващ забрани за редактиране, преименуване и други<br>
        Размер в байтове: 46572
      </li>
      <li>
        <b>Идентификатор на статия</b><br>
        Число, уникално идентифициращо статия<br>
        Размер в байтове: 4782069
      </li>
      <li>
        <b>Идентификатор на ревизия</b><br>
        Число, уникално идентифициращо ревизия на статия<br>
        Размер в байтове: 5842240
      </li>
      <li>
        <b>Клеймо (todo: timestamp - превод) на ревизия</b><br>
        Текстов низ във формат ISO 8601, отбелязващ дата и време на създаване на ревизията<br>
        Размер в байтове: 12171350
      </li>
      <li>
        <b>Автор на ревизия</b><br>
        Вложен XML елемент, съдържащ или потребителско име и идентификатор на автор, или низ с IP адреса му<br>
        Размер в байтове: TODO
      </li>
      <li>
        <b>Флаг за малка ревизия</b><br>
        Празен XML елемент, чието присъствие индикира, че промяната в ревизията е малка<br>
        Размер в байтове: 1581072
      </li>
      <li>
        <b>Коментар към ревизия</b><br>
        Текстов низ, съдържащ коментари към ревизията<br>
        Размер в байтове: 5582279
      </li>
      <li>
        <b>Текст за ревизията</b><br>
        Текстов низ, с текста на ревизията на статията<br>
        Размер в байтове: 897862416
      </li>
    </ul>

    <p>
      Естествено е опитите за компресия да бъдат концентрирани върху частта от данните, която заема най-много място.
      Не е изненадващо, че огромната част от данните са в текстовете на статиите. Относително голямо място заемат и
      метаданните за авторите. Трети по размер са заглавията на статиите. Тези, най-големи по обем, части от данните се
      изследват допълнително.
    </p>

    <h4 style="clear: both; padding-top: 30px;">Статии</h4>

    <p>
      Общият брой статии е 243427. След премахването на XML етикетите и атрибутите, остава текст с размер 888134586
      байта. След декодирането на UTF-8, в който е записан текстът, броят символи става 885671734.
      Средният брой символи за статия е ~3638.35, Максималният e 738818, а минималният е 0. При работа с данните трябва
      да се обърне внимание на въпросните празните статии, като бъдат пропуснати или обработени по по-специфичен начин.
    </p>

    <div>
      <div style="float: left; margin: 10px 30px 0 0;">
        <img src="article-lengths.png">
        <i style="display: block; padding-left: 30px;">(фиг. 1)</i>
        <img src="article-lengths-log.png">
        <i style="display: block; padding-left: 30px;">(фиг. 2)</i>
      </div>

      <p>
        На <i>фиг. 1</i> по хоризонтала са изобразени възможните дължини на статии, а по вертикала - броят статии със
        съответната дължина. Причината графиката на пръв поглед да изглежда празна, е че кривата плътно следва
        абсцисната и ординатната оси. Това ни подсказва, че съществуват малък брой много дълги статии, но огромната част
        статиите са относително кратки.
      </p>

      <p>
        Данните придобиват малко повече смисъл, ако изобразим възможните дължини на статии по логаритмична скала
        (<i>фиг. 2</i>). Тогава, кривата започва да прилича на нормално разпределение. Това подсказва за
        доближаване до логнормално разпределение (разпределение на Галтон) в дължините на статиите.
      </p>

      <p>
        Наблюдава се мода на разпределението при около 30 символа за статия. Освен това повечето статии имат
        под 100 символа. Това подсказва, че по-късно, при формирането на batch-ове (todo-translate), дължини над 100
        би трябвало да са достатъчни, за да бъде трениран модела с цялостния текст на повечето статии.
      </p>
    </div>

    <h4 style="clear: both; padding-top: 30px;">Възможни символи</h4>

    <p>
      Броят символи, които се срещат в текстовете на статиите е 10797. Във <i>фиг. 3</i> е представена честотата на
      срещане на всеки символ. Най-вляво по хоризонтала са изобразени най-често срещаните символи, а
      най-вдясно&mdash;най-рядко срещаните, като скалата е логаритмична (т.е. на числото <i>n</i> отговаря n-тия
      най-често срещан символ). По вертикала са изобразени броят срещания за всеки символ&mdash;отново върху
      логаритмична скала.
    </p>

    <div style="float: left; margin: 10px 30px 10px 0;">
      <img src="chars-density.png">
      <i style="display: block; padding-left: 30px;">(фиг. 3)</i>
    </div>

    <p>
      За такъв тип данни очакваме да важи емпирично установеният закон на Ципф<a href="#reference-zipfs-law"><sup>[x]</sup></a>.
      Върху логаритмично-логаритмична графика това би изглеждало като права линия от най-често срещания символ (горе-ляво)
      до най-рядко срещания (долу-дясно). Откриваме интересно отклонение в рамките на първите 100 символа. Освен това
      е интересно, че ако разгледаме единствено 10-те най-често срещани символа, те изпълняват закона на Ципф.
      С известно отклонение&mdash;същото може да се каже и първите 90 символа.
    </p>

    <p>
      Първоначалната интуиция на автора е, че първите 10 символа изпъкват, защото са част от Wikitext синтаксиса.
      При проверка се оказва, че символите са: интервал и буквите <i>e</i>, <i>a</i>, <i>t</i>, <i>i</i>, <i>o</i>, <i>n</i>, <i>r</i>, <i>s</i>, <i>l</i>.
      Интуицията е невярна. Но в рамките на следващите 90 символа могат да бъдат видени символите от Wikitext синтаксиса.
      Също така там се среща цялата латиница, всички десетични цифри, много пунктуационни знаци и кирилската буква <i>и</i>.
    </p>

    <div>
      <div style="float: left; margin: 10px 30px 10px 0;">
        <img src="chars-distribution.png">
        <i style="display: block; padding-left: 30px;">(фиг. 4)</i>
      </div>
    </div>

    <p>
      Интересен е и друг прочит на данните. Графиката във <i>фиг. 4</i> изброява по хоризонтала символите по честота на
      срещане&mdash;подобно на <i>фиг. 3</i>, но този път по линейна скала. За всеки символ по вертикала са разположени
      сумата от броя срещанията на въпросния символ, както и броят срещания на всички по-рядко срещани от него символи.
      Така <i>фиг. 4</i> до известна степен прилича на графика на дискретното кумулативно разпределение, но ротирано
      около ординатната ос. Изобразени по този начин, данните могат да отговорят на въпроса:
    </p>

    <div style="text-align: center;">
      ако се вземе подмножеството от <i>n</i> на брой най-често срещани символа,<br>
      каква част от символите в набора от данни<br>
      ще бъдат извън въпросното множество?
    </div>

    Например:
    <ul>
      <li>
        ако се вземат първите 2000 символа, ще останат ~10<sup>5</sup> символа в набора от данни, които не са сред
        тези 2000 символа;
      </li>
      <li>Ако се вземат 6000 символа, ще останат ~10<sup>4</sup>;</li>
      <li>При 10000 символа, ще останат ~10<sup>3</sup>.</li>
    </ul>

    В граничните случаи:
    <ul>
      <li>ако се вземат всички символи, ще останат 0 невзети (долу-дясно в графиката);</li>
      <li>ако не се вземат никакви символи, всички ще останат невзети (горе-ляво в графиката).</li>
    </ul>

    <p>
      Тази интерпретация намира приложение в глава "7.2 Размер на азбуката".
    </p>

    <h4 style="clear: both; padding-top: 30px;">Малки и големи букви</h4>

    <p>
      Интересен е един подходите, приложен в алгоритъма <i>nncp</i>><a href="#reference-nncp"><sup>[x]</sup></a>. Във въпросния
      алгоритъм, Фабрис Белар използва предварителна обработка на текстови файлове, директно взета от имплементацията
      на алгоритъма <i>cmix</i>><a href="#reference-cmix"><sup>[x]</sup></a>. Една от стъпките е всички букви да бъдат превърнати
      в малки.
    </p>

    <p>
      За брой на символите се получава 10333. Това е само с 464 по-малко от броя на всички символи. Изглежда огромната част
      от символите нямат съответстваща малка буква. Това може да бъде обяснено както с пунктуационни или специални
      символи, така и със символи от писмености които не разграничават малки от големи букви (напр. много източно
      азиатски писмености). Но в случая чистата бройка не е от такова значение. По-голям интерес представляват
      разпределението и плътността.
    </p>

    <div style="display: flex;">
      <div style="float: left;">
        <img src="lower-chars-density.png">
        <i style="display: block; padding-left: 30px;">(фиг. 5)</i>
      </div>
      <div style="float: left; margin-top: 3px;">
        <img src="lower-chars-distribution.png">
        <i style="display: block; padding-left: 30px;">(фиг. 6)</i>
      </div>
    </div>

    <p>
      На <i>фиг. 5</i> и <i>фиг. 6</i> са изобразени графики аналогични съответно с <i>фиг. 3</i> и <i>фиг. 4</i>.
      Със син цвят са изобразени данните от <i>фиг. 3</i> и <i>фиг. 4</i>, а в оранжев цвят са наложени данните получени
      при преобразувани главни букви към малки. Двете криви се движат плътно заедно. В такъв мащаб е трудно да се
      забележи смислена разлика.
    </p>

    <div>
      <div style="float: left; margin: 10px 30px 10px 0;">
        <img src="lower-chars-distribution-zoomed.png">
        <i style="display: block; padding-left: 30px;">(фиг. 7)</i>
      </div>
    </div>

    <p>
      Разликата става по-очевидна, когато се разгледат първите 200 най-често срещани символа в двата случая.
      Кривата за тях е изобразена на <i>фиг. 7</i>. Данните са идентични с тези в горната лява част на <i>фиг. 6</i>.
    </p>

    <p>
      Ако големите букви бъдат запазени, е възможно с около 100 символа да се обхванат над 99% от всички символи в
      набора от данни. Ако бъдат взети единствено малки букви, същото може да бъде постигнато с около 70 символа.
    </p>

    <p>
      Ползите, които могат да бъдат извлечени, се разглеждат в глава "7.2 Размер на азбуката".
    </p>

    <h4 style="clear: both; padding-top: 30px;">Заглавия</h4>

    <div>
      <div style="float: left; margin: 10px 30px 10px 0;">
        <img src="title-lengths.png">
        <i style="display: block; padding-left: 30px; padding-bottom: 30px;">(фиг. 8)</i>
        <img src="title-lengths-log.png">
        <i style="display: block; padding-left: 30px;">(фиг. 9)</i>
      </div>

      <div>
        <p>
          След премахването на XML етикетите, общата дължина на заглавията в байтове е 4596137, а в символи&mdash;4590258.
          Естествено, броят заглавия е колкото броя статии&mdash;243427. Най-дългото заглавие е 188 символа, а
          най-краткото&mdash;0. Подобно на текста на статиите, и при заглавията присъстват празни данни.
        </p>

        <p>
          На <i>фиг. 8</i> по хоризонтала са изобразени възможните дължини на заглавия, а по вертикала - броят заглавия
          със съответната дължина. По подобие на текста на статиите наблюдаваме доближаване до логнормално
          разпределение&mdash;по-добре видимо на <i>фиг. 9</i>.
        </p>

        <p>
          Броят уникални символи е 275&mdash;драстично по-малко от същата мярка при статиите. Очевидното обяснение е, че
          в английската Wikipedia има основно заглавия на английски. Докато в пълния текст на статията има препратки,
          етимологии и всевъзможни връзки към други езици.
        </p>
      </div>
    </div>

    <h4 style="clear: both; padding-top: 30px;">Автори</h4>
    <p>
      За всяка от статиите присъства запис с автор на ревизията. Общо - 243427 записа за автор. При проверка с кратка
      програма, може да бъде открито, че голямата част от авторите се повтарят. Броят на уникалните записи е:
    </p>

    <ul>
      <li>19506 записа с потребителско име и числов идентификатор</li>
      <li>26258 записа с IP адрес</li>
      <li>12 записа на интервали от IP адреси, записани като IP адрес (например - "129.33.49.xxx")</li>
      <li>7 записа със служебен автор, записани като IP адрес (например - "Conversion script")</li>
    </ul>

    <p>
      Общият брой силно надвишава броя на уникалните автори&mdash;има 45783 уникални записа, което е ~19% от всички.
      С проста дедупликация може да бъде извлечена известна компресия.
    </p>

    <p>
      Това се прилага в глава "6.1 Предварителна обработка на данните".
    </p>
  </div>

  <div class="pages">
    <h2>Hutter Prize (todo: превод)</h2>

    <p>
      През 2006-та година, Маркъс Хътър стартира състезанието
      Hutter Prize(todo: превод)<a href="#reference-hutter-prize"><sup>[x]</sup></a>&mdash;практическа задача за компресия на човешко знание.
      Самата задача променя правилата си във времето. Правилата, очертани в текущата итерация на Hutter Prize, са следните:

      <ul>
        <li>
          да се създаде компресираща програма, която да компресира определен набор от данни и да произведе декомпресираща програма;
        </li>
        <li>
          изпълнението на декомпресираща програма да произведе файл идентичен на оригиналния набор от данни;
        </li>
        <li>
          компресирането и декомпресирането да се изпълняват сумарно за по-малко от 100 часа, да заемат по-малко от 10GiB
          оперативна памет и по-малко от 100GiB дисково пространство;
        </li>
        <li>
          да се постигне възможно най-малка сума от размерите на компресираща и декомпресираща програми.
        </li>
      </ul>
    </p>

    <h3>Текущи най-добри резултати</h3>

    <p>
      В текущия си вид, Hutter Prize има единствен победител&mdash;Александър Ратушняк на 4 юли 2019 с алгоритъма
      <i>phda9</i> (версия 1.8). Достигнатият размер е 116673681 байта<a href="#reference-hutter-prize"><sup>[x]</sup></a>.
    </p>

    <p>
      В предишни итерации на състезанието, когато използваният набор от данни е <i>enwik8</i>, победители са:
    </p>

    <ul>
      <li>4 ноември 2017 - Александър Ратушняк. Размер&mdash;15284944 байта, достигнат с алгоритъма <i>phda9</i>;</li>
      <li>23 май 2009 - Александър Ратушняк. Размер&mdash;15949688 байта, достигнат с алгоритъма <i>decomp8</i>;</li>
      <li>14 май 2007 - Александър Ратушняк. Размер&mdash;16481655 байта, достигнат с алгоритъма <i>paq8hp12</i>;</li>
      <li>25 септември 2006 - Александър Ратушняк. Размер&mdash;байта, 17073018 достигнат с алгоритъма <i>paq8hp5</i>;</li>
      <li>24 март 2006 - Мат Махони. Размер&mdash;18324887 байта, достигнат с алгоритъма <i>paq8f</i>.</li>
    </ul>

    <p>
      Всички тези алгоритми използват модел за предвиждане на вероятности. Вероятностите се използват като входни данни
      на алгоритъм за ентропийно кодиране. Това е стандартен подход, използва и в тази дипломна работа. Повече
      имплементационни детайли са описани в глава "6 Предложен подход за компресия на наборите от данни enwikX".
    </p>

    <p>
      Друго общо за алгоритмите е, че спадат в категорията на context mixing (todo: превод) алгоритмите за компресия.
      При тях има няколко статистически модела, които дават независими предвиждания за следващи категории. Категориите
      могат да бъдат байтове или символи, но обикновено се работи на ниво битове. Вероятностите получени от всеки от
      моделите се смесват по определен начин и се получават окончателни вероятностите за всяка
      категория<a href="#reference-context-mixing"><sup>[x]</sup></a>.
    </p>

    <p>
      Интересен е и алгоритъмът <i>cmix</i>, който постига размер 14838332 байта за <i>enwik8</i> и 115714367 байта за
      <i>enwik9</i>. С такъв резултат <i>cmix</i> би бил първи, но не се вмества в изискванията за време и
      памет&mdash;за <i>enwik9</i> са му нужни ~334 часа и ~25GiB оперативна памет<a href="#reference-compression-benchmark"><sup>[x]</sup></a>.
      Подходът при него отново е context mixing (todo: превод), но един от моделите, които се mix-ират (todo: превод)
      е LSTM рекурентна невронна мрежа.
    </p>

    </p>
      Алгоритъм, който вече беше споменат в друг контекст е <i>nncp</i>. Алгоритъмът отново е комбинация от предвиждащ
      модел и ентропийно кодиране. Постигнатият резултат е 119167224 байта<a href="#reference-compression-benchmark"><sup>[x]</sup></a>.
      Причината да представлява интерес е, че за модел се използва единствено LSTM рекурентна невронна мрежа.
      И въпреки че не постига най-добър резултат, той все пак успява да се доближи до него, и демонстрира приложимостта
      на подхода.
    </p>

    <p>
      <div>Чернова</div>
      context mixing не е подробно изследван<br>
      Паралел на context mixing-а (todo: превод), който заслужава да се спомене, са ансамболовите методи за обучение (ensemble learning -- todo: превод)
      и хибридните препоръчващи системи. При тях отново има множество модели, чиито резултати се mix-ират (todo: превод).
      Добре изследвано е, че те почти винаги получават по-добри резултати от единствен модел (todo - цитати:
      https://arxiv.org/pdf/1901.03888 https://arxiv.org/abs/1106.0257 https://dl.acm.org/doi/10.1109/TKDE.2005.99).
      <br>
      Възможността за mix-ирането на предложения подход с други модели остава извън целите на тази дипломна работа.
      Но е интересна възможност.
    </p>

    <h3>Забележки относно правилата</h3>

    <p>
      За да бъде предложеният подход за компресия съвместим правилата на Hutter Prize (todo: превод), трябва преди всичко да имплементира
      компресия без загуби (lossless -- todo: превод). Доброто представяне във времето също е фактор, който трябва да
      се вземе предвид. Но това не е първостепенна цел. В голямата част от случаите, ще бъде счетено, че 100 часа
      време за изпълнение са напълно достатъчни, без да се навлиза в изчисления и оценки на предложения подход.
      Времето за изпълнение се споменава изрично, единствено при колебание, в резултат на практически експерименти.
    </p>

    <p>
      От първостепенно значение остават размерите на компресиращата и декомпресираща програми. Но за да бъде давана
      оценка на размерите, първо трябва да бъде предложен конкретен подход.
    </p>

  </div>

  <div class="pages">
    <h2>Предложен подход за компресия на наборите от данни <i>enwikX</i></h2>

    <p>
      Построените предвиждащи модели дават оценки на вероятностите за възможни следващи символи в набора от данни.
      Компресия се постига, като на базата на тези вероятности, чрез ентропийно кодиране, на по-вероятни символи се
      съпоставят по-кратки последователности от битове, а на по-малко вероятни&mdash;по-дълги. Разглеждат се кодиране
      на Хъфман и аритметично кодиране, както и отражението им върху feature engineering-а [todo: превод], който прилагаме.
    </p>

    <p>
      За целите на компресията, всеки символ от азбуката на възможни символи се разглежда като категория.
      А цялостният текст се разглежда като дискретна последователност от категории.
      Компресирането и декомпресирането на тази дискретна последователност от категории се случва чрез ентропийно кодиране.
      Съществуват различни алгоритми за ентропийно кодиране, като някои примери са кодиране на Шанън, кодиране на Шанън-Фано,
      кодиране на Хъфман, аритметично кодиране, интервално кодиране (todo: range coding - превод) и други.
    </p>

    <p>
      Общото за ентропийните кодирания е, че съпоставят по-кратки последователности от битове на по-вероятни символи, а на по-малко
      вероятни&mdash;по-дълги. Теоретичната граница на такава компресия може да бъде получена чрез формулата:
      <div style="text-align: center;">TODO</div>
      ... където H e ентропията на Шанън:
      <div style="text-align: center;">TODO</div>
      ... спомената в "3.1 Ентропия на Шанън и оптималната компресия"
    </p>

    <p>
      В ентропията на Шанън вероятността P(...) идва от предварително избран статистически модел. Изборът на
      такъв статистически модел представлява друга съществена част от дипломната работа. Тази роля може да
      бъде изпълнена от всевъзможни статистически модели с дискретни стъпки (например - Вериги на Марков,
      Скрити модели на Марков, n-грамни модели и т.н.). В случая са избрани рекурентни невронни мрежи, заради
      способността им да запомнят зависимости, както краткосрочно, така и
      дългосрочно<sup><a href="#reference-lstm">[x]</a><a href="#reference-gru">[x]</a></sup>, както и да
      предвиждат категории, отчитайки контекст<a href="#reference-unreasonable-rnn-effectiveness"><sup>[x]</sup></a>.
    </p>

    <p>
      В допълнение на това, описаният по-нагоре подход се прилага единствено за заглавието и текстовото съдържание на
      всяка от статиите. За останалите полета от XML набора от данни (описани в "4.3 Изследване на данните"),
      се прилага отделна обработка, описана в "6.1 Предварителна обработка на данните".
    </p>

    <p>
      По описания начин получаваме следния алгоритъм за компресия:<br>
      <ul>
        <li>Обработва се XML набора от данни, извличат се полетата, и се отделят заглавието и текстовото съдържание от метаданните.</li>
        <li>Метаданните преминават през отделна обработка и се записват в декомпресиращата програма</li>
        <li>Заглавието и текстовото съдържание се конкатенират, а след това, от тях се получава дискретна последователност от категории.</li>
        <li>Категории се оформят в batch-ове (todo: превод) и се използват за трениране на рекурентна невронна мрежа</li>
        <li>Полученият предвиждащ модел се записва в декомпресиращата програма</li>
        <li>
          За всяка от категориите в дискретна последователност:
          <ul>
            <li>Подаваме текущата категория на модела</li>
            <li>Взимаме двойките категория-вероятност, които моделът предвижда</li>
            <li>Подаваме текущата категория и двойките категория-вероятност на избраното ентропийно кодиране</li>
            <li>В декомпресиращата програма записваме низа от битове, върнати от ентропийното кодиране</li>
            <li>Повтаряме докато има оставащи категории</li>
          </ul>
        </li>
      </ul>
    </p>

    <p>
      Получаваме и следния алгоритъм за декомпресия:<br>
      <ul>
        <li>Декомпресиращата програма изчита предвиждащия модел.</li>
        <li>
          За всеки от битовете, получени при ентропийно кодиране:
          <ul>
            <li>Взимаме двойките категория-вероятност, които моделът предвижда</li>
            <li>Четем битове докато получим еднозначна, за ентропийното кодиране, категория</li>
            <li>Запомняме получената категория в списък от категории</li>
            <li>Подаваме получената категория на модела</li>
            <li>Повтаряме докато има оставащи битове</li>
          </ul>
        </li>
        <li>Декомпресиращата програма изчита метаданните.</li>
        <li>Комбинираме метаданните и списъкът от получени категории, за да реконструираме оригиналните данни</li>
      </ul>
    </p>

    <p>
      Трябва да се отбележи, че никъде в компресирания файл не присъстват вероятностите, нужни за ентропийно кодиране.
      Вместо това те се изчисляват от модела на всяка стъпка.
    </p>

    <h3>Предварителна обработка на данните</h3>

    <h3>Рекурентни невронни мрежи</h3>

    <h4>Класически подход и проблеми</h4>

    <h4>Свръх-нагаждане (overfitting)</h4>

    <h4>Feature Engineering</h4>

    <h3>Ентропийно кодиране</h3>

    <h4>Кодиране на Хъфман</h4>

    <h4>Аритметично кодиране</h4>
  </div>

  <div class="pages">
    <h2>Експерименти</h2>

    <div><b>ЧЕРНОВА</b></div>
    Експериментите обхващат период от около половин година. Голямата част са осъществени на GPU NVidia RTX 2060 SUPER.
    В различни моменти се разглеждат:
    <br>
    GRU vs LSTM
    <br>
    RNN с брой слоеве: 2, 3, 4, 5
    <br>
    Слоеве с различен брой RNN units (todo: превод): 256, 376, 512, 768, 1024, 1280
    <br>
    Мрежи със и без embedding layer (todo: превод)
    <br>
    Азбуки с размер: 72, 256
    <br>
    Речници от под-думи с размери: 128, 256, 512, 1024, 2048, 4096
    <br>
    Наборите от данни: enwik8 и enwik9
    <br>
    Общият брой комбинации е 2 * 4 * 6 * 2 * 2 * 6 * 2 = 2304.
    <br><br>
    Средното време за трениране варира много, тъй като размерите на модела и входните данни могат да се различават с
    порядъци. Но е разумно като средно време за трениране на един модел да се дадат 48 часа. Така изчерпателното изследване на всички
    комбинации би отнело 12 години и половина.
    <br><br>
    Любопитно е да се отбележи, че в началото, експерименти бяха извършвани върху процесор AMD A10 7890K - четириядрен,
    осемнишков, с честота 4.1GHz и 8GiB оперативна памет. При преминаването към трениране на GPU, беше установено
    ускорение в размер на около 300 (триста) пъти. Куриозно е, че върху този хардуер, изчерпването на всички комбинации
    би отнело над 3800 (три хиляди и осемстотин) години.
    <br><br>
    Към момента на писане на дипломанта работа, гореспоменатата видео карта е един от най-мощните начини за трениране
    на невронни мрежи. Изчислителната ѝ мощ се оценява на около 7 терафлопа при single precision (todo: превод) изчисления.
    Съществуват и по-добри решения, но те започват да навлизат в сегмента на супер-компютрите и не са широкодостъпни за
    крайни потребители. Такава алтернатива е извън възможностите на автора.
    <br><br>
    Поради тази причина, изследването на параметрите на модела е силно ограничено. На много места се прилагат сравнения,
    за които не е гарантирано, че важат в цялото пространство от параметри. Цялостният подход за трениране повече прилича
    на hill-climbing (todo: превод) отколкото на пълно изчерпване (brute-force).

    <h3>LSTM срещу GRU</h3>

    <h3>Размер на азбуката</h3>

    <h3>Размер на речник от под-думи</h3>

    <h3>Брой и размер на рекурентните слоеве</h3>
  </div>

  <div class="pages">
    <h2>Методи и материали</h2>

    <p>
      Текстът на дипломанта работа е написан на HTML/CSS.
      За синтактичното оцветяване е използвана библиотеката PrismJS<!-- https://prismjs.com/ --> и цветовата ѝ схема
      prism-vs.<!-- https://github.com/PrismJS/prism-themes/blob/master/themes/prism-vs.css -->
      За математически формули е използван TeX, който бива изобразен като стандартен MathML.
      За разширена MathML съвместимост, както и за поддръжката на подмножество от езика TeX се използва библиотеката MathJax.
    </p>

    <p>
      Всички експерименти са проведени в Python 3.7 с помощта на <i>Jupyter</i> 1.0.0.
      За общи изчисления е използван <i>numpy</i> 1.18.1.
      За графики е използван <i>matplotlib</i> 3.2.0.
      За получаване на речника от под-думи, описан в "5 Размер на речник от под-думи" се използва класът
      <i>SubwordTextEncoder</i> от библиотеката <i>tensorflow-datasets</i> 3.1.0.
      За експерименти с невронни мрежи е използван <i>Tensorflow</i> - версии 2.1 и 2.2.
    </p>

    <p>
      Огромната част от експериментите в <i>Tensorflow</i> са проведени върху видео карта NVidia GeForce RTX 2060 SUPER,
      разполагаща с 8GiB оперативна памет и 272 тензорни ядра.
    </p>

    <p>
      Правилата на Hutter Prize търпят известни ревизии във времето. В момента на започване на тази дипломна работа,
      в правилата се включва единствено размера на декомпресиращата програма. По тази причина за първоначалния код,
      имплементиращ "6.1 Предварителна обработка на данните", е избран езика Java и библиотеките <i>xjc</i> и <i>jaxb</i>.
      Изборът е естествен, поради удобството и широкия набор от инструменти за работата с XML и XML схеми, предоставени
      от Java. Но в хода на дипломната работа, в правилата се включва и размера на компресиращата програма.
      Така се стига до текущия вид на правилата, описан в глава "5 Hutter Prize". С втората ревизия на правилата,
      използването на Java се превръща в пречка, заради големият обем на получените изпълними файлове и неяснотите около
      дистрибутиране на файловете асоциирани с Java виртуалната машина.
    </p>

    <p>
      Така във финалната ревизия, за имплементациите на компресиращата и декомпресираща програми, е използван C++.
      Използвани са известно количество конструкции от C++17. Кодът е компилиран върху Microsoft Visual Studio 2019,
      но е достатъчно преносим, за да работи на всеки друг C++17 компилатор.
    </p>

    <p>
      За обработка на XML е използвана библиотеката <i>libstudxml</i>. <!--https://www.codesynthesis.com/projects/libstudxml/-->
      Библиотеката е сметната за подходяща заради способността ѝ да обработката на невалидни XML документи. Това се
      налага заради липсващи затварящи тагове&mdash;особеност описана в глава "4.1 MediaWiki page export format".
      Друго предимство е, че библиотеката е достъпна под MIT лиценз. Това контрастира на конкурентите ѝ, които
      използват относително овързващи варианти на GPL лиценза.
    </p>
  </div>

  <div class="pages">
    <h2>Резултати и дискусии</h2>

    <div>Чернова</div>
    Бъдеща работа:
    да се изследва възможността за постигане на резултат с transformer;
    да се изследва приложимостта на данните от графа (чрез embedding-и -- todo: превод);
    да се тренира с bfloat16
    context mixing чрез множество RNN модели

    <h2>Заключение</h2>
  </div>

  <div class="pages references">
    <h2>Библиография</h2>
    <ol>
      <li id="reference-turing-test">
        Turing, Alan Mathison (July 1950). "COMPUTING MACHINERY AND INTELLIGENCE". Computing Machinery and Intelligence. Mind 49: 433-460.
      </li>
      <li id="reference-employment-test">
        Nilsson, Nils John (WINTER 2005). "Human-Level Artificial Intelligence? Be Serious!". AI MAGAZINE
      </li>
      <li id="reference-robot-college-test">
        Goertzel, Ben (5 September 2012). "What counts as a conscious thinking machine?". NewScientist Magazine issue 2881;
        https://www.newscientist.com/article/mg21528813-600-what-counts-as-a-conscious-thinking-machine/
      </li>
      <li id="reference-mc-carthy-on-intelligence">
        McCarthy, John (12 November 12 2007). "WHAT IS ARTIFICIAL INTELLIGENCE?".
        http://www-formal.stanford.edu/jmc/whatisai/node1.html
      </li>
      <li id="reference-hutter-ai-theory">
        Hutter, Marcus (September 2001). "Towards a Universal Theory of Artificial Intelligence based on Algorithmic Probability and Sequential Decisions",
        Lecture Notes in Artificial Intelligence (LNAI 2167), Proc. 12th European Conf. on Machine Learning, ECML (2001) 226--238
      </li>
      <li id="reference-mahoney-on-compression">
        Mahoney, Matt (20 August 2006). "Rationale for a Large Text Compression Benchmark",
        https://cs.fit.edu/~mmahoney/compression/rationale.html
      </li>
      <li id="reference-hutter-prize">
        Hutter, Marcus (March 2006) "50'000€ Prize for Compressing Human Knowledge",
        http://prize.hutter1.net/
      </li>
      <li id="reference-hutter-incomputablility">
        Jan Leike and Marcus Hutter (20 October 2015). "On the Computability of AIXI". UAI 2015. arXiv:1510.05572
      </li>
      <li id="reference-shannon-human-baseline">
        Shannon, Cluade E. (15 September 1950). “Prediction and Entropy of Printed English”, Bell Sys. Tech. J (3) p. 50-64, 1950.
      </li>
      <li id="reference-overfitting for compression">
        Egawa, Hiroki & Shibata, Yuichiro. (January 2019). "Storing and Compressing Video into Neural Networks by Overfitting".
        10.1007/978-3-319-93659-8_56.
      </li>
      <li id="reference-unreasonable-rnn-effectiveness">
        Karpathy, Andrej (21 May 2015). "The Unreasonable Effectiveness of Recurrent Neural Networks"
        http://karpathy.github.io/2015/05/21/rnn-effectiveness/
      </li>
      <li id="reference-jim-bowery-c-prize">
        Bowery, Jim (May 2005) "The C-Prize",
        https://groups.google.com/forum/#!topic/comp.compression/JHxrwaMMkv0
      </li>
      <li id="reference-media-wiki-xml-xsd">
        Wikimedia Foundation. MediaWiki's page export format XML schema
        https://www.mediawiki.org/xml/export-0.10.xsd
      </li>
      <li id="reference-data-exploration">
        Stratos Idreos, Olga Papaemmonouil, Surajit Chaudhuri. "Overview of Data Exploration Techniques". FOSTER Open Science.
        https://www.fosteropenscience.eu/sites/default/files/pdf/2933.pdf
      </li>
      <li id="reference-zipfs-law">
        Powers, David M W (1998). "Applications and explanations of Zipf's law". Association for Computational Linguistics: 151–160.
      </li>
      <li id="reference-nncp">
        Bellard, Fabrice (4 May 2019). "Lossless Data Compression with Neural Networks". https://bellard.org/nncp/nncp.pdf
      </li>
      <li id="reference-cmix">
        Knoll, Byron (1 August 2019). CMIX version 18, http://www.byronknoll.com/cmix.html.
      </li>
      <li id="reference-compression-benchmark">
        Mahoney, Matt (9 Mar. 2020). "Large Text Compression Benchmark".
        http://mattmahoney.net/dc/text.html
      </li>
      <li id="reference-context-mixing">
        Mahoney, M. (2005), "Adaptive Weighing of Context Models for Lossless Data Compression", Florida Tech. Technical Report CS-2005-16
      </li>
      <li id="reference-lstm">
        Sepp Hochreiter; Jürgen Schmidhuber (15 November 1997). "Long short-term memory".
        Neural Computation. 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735. PMID 9377276.
      </li>
      <li id="reference-gru">
        Cho, Kyunghyun; van Merrienboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua (2014).
        "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation". arXiv:1406.1078
      </li>
      <li id="reference-1">
        Shannon, Claude Elwood (July 1948). "A Mathematical Theory of Communication". Bell System Technical Journal. 27 (3): 379–423
      </li>
      <li id="reference-2">
        Shannon, Claude Elwood (October 1948). "A Mathematical Theory of Communication". Bell System Technical Journal. 27 (4): 623–666
      </li>
    </ol>
  </div>
</body>
