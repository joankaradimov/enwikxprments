<head>
  <style>
    * {
      box-sizing: border-box;
      text-justify: auto;
    }

    @page {
      size: A4;
      padding: 2cm;
      margin: 1cm auto;
    }

    @media print {
      .page {
        border: initial;
        border-radius: initial;
        width: initial;
        min-height: initial;
        box-shadow: initial;
        background: initial;
        page-break-after: always;
      }
    }

    .page {
      width: 21cm;
      min-height: 29.7cm;
      padding: 2cm;
      margin: 1cm auto;
      border: 1px #D3D3D3 solid;
      border-radius: 3px;
      background: white;
    }

    sup {
      font-size: 7px;
    }

    ol { counter-reset: item; }
    li{ display: block; }
    li:before {
      content: counters(item, ".") " ";
      counter-increment: item;
    }
  </style>
</head>
<body style="width: 800px; margin: auto;">
  <div class="page" style="text-align: center;">
    <h1>
      Компресиране на някои представяния на знание с приложение на рекурентни невронни мрежи
    </h1>
    <p>ТОДО: дата, име, ръководител, катедра(?)</p>
    <i>
      <div>joan.karadimov@gmail.com</div>
      <div>Факултет по математика и информатика</div>
      <div>Софийски Университет</div>
    </i>

    <p style="text-align: center; font-weight: bold;">Абстракт</p>
    <div style="text-align: center;">TODO</div>
  </div>

  <div class="page">
    <h2>1. Въведение</h2>

    <p>
      Постигането на силен изкуствен интелект и обща интелигентност е една от задачите, с които изкуствения
      интелект (ИИ) се сблъсква още от създаването си. Въпреки относително бурното развитие на ИИ през последните
      години, прогресът в областта на общата интелигентност за момента е слаб. От части това може да се обясни с
      хардуерни ограничения, лимитиращи способността ни да достигаме смислени практически резултати. Но реалността е, че
      липсват универсално утвърдени теоретични числени формализации, които да служат като насока. И по-важното
      - да се използват като валидация на научни трудове и експерименти.
    </p>

    <p>
      Добре известен е тестът на Тюринг <a href="#reference-turing-test"><sup>[x]</sup></a>. Но още в първата глава
      "The Imitation Game" на основополагащата си статия, Тюринг отказва да се ангажира с дефиниция на интелект и
      вместо това избира да търси алтернативен въпрос. Подобен, но по-малко известен е тестът на работното място
      (The employment test)<a href="#reference-employment-test"><sup>[x]</sup></a>, формулиран от Нилс Нилсън - автор
      на алгоритъма A* и планировчика STRIPS. Целта е да се провери дали агент може да се справи поне толкова
      добре колкото човек в служба съществена за икономиката. Тестът на студента-робот (The Robot College Student
      Test)<a href="#reference-robot-college-test"><sup>[x]</sup></a> на Бен Гьорцел цели да провери способността на агент
      успешно да премине и завърши пълен университетски курс.
    </p>

    <p>
      Множество други автори предлагат подобни тестове. И въпреки че много от тях представят интересни перспективи,
      обединяващо е че никой не предлага строга формализация или подход към решаването на проблема.
    </p>

    <h3>Предмет на дипломната работа</h3>

    <p>
      В книгата си Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability, в търсене на
      числен модел на интелекта, Маркъс Хътър предлага формализма на универсалния алгоритмичен AIXI агент. Формализмът е
      обвързан с идеите за универсална индукция на Рей Соломонов и за сложност по Колмогоров. Но Хътър отбелява, че
      въпреки възможността да се счете AIXI за формална дефиниция на математическо решение на ИИ, такова решение не е
      практично заради неизчислимостта му<a href="#reference-hutter-incomputablility"><sup>[x]</sup></a>.
    </p>

    <p>
      Като стъпка в посоката на практическа теория на ИИ Хътър предлага AIXItl &mdash; изчислим модел на AIXI, разглеждащ
      програми ограничени по размер и време за изчисление. Това се допълва от преставената от него теорема за най-бърз
      и най-кратък алгоритъм<a href="#"><sup>[todo: цитат - страница 219]</sup></a>, която грубо казано твърди, че
      съществува единствена програма, която е най-бързата и най-кратката. Иначе казано - има единствена програма, която
      води до най-добро приближение на AIXItl до AIXI.
      [todo: малко повече пояснения, може би? Или мястото им не е в увода...]
    </p>

    <div style="font-style: italic; display: inline-block; margin-bottom: 12px;">
      <div style="float: left; width: 38%; margin: 0 3%;">
        [...] being able to compress well is closely related to acting intelligently, thus reducing the slippery concept
        of intelligence to hard file size numbers. In order to compress data, one has to find regularities in them, which
        is intrinsically difficult.
      </div>

      <div style="float: right; width: 50%; margin: 0 3%;">
        [...] способността да се компресира добре е тясно свързана с възможността да действаш интелигентното. Така
        неясната идея за интелигентност може да се сведе размери на файлове и числа. А за да се компресират данни, е
        нужно да се намерят закономерности в тях, което е присъщо трудно.
      </div>
    </div>
    <div style="text-align: right; font-style: italic; margin-bottom: 12px;">
      [TODO -- citation; Marcus Hutter; ]
    </div>

    <p>
      Добрите модели се описват от по-кратки програми. За по-кратка програма, може да се каже че е компресирана.
      В такъв случай може да се твърди, че демонстрирането на възможност за добро компресиране е тясно свързано с
      построяването на добър предвиждащ модел.
      От друга страна разбирането на входните данни позволява да се построи добър предвиждащ модел за данните.
      Отворен е въпросът дали построяването на добър модел предполага разбиране. Хътър хипотетизира, че двете са еквивалентни [todo: citation needed].
      Базирайки се на това, може да се твърди, че описването на модел в по-кратка (компресирана) програма е еквивалентно на разбиране.
    </p>

    <p>
      Разглеждането на компресия като еквивалентна на разбиране предоставя мощен инструмент. Разполагайки с набор от
      данни, способността да се смали размера на данните с даден коефициент дава числова мярка на разбирането.
    </p>

    <p>
      Интересен е въпросът - ако сравняваме алгоритми за компресия, какъв набор от данни бихме избрали. Очевиден отговор
      е "цялото човешко знание". Но такъв отговор няма добра практическа стойност. Нужна е някаква текстова
      репрезентация на възможно най-голяма част от човешкото знание. Wikipedia, например, е един добър кандидат в този смисъл. От една страна заради
      съществения обем, който предлага. От друга - заради отвореността и лесният достъп до данните ѝ.
    </p>

    <p>
      Това води към същински практически проблем - може ли да се постигне добра компресия на данните в Wikipedia?
      В частност - набори от данни наречени enwikX, където X ∈ {7, 8, 9, 10} и съдържа първите 10<sup>X</sup> байта от
      XML репрезентация на данните в Wikipedia.
      Естествен въпрос е какво означава "добра" компресия. Обективна мярка е "по-добър от съществуващия машинен подход".
      Но по-често използваната субективна мярка е "по-добра от човек".
      <br>
      [todo: да спомена Шанън - Prediction and Entropy of Printed English - 15 септември 1951 -- коефициентът, под който представянето е по-добро от човек]
      <br>
      [todo: да спомена, че не съществува оптимална не-ентропийна компресия]
    </p>

    <h3>Цел на дипломната работа</h3>

    <p>
      Целта на тази дипломна работа е, използвайки актуални изследвания и технологии, да се построи модел за предвиждане на данните в enwik.
      Моделът се базира на рекурентни невронни мрежи - изследвани са LSTM и GRU. Необичайно в случая е стремежът към
      overfitting [todo: превод]. Друго нехарактерно е feature engineering-а [todo: превод],
      който се прилага върху данните. Този подход, необходим за много от алгоритмите за машинно самоубочение, обикновено
      се заобиколя при невронните мрежи. Но в случая, feature engieneering-а се налага, и има паралели с някои
      подходи при класически алгоритми за компресия.
    </p>

    <p>
      Получените модели дават оценка на вероятностите за възможни следващи символи. Това се комбинира с подходи за
      ентропийно кодиране. Разглеждат се кодове на Хъфман и аритметично кодиране, както и отражението им върху feature
      engineering-а [todo: превод], който прилагаме.
    </p>
  </div>

  <div class="page">
    <h2>2. Съдържание -- TODO</h2>
    <ol>
      <li>
        Въведение
        <ol>
          <li>Предмет на дипломната работа</li>
          <li>Цел на дипломната работа</li>
        </ol>
      </li>
      <li>Съдържание</li>
      <li>
        Компресията като мярка на интелект и разбиране
        <ol>
          <li>Ентропия на Шанън и оптималната компресия</li>
          <li>Сложност по Колмогоров и универсална индукция на Соломонов</li>
          <li>AIXI и AIXItl</li>
          <li>Теорема за най-бърз и най-кратък алгоритъм</li>
        </ol>
      </li>
      <li>
        Наборите от данни enwikX
        <ol>
          <li>WikiMedia XML</li>
          <li>Wikitext</li>
          <li>Изследване на данните</li>
        </ol>
      </li>
      <li>
        Предложен подход за компресия на наборите от данни enwikX
        <ol>
          <li>Текущи най-добри резултати</li>
          <li>Kодове на Хъфман и аритметично кодиране</li>
          <li>
            Рекурентни невронни мрежи
            <ol>
              <li>Класически подход и проблеми</li>
              <li>Overfitting</li>
              <li>Feature Engineering</li>
            </ol>
          </li>
        </ol>
      </li>
      <li>
        Изследване на хиперпараметри
        <ol>
          <li>LSTM срещу GRU</li>
          <li>Размер на азбуката</li>
          <li>Размер на речник от поддуми</li>
          <li>Брой и размер на рекурентните слоеве</li>
        </ol>
      </li>
      <li>
        Методи и материали -- todo: дали не трябва да е по-напред?
        <ol>
          <li>Използвани инструменти</li>
          <li>Приложен метод</li>
        </ol>
      </li>
      <li>Резултати и дискусии</li>
      <li>Заключение</li>
    </ol>
  </div>

  <div class="page">
    <h2>?. Методи и материали -- TODO</h2>

    <h3>Използвани инструменти -- TODO</h3>
    <p>
      Python 3.7
      <br>
      tensorflow 2.1, 2.2
      <br>
      tensorflow-datasets
      <br>
      jupyter
      <br>
      numpy
      <br>
      C++ 17
      <br>
      libstudxml - https://www.codesynthesis.com/projects/libstudxml/
      <br>
      NVIDIA GeForce RTX 2060 SUPER
      <br>
      Java, xjc, jaxb -- legacy
    </p>

    <h2>Библиография</h2>
    <div style="font-size: x-small;">
      <div id="reference-turing-test">
        [x] Turing, Alan Mathison (July 1950). "COMPUTING MACHINERY AND INTELLIGENCE". Computing Machinery and Intelligence. Mind 49: 433-460.
      </div>
      <div id="reference-employment-test">
        [x] Nilsson, Nils John (WINTER 2005). "Human-Level Artificial Intelligence? Be Serious!". AI MAGAZINE
      </div>
      <div id="reference-robot-college-test">
        [x] Ben Goertzel (5 September 2012). "What counts as a conscious thinking machine?". NewScientist Magazine issue 2881;
        https://www.newscientist.com/article/mg21528813-600-what-counts-as-a-conscious-thinking-machine/
      </div>
      <div id="reference-hutter-incomputablility">
        [x] Jan Leike and Marcus Hutter (20 October 2015). "On the Computability of AIXI". UAI 2015. arXiv:1510.05572
      </div>
      <div id="reference-1">
        [1] Shannon, Claude Elwood (July 1948). "A Mathematical Theory of Communication". Bell System Technical Journal. 27 (3): 379–423
      </div>
      <div id="reference-2">
        [2] Shannon, Claude Elwood (October 1948). "A Mathematical Theory of Communication". Bell System Technical Journal. 27 (4): 623–666
      </div>
    </div>
  </div>
</body>
