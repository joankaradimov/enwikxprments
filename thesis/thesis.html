<head>
  <style>
    * {
      box-sizing: border-box;
      text-justify: auto;
    }

    @page {
      size: A4;
      padding: 2cm;
      margin: 1cm auto;
    }

    .pages {
      width: 21cm;
      min-height: 29.7cm;
      padding: 2cm;
      margin: 1cm auto;
      border: 1px #D3D3D3 solid;
      border-radius: 3px;
    }

    @media print {
      .pages {
        border: initial;
        border-radius: initial;
        width: initial;
        min-height: initial;
        box-shadow: initial;
        background: initial;
        page-break-after: always;
      }
    }

    sup {
      font-size: 7px;
    }

    .contents ol { counter-reset: item; }
    .contents ol > li { display: block; }
    .contents ol > li:before { content: counters(item, ".") " "; counter-increment: item; }

    .references ol { counter-reset: item; }
    .references ol > li { display: block; }
    .references ol > li:before { content: "[" counters(item, '') "]"; counter-increment: item; }

    pre {
      white-space: pre-wrap;
      word-break: break-word;
    }
  </style>

  <script>
MathJax = {
  options: {
    renderActions: {
      find: [10, function (doc) {
        for (var node of document.querySelectorAll('script[type^="math/tex"]')) {
          var display = !!node.type.match(/; *mode=display/);
          var math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
          var text = document.createTextNode('');
          node.parentNode.replaceChild(text, node);
          math.start = {node: text, delim: '', n: 0};
          math.end = {node: text, delim: '', n: 0};
          doc.math.push(math);
        }
      }, '']
    }
  }
};
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body style="width: 800px; margin: auto;">
  <div class="pages" style="text-align: center;">
    <h1>
      Компресиране на някои представяния на знание с приложение на рекурентни невронни мрежи
    </h1>
    <p>ТОДО: дата, име, ръководител, катедра(?)</p>
    <i>
      <div>joan.karadimov@gmail.com</div>
      <div>Факултет по математика и информатика</div>
      <div>Софийски Университет</div>
    </i>

    <p style="text-align: center; font-weight: bold;">Абстракт</p>
    <div style="text-align: center;">TODO</div>
  </div>

  <div class="pages">
    <h2>1. Въведение</h2>

    <p>
      Постигането на силен изкуствен интелект и обща интелигентност е една от задачите, с които изкуствения
      интелект (ИИ) се сблъсква още от създаването си. Въпреки относително бурното развитие на ИИ през последните
      години, прогресът в областта на общата интелигентност за момента е слаб. От части това може да се обясни с
      хардуерни ограничения, лимитиращи способността ни да достигаме смислени практически резултати. От друга страна -
      преди въобще да се търси смислен резултат в контекста на силния изкуствен интелект, трябва да бъде дефиниран
      начин за проверка и тестване на въпросния резултат.
    </p>

    <p>
      Добре известен е тестът на Тюринг <a href="#reference-turing-test"><sup>[x]</sup></a>. Но още в първата глава
      "The Imitation Game" на основополагащата си статия, Тюринг отказва да се ангажира с дефиниция на интелект и
      вместо това избира да търси алтернативен въпрос. Подобен, но по-малко известен е тестът на работното място
      (The employment test)<a href="#reference-employment-test"><sup>[x]</sup></a>, формулиран от Нилс Нилсън.
      Целта е да се провери дали агент може да се справи поне толкова
      добре колкото човек в служба, съществена за икономиката. Тестът на студента-робот (The Robot College Student
      Test)<a href="#reference-robot-college-test"><sup>[x]</sup></a> на Бен Гьорцел цели да провери способността на агент
      успешно да премине и завърши пълен университетски курс.
    </p>

    <p>
      Множество други автори предлагат подобни тестове. И въпреки че много от тях представят интересни перспективи,
      обединяващо е, че никой не предлага строга формализация или подход към решаването на проблема. Иначе казано -
      изброените тестове дават крайна цел, но не и явна функция, която да оптимизираме, за да стигнем до силен
      изкуствен интелект. За такова начинание, за начало, е нужна общоприета дефиниция на интелект. По темата,
      Джон МакКарти казва: "Проблемът е, че като цяло, все още не можем да характеризираме кои изчислителни процедури да
      наречем интелигентни"<a href="#reference-mc-carthy-on-intelligence"><sup>[x]</sup></a>. Липсата на такава
      общоприета и твърда дефиниция на интелект оставя място за интерпретация, но и създава интересни възможности за
      изследвания.
    </p>

    <h3>Математическо моделиране на интелекта</h3>

    <p>
      В книгата си Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability, в търсене на
      числен модел на интелекта, Маркъс Хътър предлага формализма на универсалния алгоритмичен агент&mdash;AIXI.
      AIXI е агент за обучение с утвърждение, чиято цел е на базата на предишни свои действия, както и на
      взаимодействие дадена среда, описана чрез машина на Тюринг - да максимизира целева награда, оценена с число.
      Хътър доказва, че оптималното поведение на такъв агент е да предполага, че средата, с която взаимодейства,
      е описана от най-кратката възможна програма<a href="#reference-hutter-ai-theory"><sup>[x]</sup></a>.
    </p>

    <p>
      Агентът не разполага с функцията, описваща на средата в явен вид. Така неговата цел става построяването на най-малък
      възможен модел на въпросната среда, базирайки се на взаимодействието си с нея. Тъй като такъв модел би бил описан
      с програма, целта на агента, на практика, е да компресира наблюденията върху средата си до най-кратката такава
      програма<a href="#reference-mahoney-on-compression"><sup>[x]</sup></a>.
    </p>

    <div style="font-style: italic; display: inline-block; margin-bottom: 12px;">
      <div style="float: left; width: 38%; margin: 0 3%;">
        [...] being able to compress well is closely related to acting intelligently, thus reducing the slippery concept
        of intelligence to hard file size numbers. In order to compress data, one has to find regularities in them, which
        is intrinsically difficult.
      </div>

      <div style="float: right; width: 50%; margin: 0 3%;">
        [...] способността да се компресира добре е тясно свързана с възможността да действаш интелигентното. Така
        неясната идея за интелигентност може да се сведе размери на файлове и числа. А за да се компресират данни, е
        нужно да се намерят закономерности в тях, което е присъщо трудно.
      </div>
    </div>
    <div style="text-align: right; font-style: italic; margin-bottom: 12px;">
      Маркъс Хътър<a href="#reference-hutter-prize"><sup>[x]</sup></a>
    </div>

    <p>
      Разглеждането на компресия като еквивалентна на интелект предоставя мощен инструмент. Разполагайки с набор от
      данни, способността да се смали размера на данните с даден коефициент, дава числова мярка на интелекта.
    </p>

    <h3>Цел на дипломната работа</h3>

    <p>
      През 2006-та година, Маркъс Хътър стартира състезанието
      Hutter Prize(todo: превод)<a href="#reference-hutter-prize"><sup>[x]</sup></a>&mdash;практическа задача за компресия на човешко знание.
      Целта на тази дипломна работа е да очертае подход за компресия, съвместим с изискването на състезанието. Това се постига,
      като се строи модел за предвиждане на данни, чрез използването на актуални изследвания и технологии.
      Изследването набляга на прилагането на рекурентни невронни мрежи за строенето на модела&mdash;и
      по-специално&mdash;LSTM и GRU. Тези архитектури са показали своята ефективност в предвиждането на последователности
      от данни<a href="#reference-unreasonable-rnn-effectiveness"><sup>[x]</sup></a>.
    </p>

    <p>
      Построените предвиждащи модели дават оценки на вероятностите за възможни следващи символи в набора от данни.
      Компресия се постига, като на базата на тези вероятности, чрез ентропийно кодиране, на по-вероятни символи се
      съпоставят по-кратки последователности от битове, а на по-малко вероятни&mdash;по-дълги. Разглеждат се кодиране
      на Хъфман и аритметично кодиране, както и отражението им върху feature engineering-а [todo: превод], който прилагаме.
    </p>

    <p>
      Интересен е въпросът&mdash;ако се сравняват алгоритми за компресия&mdash;какъв набор от данни да бъде избран.
      Очевиден отговор е "цялото човешко знание". Но такъв отговор няма добра практическа стойност.
      Нужна е някаква текстова репрезентация на възможно най-голяма част от човешкото знание.
      Като стандартен benchmark (todo: превод) Джим Бауъри предлага текстовото съдържание на
      Wikipedia<a href="#reference-jim-bowery-c-prize"><sup>[x]</sup></a>. Тя е един добър кандидат, тъй като
      притежава съществен обем. От друга страна е подходяща заради отвореността и лесният достъп до данните ѝ.
      Хътър подкрепя идеята и решава да използва Wikipedia за целите на Hutter Prize(todo: превод).
    </p>

    <p>
      Това е и практическият проблем, който разглеждаме&mdash;постигането на добра компресия на данните в Wikipedia.
      В частност&mdash;върху набори от данни наречени <i>enwikX</i>, където X ∈ {5, 6, 7, 8, 9} и съдържа първите
      10<sup>X</sup> байта от XML репрезентация на данните в Wikipedia.
    </p>

    <p>
      Естествен въпрос е какво означава "добра" компресия. Често използвани мерки са "по-добра от съществуващия машинен подход",
      както и "по-добра от човек". В контекста на силния изкуствен интелект, второто представлява по-голям интерес.
      Изследвайки ентропия и компресия, Клод Шанън провежда експерименти с хора, опитвайки се да оцени способността
      им да служат като модел за предвиждане на текст. Използвайки 27 символа - буквите от латиницата в английския,
      допълнени със символ за интервал - Шанън оценява способността на хората на еквивалентна на компресия в
      границите между 0.6 и 1.3 бита за символ<a href="#reference-shannon-human-baseline"><sup>[x]</sup></a>.
      Махони провежда симулации с конкретни модели и оценява същата мярка на около 1 бит за символ, като добавянето на
      пунктуация и разграничаването на големи/малки букви вдига мярката до 1.25 бита за
      символ<a href="#reference-mahoney-on-compression"><sup>[x]</sup></a>.
    </p>

    <p>
      Необичайно в случая е, че избягването на свръх-нагаждането (overfitting) няма да бъде цел. Интересно е, че за
      компресия чрез невронни мрежи, може дори да бъде полезно<a href="#reference-overfitting for compression"><sup>[x]</sup></a>.
      Друго нехарактерно е feature engineering-а [todo: превод], който се прилага върху данните.
      Този подход, необходим за много от алгоритмите за машинно самообучение, обикновено се заобиколя при невронните мрежи.
      Но в случая, feature engieneering-а се налага, и има паралели с някои подходи при класически алгоритми за компресия.
    </p>

    <h3>План на целите</h3>

    <h4>Изследване и предварителна обработка на данните</h4>

    <p>
      В глава "5.2 Предварителна обработка на данните" се описва обработката на XML данните и отделянето на метаданните
      от същинското текстово съдържание на статиите. Разглежда се наивна стратегия за компресиране на метаданните, която
      не използва подходи от машинното самообучение и изкуствения интелект, но притежава интересни паралели с
      преобразуванието на Бъроуз-Уилър. За самото текстово съдържание, се описва интерпретацията му като последователност
      от категории. Разглеждат се начини за намаляване на броя на категориите и за съкращаване на входните данни.
    </p>

    <h4>Имплементации и изследване на ентропийни кодирания</h4>

    <p>
      В глава "5.3 Kодове на Хъфман и аритметично кодиране" се сравняват асимптотичните сложности и някои практически
      проблеми при две от най-разпространените ентропийни кодировки. Сравнява се представянето им във времето и се
      демонстрират идеи за оптимизации.
    </p>

    <h4>Моделиране с невронни мрежи</h4>

    <p>
      В глава "5.4 Рекурентни невронни мрежи" се разглежда способността на рекурентни невронни мрежи да служат
      като модели за предвиждане на последователности от данни. Представят се резултати от други изследвания и се описва
      общата им структура, таксономията им, и развитието им във времето. Мотивира се избора на разглежданите архитектури
      и се дават някои алтернативи. Дефинират се свръх-нагаждане (overfitting) и Feature Engineering и се описват начините, по които
      употребата им в предложения подход се различава от традиционните им приложения.
    </p>

    <h4>Експерименти</h4>

    <p>
      В глава "6 Експерименти" се разглежда избора на конкретна архитектура на рекурентни невронни мрежи. Изследват се
      хиперпараметрите на въпросните невронни мрежи - брой слоеве, размер на embedding (todo: превод) слой, размер на
      рекурентен (todo: превод) слой, както и по-общите хиперпараметри - размер на азбуката от символи и брой на n-грами
      (todo: превод), които се разглеждат.
    </p>
  </div>

  <div class="pages contents">
    <h2>2. Съдържание -- TODO</h2>
    <ol>
      <li>
        Въведение
        <ol>
          <li>Математическо моделиране на интелекта</li>
          <li>Цел на дипломната работа</li>
          <li>План на целите</li>
        </ol>
      </li>
      <li>Съдържание</li>
      <li>
        Компресията като мярка на интелект
        <ol>
          <li>Ентропия на Шанън и оптималната компресия</li>
          <li>Сложност по Колмогоров и универсална индукция на Соломонов</li>
          <li>AIXI и AIXItl</li>
          <li>Теорема за най-бърз и най-кратък алгоритъм</li>
        </ol>
      </li>
      <li>
        Наборите от данни <i>enwikX</i>
        <ol>
          <li>MediaWiki page export format</li>
          <li>Wikitext</li>
          <li>Изследване на данните</li>
        </ol>
      </li>
      <li>
        Предложен подход за компресия на наборите от данни <i>enwikX</i>
        <ol>
          <li>Текущи най-добри резултати</li>
          <li>Предварителна обработка на данните</li>
          <li>Kодове на Хъфман и аритметично кодиране</li>
          <li>
            Рекурентни невронни мрежи
            <ol>
              <li>Класически подход и проблеми</li>
              <li>Свръх-нагаждане (overfitting)</li>
              <li>Feature Engineering</li>
            </ol>
          </li>
        </ol>
      </li>
      <li>
        Експерименти
        <ol>
          <li>LSTM срещу GRU</li>
          <li>Размер на азбуката</li>
          <li>Размер на речник от под-думи</li>
          <li>Брой и размер на рекурентните слоеве</li>
        </ol>
      </li>
      <li>
        Методи и материали -- todo: дали не трябва да е по-напред?
        <ol>
          <li>Използвани инструменти</li>
          <li>Приложен метод</li>
        </ol>
      </li>
      <li>Резултати и дискусии</li>
      <li>Заключение</li>
    </ol>
  </div>

  <div class="pages">

    <h2>Компресията като мярка на интелект</h2>

    <h3>Ентропия на Шанън и оптималната компресия</h3>

    <p>
      <script type="math/tex; mode=display">H(X) = -\sum_i P_X(x_i) \log_b{P_X(x_i)}</script>
    </p>

    <div>ЧЕРНОВА</div>
    <i>
    <p>
      Kак се третира предвиждащ модел, който просто знае тестовия набор от данни наизуст? Тогава,
      изчислявайки информационната ентропия на този набор от данни, ще се получи нулева ентропия. Но за произволен друг
      набор от данни, ентропията би била не-нулева и висока. На практика това означава, че съществува алгоритъм за
      компресия, произвеждащ най-добрата възможна компресия (0 бита) за нашия набор от данни, но даващ много лоша
      компресия за всичко друго.
    </p>

    <p>
      На пръв поглед този проблем може да се избегне, ако вместо един набор от данни, оценяваме и втори. Но тогава,
      можем просто да вземем модел, който знае двата набора от данни наизуст. Тогава компресията ще работи почти идеално
      за тях, но ще се проваля за всичко друго.
    </p>
    </i>

    <h3>Сложност по Колмогоров и универсална индукция на Соломонов</h3>

    [todo: да спомена, че не съществува оптимална не-ентропийна компресия]

    <h3>AIXI и AIXItl</h3>

    <!--p>
      Формализмът е
      обвързан с идеите за универсална индукция на Рей Соломонов и за сложност по Колмогоров. Но Хътър отбелязва, че
      въпреки възможността да се счете AIXI за формална дефиниция на математическо решение на ИИ, такова решение не е
      практично заради неизчислимостта му<a href="#reference-hutter-incomputablility"><sup>[x]</sup></a>.
    </p>

    <p>
      Като стъпка в посоката на практическа теория на ИИ Хътър предлага AIXItl &mdash; изчислим модел на AIXI, разглеждащ
      програми ограничени по размер и време за изчисление. Това се допълва от представената от него теорема за най-бърз
      и най-кратък алгоритъм<a href="#"><sup>[todo: цитат - страница 219]</sup></a>, която грубо казано твърди, че
      съществува единствена програма, която е най-бързата и най-кратката. Иначе казано - има единствена програма, която
      води до най-добро приближение на AIXItl до AIXI.
      [todo: малко повече пояснения, може би? Или мястото им не е в увода...]
    </p>

    <p>
      Добрите модели се описват от по-кратки програми. За по-кратка програма, може да се каже че е компресирана.
      В такъв случай може да се твърди, че демонстрирането на възможност за добро компресиране е тясно свързано с
      построяването на добър предвиждащ модел.
      От друга страна, разбирането на входните данни, изискващо интелект, позволява да се построи добър предвиждащ модел за данните.
      Отворен е въпросът, дали построяването на добър модел предполага интелект. Хътър хипотетизира, че двете са еквивалентни [todo: citation needed].
      Базирайки се на това, може да се твърди, че описването на модел в по-кратка (компресирана) програма е еквивалентно на интелект.
    </p-->

    <h3>Теорема за най-бърз и най-кратък алгоритъм</h3>

    <h2>Наборите от данни <i>enwikX</i></h2>

    <p>
      Съществуват няколко набора от данни, носещи подобните имена: <i>enwik6</i>, <i>enwik7</i>, <i>enwik8</i>,
      <i>enwik9</i>, и <i>enwik10</i>. Общият вид на имената е enwikX, където X ∈ {5, 6, 7, 8, 9}. Всеки от тях
      представлява първите 10<sup>X</sup> байта от една определена текстова репрезентация на свободно достъпната
      енциклопедия Wikipedia. Данните са взети единствено от англоезичната версия.
    </p>

    <p>
      Въпросните набори от данни са широко използвани в
      изследвания в областта на ИИ, и по-конкретно - в областите на моделиране и синтез на естествени езици. Честият
      избор, на набори от данни базирани на Wikipedia, при тези изследвания, е продиктуван от няколко фактора:
      <ul>
        <li>
          Данните са достъпни под два лиценза - "GNU Free Documentation License" и "Creative Commons Attribution-ShareAlike".
          И двата позволяващи употребата и разпространението на данните с много малко ограничения.
        </li>
        <li>
          Данните имат относително високото качество, често с множество автори и редактори за всяка от статиите.
        </li>
        <li>
          Данните са взети от файл, чиито оригинален размер е 4.8 GiB - над 2 500 000 страници текст, при 2000
          символа на страница - безспорно значително количество текст.
        </li>
      </ul>
    </p>

    <p>
      Трябва да се отбележи, че данните са във вида, в който са съществували в Wikipedia на 3 март 2006. Към момента
      обемът на Wikipedia е над 4 пъти по-голям. Разумно е и да спекулираме, че качеството на статиите се е повишило.
      Но въпреки това, enwikX не се обновяват &ndash; това гарантира консистентност на изследванията във времето.
    </p>

    <h3>MediaWiki page export format</h3>

    <p>
      Преди започване на изследване на данните, е добре да се разгледа репрезентацията им. Вече беше споменато, че
      наборът от данни е текстов, но не беше уточнен конкретен формат и съдържание. Те ще бъдат предмет на интерес
      занапред в дипломната работа, и заслужава да бъдат разгледани по-подробно. Първите 15 реда могат да бъдат получени с Unix
      командата "<i>head -n 15 enwik9</i>":

      <pre>
        <code>
&lt;mediawiki xmlns=&quot;http://www.mediawiki.org/xml/export-0.3/&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd&quot; version=&quot;0.3&quot; xml:lang=&quot;en&quot;&gt;
&lt;siteinfo&gt;
  &lt;sitename&gt;Wikipedia&lt;/sitename&gt;
  &lt;base&gt;http://en.wikipedia.org/wiki/Main_Page&lt;/base&gt;
  &lt;generator&gt;MediaWiki 1.6alpha&lt;/generator&gt;
  &lt;case&gt;first-letter&lt;/case&gt;
    &lt;namespaces&gt;
    &lt;namespace key=&quot;-2&quot;&gt;Media&lt;/namespace&gt;
    &lt;namespace key=&quot;-1&quot;&gt;Special&lt;/namespace&gt;
    &lt;namespace key=&quot;0&quot; /&gt;
    &lt;namespace key=&quot;1&quot;&gt;Talk&lt;/namespace&gt;
    &lt;namespace key=&quot;2&quot;&gt;User&lt;/namespace&gt;
    &lt;namespace key=&quot;3&quot;&gt;User talk&lt;/namespace&gt;
    &lt;namespace key=&quot;4&quot;&gt;Wikipedia&lt;/namespace&gt;
    &lt;namespace key=&quot;5&quot;&gt;Wikipedia talk&lt;/namespace&gt;
        </code>
      </pre>

      Моментално е видно, че става въпрос за XML-базиран формат. Форматът се нарича <i>MediaWiki page export format</i>
      (todo: превод в допълнение на официалното английско име на формата?), описан е в XML
      схема<a href="#reference-media-wiki-xml-xsd"><sup>[x]</sup></a>, и съдържа статии, метаданни за статиите,
      както и общи метаданни. Откъсът по-горе описва общи метаданни, и по-специално в случая &ndash; името на сайта,
      адреса му, версията на използвания формат, както и част от основните секции в Wikipedia.
    </p>

    <p>
      Трябва да бъде отбелязано, че никой от <i>enwikX</i> наборите от данни не представлява валиден XML документ.
      Причината е, че всеки от тях съдържа определено количество данни, започващи от началото на валиден документ
      от тип MediaWiki page export format. Взимането на съдържание единствено от началото и изрязването му от края,
      води до незатворени XML етикети. Това от своя страна неизбежно води до споменатата липса на валидност.
    </p>

    <p>
      По-напред в съдържанието се намират данни и за самите статии. Например - редове от 120 до 140 могат да бъдат
      разгледани чрез "<i>head -n 120 enwik9 | tail -n 20</i>"

      <pre>
        <code>
&lt;/revision&gt;
&lt;/page&gt;
&lt;page&gt;
  &lt;title&gt;AdA&lt;/title&gt;
  &lt;id&gt;11&lt;/id&gt;
  &lt;revision&gt;
    &lt;id&gt;15898946&lt;/id&gt;
    &lt;timestamp&gt;2002-09-22T16:02:58Z&lt;/timestamp&gt;
    &lt;contributor&gt;
      &lt;username&gt;Andre Engels&lt;/username&gt;
      &lt;id&gt;300&lt;/id&gt;
    &lt;/contributor&gt;
    &lt;minor /&gt;
    &lt;text xml:space=&quot;preserve&quot;&gt;#REDIRECT [[Ada programming language]]&lt;/text&gt;
  &lt;/revision&gt;
&lt;/page&gt;
&lt;page&gt;
  &lt;title&gt;Anarchism&lt;/title&gt;
  &lt;id&gt;12&lt;/id&gt;
  &lt;revision&gt;
        </code>
      </pre>

      Това е статия за програмния език Ада. Като метаданни могат да бъдат открити името на статията и уникалния ѝ числов
      идентификатор. Прави впечатление XML етикета '&lt;revision&gt;'. Причината за съществуването му е, че Wikipedia
      поддържа история от редакциите на всяка статия, наричани ревизии. В <i>enwikX</i> наборите от данни,
      винаги присъства единствено последната ревизия. Това е и мястото, на което могат да бъдат открити някои от
      най-важните данни - уникален числов идентификатор на ревизията, време на създаване, автор, индикация дали
      редакцията е малка, коментар (липсващ тук), и текст на статията.
    </p>

    <h3>Wikitext</h3>

    <p>
      В примера по-горе, текстът на примерната статия е "#REDIRECT [[Ada programming language]]". Очевидно пунктуацията
      и специалните символи са повече, отколкото бихме очаквали, от текст на естествен език. Причината е, че текстът е
      във формат Wikitext &ndash; markup (todo: превод) език специално създаден за целите на Wikipedia. (todo: да разкажа за markup езици и за подобни формати, може би?)
    </p>

    <p>
      Форматът позволява декларирането на секции, подсекции, връзки към други статии и техни секции, автоматично
      пренасочване към други статии и техни секции, влагане на изображения, списъци с подточки, таблици и други.
      Между редове 135 и 140, например, може да бъде открито:

      <pre>
        <code>
          The word '''anarchism''' is [[etymology|derived from]] the [[Greek language|Greek]] ''[[Wiktionary:&amp;amp;#945;&amp;amp;#957;&amp;amp;#945;&amp;amp;#961;&amp;amp;#967;&amp;amp;#943;&amp;amp;#945;|&amp;amp;#945;&amp;amp;#957;&amp;amp;#945;&amp;amp;#961;&amp;amp;#967;&amp;amp;#943;&amp;amp;#945;]]'' (&amp;quot;without [[archon]]s (ruler, chief, king)&amp;quot;). Anarchism as a [[political philosophy]], is the belief that ''rulers'' are unnecessary and should be abolished, although there are differing interpretations of what this means. Anarchism also refers to related [[social movement]]s) that advocate the elimination of authoritarian institutions, particularly the [[state]].&amp;lt;ref&amp;gt;[http://en.wikiquote.org/wiki/Definitions_of_anarchism Definitions of anarchism] on Wikiquote, accessed 2006&amp;lt;/ref&amp;gt; The word &amp;quot;[[anarchy]],&amp;quot; as most anarchists use it, does not imply [[chaos]], [[nihilism]], or [[anomie]], but rather a harmonious [[anti-authoritarian]] society. In place of what are regarded as authoritarian political structures and coercive economic institutions, anarchists advocate social relations based upon [[voluntary association]] of autonomous individuals, [[mutual aid]], and [[self-governance]].

          While anarchism is most easily defined by what it is against, anarchists also offer positive visions of what they believe to be a truly free society. However, ideas about how an anarchist society might work vary considerably, especially with respect to economics; there is also disagreement about how a free society might be brought about.
        </code>
      </pre>

      Откъсът съдържа примери за връзки към други статии и техни секции (оградени с двойни квадратни скоби), удебелен
      текст (тройки кавички) и други. Присъствието на толкова много специални символи, поставя под въпрос
      съпоставимостта с базовата (todo: превод на baseline) оценка от между 0.6 и 1.3 бита за символ, която Шанън дава.
      Оценката, както вече беше споменато, е за азбука от 27 символа, която игнорира числа, пунктуация, големи и малки букви
      и други възможно специални или служебни символи. Това поставя въпроса - какво може да бъде видяно в данните?
    </p>

    <h3>Изследване на данните</h3>

    Разглеждайки <i>enwikX</i> наборите от данни, както и тяхната XML схема, може да се достигне до списък от имена на
    полета с метаданни. В търсенето на подход за компресия, от значение са типовете на въпросните метаданни. Чрез
    кратка помощна програма, са изчислени и приложени сумарните размерите на данните, разбити по поле.

    <ul>
      <li>
        <b>Общи метаданни</b>:<br>
        Метаданни, които не са конкретни за определена статия, а общи за набора от данни.<br>
        Размер в байтове: 1403
      </li>
      <li>
        <b>Заглавие на статия</b><br>
        Текстов низ, съдържащ заглавието<br>
        Размер в байтове: TODO
      </li>
      <li>
        <b>Ограничения върху статията</b><br>
        Текстов низ в специфичен формат, описващ забрани за редактиране, преименуване и други<br>
        Размер в байтове: TODO
      </li>
      <li>
        <b>Идентификатор на статия</b><br>
        Число, уникално идентифициращо статия<br>
        Размер в байтове: TODO
      </li>
      <li>
        <b>Идентификатор на ревизия</b><br>
        Число, уникално идентифициращо ревизия на статия<br>
        Размер в байтове: TODO
      </li>
      <li>
        <b>Клеймо (todo: timestamp - превод) на ревизия</b><br>
        Текстов низ във формат ISO 8601, отбелязващ дата и време на създаване на ревизията<br>
        Размер в байтове: TODO
      </li>
      <li>
        <b>Автор на ревизия</b><br>
        TODO<br>
        Размер в байтове: TODO
      </li>
      <li>
        <b>Флаг за малка ревизия</b><br>
        Празен XML елемент, чието присъствие индикира, че промяната в ревизията е малка<br>
        Размер в байтове: TODO
      </li>
      <li>
        <b>Коментар към ревизия</b><br>
        Текстов низ, съдържащ коментари към ревизията<br>
        Размер в байтове: TODO
      </li>
      <li>
        <b>Текст за ревизията</b><br>
        Текстов низ, с текста на ревизията на статията<br>
        Размер в байтове: TODO
      </li>
    </ul>

    <br>
    todo: изследване на броя символи
    <div>
      <div style="float: left;">
        <img src="chars-density.png">
        <i style="display: block; padding-left: 30px;">(фиг. 1)</i>
      </div>
      <div style="float: left; margin-top: 3px;">
        <img src="chars-distribution.png">
        <i style="display: block; padding-left: 30px;">(фиг. 2)</i>
      </div>
    </div>

    <br>
    todo: малки срещу големи букви
    <div>
      <div style="float: left;">
        <img src="lower-chars-density.png">
        <i style="display: block; padding-left: 30px;">(фиг. 3)</i>
      </div>
      <div style="float: left; margin-top: 3px;">
        <img src="lower-chars-distribution.png">
        <i style="display: block; padding-left: 30px;">(фиг. 4)</i>
      </div>
    </div>

    <div>
      <div style="float: left;">
        <img src="lower-chars-distribution-zoomed.png">
        <i style="display: block; padding-left: 30px;">(фиг. 5)</i>
      </div>
    </div>

    <h4 style="clear: both; padding-top: 60px;">Статии</h4>

    Общият брой статии е 243427. При общ брой байтове 888134586, кодирани в UTF-8, броят символи е 885671734.
    Средният брой символи за статия е ~3638.35, Максималният e 738818, а минималният е 0.

    <div>
      <div style="float: left; margin: 10px 30px 0 0;">
        <img src="articles-length.png">
        <i style="display: block; padding-left: 30px;">(фиг. 6)</i>
      </div>

      На <i>фиг. 6</i> по хоризонтала са изобразени възможните дължини на статии (логаритмична скала), а по вертикала -
      броят статии със съответната дължина. Наблюдава се максимум при около 30 символа. Освен това повечето статии имат
      под 100 символа. Това подсказва, че по-късно, при формирането на batch-ове (todo-translate), дължини над 100
      би трябвало да са достатъчни, за да бъде трениран модела с цялостния текст на повечето статии.
    </div>

  </div>

  <div class="pages">
    <h2>Предложен подход за компресия на наборите от данни <i>enwikX</i></h2>

    <p>
      През 2006-та година, Маркъс Хътър стартира състезанието
      Hutter Prize(todo: превод)<a href="#reference-hutter-prize"><sup>[x]</sup></a>&mdash;практическа задача за компресия на човешко знание.
      Самата задача променя правилата си във времето. Правилата, очертани в текущата итерация на Hutter Prize, са следните:

      <ul>
        <li>
          да се създаде компресираща програма, която да компресира определен набор от данни и да произведе декомпресираща програма;
        </li>
        <li>
          изпълнението на декомпресираща програма да произведе файл идентичен на оригиналния набор от данни;
        </li>
        <li>
          компресирането и декомпресирането да се изпълняват сумарно за по-малко от 100 часа, да заемат по-малко от 10GiB
          оперативна памет и по-малко от 100GiB дисково пространство;
        </li>
        <li>
          да се постигне възможно най-малка сума от размерите на компресираща и декомпресираща програми.
        </li>
      </ul>
    </p>

    <p>
      За да бъде предложеният подход за компресия съвместим с тези правила, трябва преди всичко да имплементира
      компресия без загуби (lossless -- todo: превод). Доброто представяне във времето също е фактор, който трябва да
      се вземе предвид. Но това не е първостепенна цел. В голямата част от случаите, се счита, че 100 часа
      време за изпълнение са достатъчни, без да се навлиза в изчисления и оценки на предложения подход. Времето за
      изпълнение се споменава изрично, единствено при колебание, в резултат на практически експерименти.
    </p>

    <p>
      От първостепенно значение остават размерите на компресиращата и декомпресираща програми. Но преди да бъде давана
      оценка, първо трябва да бъде разгледан предложения подход...
    </p>

    <p>
      Построените предвиждащи модели дават оценки на вероятностите за възможни следващи символи в набора от данни.
      Компресия се постига, като на базата на тези вероятности, чрез ентропийно кодиране, на по-вероятни символи се
      съпоставят по-кратки последователности от битове, а на по-малко вероятни&mdash;по-дълги. Разглеждат се кодиране
      на Хъфман и аритметично кодиране, както и отражението им върху feature engineering-а [todo: превод], който прилагаме.
    </p>

    <p>
      За целите на компресията, всеки символ от азбуката на възможни символи се разглежда като категория.
      А цялостният текст се разглежда като дискретна последователност от категории.
      Компресирането и декомпресирането на тази дискретна последователност от категории се случва чрез ентропийно кодиране.
      Съществуват различни алгоритми за ентропийно кодиране, като някои примери са кодиране на Шанън, кодиране на Шанън-Фано,
      кодиране на Хъфман, аритметично кодиране, интервално кодиране (todo: range coding - превод) и други.
    </p>

    <p>
      Общото за ентропийните кодирания е, че съпоставят по-кратки последователности от битове на по-вероятни символи, а на по-малко
      вероятни&mdash;по-дълги. Теоретичната граница на такава компресия може да бъде получена чрез формулата:
      <div style="text-align: center;">TODO</div>
      ... където H e ентропията на Шанън:
      <div style="text-align: center;">TODO</div>
      ... спомената в "3.1 Ентропия на Шанън и оптималната компресия"
    </p>

    <p>
      В ентропията на Шанън вероятността P(...) идва от предварително избран статистически модел. Изборът на
      такъв статистически модел представлява друга съществена част от дипломната работа. Тази роля може да
      бъде изпълнена от всевъзможни статистически модели с дискретни стъпки (например - Вериги на Марков,
      Скрити модели на Марков, n-грамни модели и т.н.). В случая са избрани рекурентни невронни мрежи, заради
      способността им да запомнят зависимости, както краткосрочно, така и
      дългосрочно<sup><a href="#reference-lstm">[x]</a><a href="#reference-gru">[x]</a></sup>, както и да
      предвиждат категории, отчитайки контекст<a href="#reference-unreasonable-rnn-effectiveness"><sup>[x]</sup></a>.
    </p>

    <p>
      В допълнение на това, описаният по-нагоре подход се прилага единствено за заглавието и текстовото съдържание на
      всяка от статиите. За останалите полета от XML набора от данни (описани в "4.3 Изследване на данните"),
      се прилага отделна обработка, описана в "5.2 Предварителна обработка на данните".
    </p>

    <p>
      По описания начин получаваме следния алгоритъм за компресия:<br>
      <ul>
        <li>Обработва се XML набора от данни, извличат се полетата, и се отделят заглавието и текстовото съдържание от метаданните.</li>
        <li>Метаданните преминават през отделна обработка и се записват в декомпресиращата програма</li>
        <li>Заглавието и текстовото съдържание се конкатенират, а след това, от тях се получава дискретна последователност от категории.</li>
        <li>Категории се оформят в batch-ове (todo: превод) и се използват за трениране на рекурентна невронна мрежа</li>
        <li>Полученият предвиждащ модел се записва в декомпресиращата програма</li>
        <li>
          За всяка от категориите в дискретна последователност:
          <ul>
            <li>Подаваме текущата категория на модела</li>
            <li>Взимаме двойките категория-вероятност, които моделът предвижда</li>
            <li>Подаваме текущата категория и двойките категория-вероятност на избраното ентропийно кодиране</li>
            <li>В декомпресиращата програма записваме низа от битове, върнати от ентропийното кодиране</li>
            <li>Повтаряме докато има оставащи категории</li>
          </ul>
        </li>
      </ul>
    </p>

    <p>
      Получаваме и следния алгоритъм за декомпресия:<br>
      <ul>
        <li>Декомпресиращата програма изчита предвиждащия модел.</li>
        <li>
          За всеки от битовете, получени при ентропийно кодиране:
          <ul>
            <li>Взимаме двойките категория-вероятност, които моделът предвижда</li>
            <li>Четем битове докато получим еднозначна, за ентропийното кодиране, категория</li>
            <li>Запомняме получената категория в списък от категории</li>
            <li>Подаваме получената категория на модела</li>
            <li>Повтаряме докато има оставащи битове</li>
          </ul>
        </li>
        <li>Декомпресиращата програма изчита метаданните.</li>
        <li>Комбинираме метаданните и списъкът от получени категории, за да реконструираме оригиналните данни</li>
      </ul>
    </p>

    <p>
      Трябва да се отбележи, че никъде в компресирания файл не присъстват вероятностите, нужни за ентропийно кодиране.
      Вместо това те се изчисляват от модела на всяка стъпка.
    </p>

    <h3>Текущи най-добри резултати</h3>

    <h3>Предварителна обработка на данните</h3>

    <h3>Kодове на Хъфман и аритметично кодиране</h3>

    <h3>Рекурентни невронни мрежи</h3>

    <h4>Класически подход и проблеми</h4>

    <h4>Свръх-нагаждане (overfitting)</h4>

    <h4>Feature Engineering</h4>

  </div>

  <div class="pages">
    <h2>Експерименти</h2>

    <div><b>ЧЕРНОВА</b></div>
    Експериментите обхващат период от около половин година. Голямата част са осъществени на GPU NVidia RTX 2060 SUPER.
    В различни моменти се разглеждат:
    <br>
    GRU vs LSTM
    <br>
    RNN с брой слоеве: 2, 3, 4, 5
    <br>
    Слоеве с различен брой RNN units (todo: превод): 256, 376, 512, 768, 1024, 1280
    <br>
    Мрежи със и без embedding layer (todo: превод)
    <br>
    Азбуки с размер: 72, 256
    <br>
    Речници от под-думи с размери: 128, 256, 512, 1024, 2048, 4096
    <br>
    Наборите от данни: enwik8 и enwik9
    <br>
    Общият брой комбинации е 2 * 4 * 6 * 2 * 2 * 6 * 2 = 2304.
    <br><br>
    Средното време за трениране варира много, тъй като размерите на модела и входните данни могат да се различават с
    порядъци. Но е разумно като средно време за трениране на един модел да се дадат 48 часа. Така изчерпателното изследване на всички
    комбинации би отнело 12 години и половина.
    <br><br>
    Любопитно е да се отбележи, че в началото, експерименти бяха извършвани върху процесор AMD A10 7890K - четириядрен,
    осемнишков, с честота 4.1GHz и 8GiB оперативна памет. При преминаването към трениране на GPU, беше установено
    ускорение в размер на около 300 (триста) пъти. Куриозно е, че върху този хардуер, изчерпването на всички комбинации
    би отнело над 3800 (три хиляди и осемстотин) години.
    <br><br>
    Към момента на писане на дипломанта работа, гореспоменатата видео карта е един от най-мощните начини за трениране
    на невронни мрежи. Изчислителната ѝ мощ се оценява на около 7 терафлопа при single precision (todo: превод) изчисления.
    Съществуват и по-добри решения, но те започват да навлизат в сегмента на супер-компютрите и не са широкодостъпни за
    крайни потребители. Такава алтернатива е извън възможностите на автора.
    <br><br>
    Поради тази причина, изследването на параметрите на модела е силно ограничено. На много места се прилагат сравнения,
    за които не е гарантирано, че важат в цялото пространство от параметри. Цялостният подход за трениране повече прилича
    на hill-climbing (todo: превод) отколкото на пълно изчерпване (brute-force).

    <h3>LSTM срещу GRU</h3>

    <h3>Размер на азбуката</h3>

    <h3>Размер на речник от под-думи</h3>

    <h3>Брой и размер на рекурентните слоеве</h3>
  </div>

  <div class="pages">
    <h2>Методи и материали</h2>

    <h3>Използвани инструменти</h3>
    <p>
      Python 3.7
      <br>
      tensorflow 2.1, 2.2
      <br>
      tensorflow-datasets
      <br>
      jupyter
      <br>
      numpy
      <br>
      C++ 17
      <br>
      libstudxml - https://www.codesynthesis.com/projects/libstudxml/
      подходящ лиценз;
      позволява обработката на невалидни XML документи - с липсващи затварящи тагове
      В глава "4.1 MediaWiki page export format" са споменати проблемите, следващи от използването на невалиден XML документ;
      <br>
      NVIDIA GeForce RTX 2060 SUPER
      <br>
      Java, xjc, jaxb -- legacy
    </p>

    <h3>Приложен метод</h3>
  </div>

  <div class="pages">
    <h2>Резултати и дискусии</h2>

    <div>Чернова</div>
    Бъдеща работа:
    да се изследва възможността за постигане на резултат с transformer;
    да се изследва приложимостта на данните от графа (чрез embedding-и -- todo: превод);

    <h2>Заключение</h2>
  </div>

  <div class="pages references">
    <h2>Библиография</h2>
    <ol style="font-size: x-small;">
      <li id="reference-turing-test">
        Turing, Alan Mathison (July 1950). "COMPUTING MACHINERY AND INTELLIGENCE". Computing Machinery and Intelligence. Mind 49: 433-460.
      </li>
      <li id="reference-employment-test">
        Nilsson, Nils John (WINTER 2005). "Human-Level Artificial Intelligence? Be Serious!". AI MAGAZINE
      </li>
      <li id="reference-robot-college-test">
        Goertzel, Ben (5 September 2012). "What counts as a conscious thinking machine?". NewScientist Magazine issue 2881;
        https://www.newscientist.com/article/mg21528813-600-what-counts-as-a-conscious-thinking-machine/
      </li>
      <li id="reference-mc-carthy-on-intelligence">
        McCarthy, John (12 November 12 2007). "WHAT IS ARTIFICIAL INTELLIGENCE?".
        http://www-formal.stanford.edu/jmc/whatisai/node1.html
      </li>
      <li id="reference-hutter-ai-theory">
        Hutter, Marcus (September 2001). "Towards a Universal Theory of Artificial Intelligence based on Algorithmic Probability and Sequential Decisions",
        Lecture Notes in Artificial Intelligence (LNAI 2167), Proc. 12th Eurpean Conf. on Machine Learning, ECML (2001) 226--238
      </li>
      <li id="reference-mahoney-on-compression">
        Mahoney, Matt (20 August 2006). "Rationale for a Large Text Compression Benchmark",
        https://cs.fit.edu/~mmahoney/compression/rationale.html
      </li>
      <li id="reference-hutter-prize">
        Hutter, Marcus (March 2006) "50'000€ Prize for Compressing Human Knowledge",
        http://prize.hutter1.net/
      </li>
      <li id="reference-hutter-incomputablility">
        Jan Leike and Marcus Hutter (20 October 2015). "On the Computability of AIXI". UAI 2015. arXiv:1510.05572
      </li>
      <li id="reference-shannon-human-baseline">
        Shannon, Cluade E. (15 September 1950). “Prediction and Entropy of Printed English”, Bell Sys. Tech. J (3) p. 50-64, 1950.
      </li>
      <li id="reference-overfitting for compression">
        Egawa, Hiroki & Shibata, Yuichiro. (January 2019). "Storing and Compressing Video into Neural Networks by Overfitting".
        10.1007/978-3-319-93659-8_56.
      </li>
      <li id="reference-unreasonable-rnn-effectiveness">
        Karpathy, Andrej (21 May 2015). "The Unreasonable Effectiveness of Recurrent Neural Networks"
        http://karpathy.github.io/2015/05/21/rnn-effectiveness/
      </li>
      <li id="reference-jim-bowery-c-prize">
        Bowery, Jim (May 2005) "The C-Prize",
        https://groups.google.com/forum/#!topic/comp.compression/JHxrwaMMkv0
      </li>
      <li id="reference-media-wiki-xml-xsd">
        Wikimedia Foundation. MediaWiki's page export format XML schema
        https://www.mediawiki.org/xml/export-0.10.xsd
      </li>
      <li id="reference-lstm">
        Sepp Hochreiter; Jürgen Schmidhuber (15 November 1997). "Long short-term memory".
        Neural Computation. 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735. PMID 9377276.
      </li>
      <li id="reference-gru">
        Cho, Kyunghyun; van Merrienboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua (2014).
        "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation". arXiv:1406.1078
      </li>
      <li id="reference-1">
        Shannon, Claude Elwood (July 1948). "A Mathematical Theory of Communication". Bell System Technical Journal. 27 (3): 379–423
      </li>
      <li id="reference-2">
        Shannon, Claude Elwood (October 1948). "A Mathematical Theory of Communication". Bell System Technical Journal. 27 (4): 623–666
      </li>
    </ol>
  </div>
</body>
