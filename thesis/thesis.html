<head>
  <style>
    * {
      box-sizing: border-box;
      text-justify: auto;
    }

    @page {
      size: A4;
      padding: 2cm;
      margin: 1cm auto;
    }

    .page {
      width: 21cm;
      min-height: 29.7cm;
      padding: 2cm;
      margin: 1cm auto;
      border: 1px #D3D3D3 solid;
      border-radius: 3px;
    }

    @media print {
      .page {
        border: initial;
        border-radius: initial;
        width: initial;
        min-height: initial;
        box-shadow: initial;
        background: initial;
        page-break-after: always;
      }
    }

    sup {
      font-size: 7px;
    }

    ol { counter-reset: item; }
    ol > li{ display: block; }
    ol > li:before {
      content: counters(item, ".") " ";
      counter-increment: item;
    }

    pre {
      white-space: pre-wrap;
      word-break: break-word;
    }
  </style>
</head>
<body style="width: 800px; margin: auto;">
  <div class="page" style="text-align: center;">
    <h1>
      Компресиране на някои представяния на знание с приложение на рекурентни невронни мрежи
    </h1>
    <p>ТОДО: дата, име, ръководител, катедра(?)</p>
    <i>
      <div>joan.karadimov@gmail.com</div>
      <div>Факултет по математика и информатика</div>
      <div>Софийски Университет</div>
    </i>

    <p style="text-align: center; font-weight: bold;">Абстракт</p>
    <div style="text-align: center;">TODO</div>
  </div>

  <div class="page">
    <h2>1. Въведение</h2>

    <p>
      Постигането на силен изкуствен интелект и обща интелигентност е една от задачите, с които изкуствения
      интелект (ИИ) се сблъсква още от създаването си. Въпреки относително бурното развитие на ИИ през последните
      години, прогресът в областта на общата интелигентност за момента е слаб. От части това може да се обясни с
      хардуерни ограничения, лимитиращи способността ни да достигаме смислени практически резултати. От друга страна -
      преди въобще да се търси смислен резултат в контекста на силния изкуствен интелект, трябва да бъде дефиниран
      начин за проверка и тестване на въпросния резултат.
    </p>

    <p>
      Добре известен е тестът на Тюринг <a href="#reference-turing-test"><sup>[x]</sup></a>. Но още в първата глава
      "The Imitation Game" на основополагащата си статия, Тюринг отказва да се ангажира с дефиниция на интелект и
      вместо това избира да търси алтернативен въпрос. Подобен, но по-малко известен е тестът на работното място
      (The employment test)<a href="#reference-employment-test"><sup>[x]</sup></a>, формулиран от Нилс Нилсън - автор
      на алгоритъма A* и планировчика STRIPS. Целта е да се провери дали агент може да се справи поне толкова
      добре колкото човек в служба съществена за икономиката. Тестът на студента-робот (The Robot College Student
      Test)<a href="#reference-robot-college-test"><sup>[x]</sup></a> на Бен Гьорцел цели да провери способността на агент
      успешно да премине и завърши пълен университетски курс.
    </p>

    <p>
      Множество други автори предлагат подобни тестове. И въпреки че много от тях представят интересни перспективи,
      обединяващо е, че никой не предлага строга формализация или подход към решаването на проблема. Иначе казано -
      изброените тестове дават крайна цел, но не и явна функция, която да оптимизираме, за да стигнем до силен
      изкуствен интелект. За такова начинание, за начало, е нужна общоприета дефиниция на интелект. По темата,
      Джон МакКарти казва: "Проблемът е, че като цяло, все още не можем да характеризираме кои изчислителни процедури да
      наречем интелигентни"<a href="#reference-mc-carthy-on-intelligence"><sup>[x]</sup></a>. Липсата на такава
      общоприета и твърда дефиниция на интелект оставя място за интерпретация, но и създава интересни възможности за
      изследвания.
    </p>

    <h3>Предмет на дипломната работа</h3>

    <p>
      В книгата си Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability, в търсене на
      числен модел на интелекта, Маркъс Хътър предлага формализма на универсалния алгоритмичен агент &ndash; AIXI.
      Целта на AIXI агента е - на базата на предишни свои действия, както и на взаимодействие дадена среда, описана
      чрез машина на Тюринг - да максимизира дадена награда, оценена с число. Хътър доказва, че оптималното поведение
      на такъв агент е да предполага, че средата, с която взаимодейства, е описана от най-кратката възможна
      програма<a href="#reference-hutter-ai-theory"><sup>[x]</sup></a>.
    </p>

    <p>
      Агентът не разполага с функцията, описваща на средата в явен вид. Така неговата цел става построяването на най-малък
      възможен модел на въпросната среда, базирайки се на взаимодействието си с нея. Тъй като такъв модел би бил описан
      с програма, целта на агента, на практика, е да компресира наблюденията върху средата си до най-кратката такава
      програма<a href="#reference-mahoney-on-compression"><sup>[x]</sup></a>.
    </p>

    <div style="font-style: italic; display: inline-block; margin-bottom: 12px;">
      <div style="float: left; width: 38%; margin: 0 3%;">
        [...] being able to compress well is closely related to acting intelligently, thus reducing the slippery concept
        of intelligence to hard file size numbers. In order to compress data, one has to find regularities in them, which
        is intrinsically difficult.
      </div>

      <div style="float: right; width: 50%; margin: 0 3%;">
        [...] способността да се компресира добре е тясно свързана с възможността да действаш интелигентното. Така
        неясната идея за интелигентност може да се сведе размери на файлове и числа. А за да се компресират данни, е
        нужно да се намерят закономерности в тях, което е присъщо трудно.
      </div>
    </div>
    <div style="text-align: right; font-style: italic; margin-bottom: 12px;">
      [TODO -- citation; Marcus Hutter; ]
    </div>

    <p>
      Добрите модели се описват от по-кратки програми. За по-кратка програма, може да се каже че е компресирана.
      В такъв случай може да се твърди, че демонстрирането на възможност за добро компресиране е тясно свързано с
      построяването на добър предвиждащ модел.
      От друга страна, разбирането на входните данни, изискващо интелект, позволява да се построи добър предвиждащ модел за данните.
      Отворен е въпросът, дали построяването на добър модел предполага интелект. Хътър хипотетизира, че двете са еквивалентни [todo: citation needed].
      Базирайки се на това, може да се твърди, че описването на модел в по-кратка (компресирана) програма е еквивалентно на интелект.
    </p>

    <p>
      Разглеждането на компресия като еквивалентна на интелект предоставя мощен инструмент. Разполагайки с набор от
      данни, способността да се смали размера на данните с даден коефициент, дава числова мярка на интелекта.
    </p>

    <p>
      Интересен е въпросът - ако сравняваме алгоритми за компресия, какъв набор от данни бихме избрали. Очевиден отговор
      е "цялото човешко знание". Но такъв отговор няма добра практическа стойност. Нужна е някаква текстова
      репрезентация на възможно най-голяма част от човешкото знание. Wikipedia, например, е един добър кандидат в този смисъл. От една страна заради
      съществения обем, който предлага. От друга - заради отвореността и лесният достъп до данните ѝ.
    </p>

    <p>
      Това води към същински практически проблем - може ли да се постигне добра компресия на данните в Wikipedia?
      В частност - набори от данни наречени <i>enwikX</i>, където X ∈ {5, 6, 7, 8, 9} и съдържа първите 10<sup>X</sup> байта от
      XML репрезентация на данните в Wikipedia.
      Естествен въпрос е какво означава "добра" компресия. Обективна мярка е "по-добър от съществуващия машинен подход".
      Но по-често използваната субективна мярка е "по-добра от човек".
      <br>
      [todo: да спомена Шанън - Prediction and Entropy of Printed English - 15 септември 1951 -- коефициентът, под който представянето е по-добро от човек]
      <br>
    </p>

    <h3>Цел на дипломната работа</h3>

    <p>
      Целта на тази дипломна работа е, използвайки актуални изследвания и технологии, да се построи модел за предвиждане
      на данните в enwikX. Моделът се базира на рекурентни невронни мрежи - изследвани са LSTM и GRU. Изборът на
      рекурентни невронни мрежи е продиктуван от ефективността им в предвиждането на последователности от данни
      <a href="#reference-unreasonable-rnn-effectiveness"><sup>[x]</sup></a>. Необичайно в случая е стремежът към
      overfitting [todo: превод]. Друго нехарактерно е feature engineering-а [todo: превод],
      който се прилага върху данните. Този подход, необходим за много от алгоритмите за машинно самообучение, обикновено
      се заобиколя при невронните мрежи. Но в случая, feature engieneering-а се налага, и има паралели с някои
      подходи при класически алгоритми за компресия.
    </p>

    <p>
      Построените в рамките на тази работа модели дават оценка на вероятностите за възможни следващи символи.
      Това се комбинира с подходи за ентропийно кодиране. Разглеждат се кодове на Хъфман и аритметично кодиране, както и отражението им върху feature
      engineering-а [todo: превод], който прилагаме.
    </p>
  </div>

  <div class="page">
    <h2>2. Съдържание -- TODO</h2>
    <ol>
      <li>
        Въведение
        <ol>
          <li>Предмет на дипломната работа</li>
          <li>Цел на дипломната работа</li>
        </ol>
      </li>
      <li>Съдържание</li>
      <li>
        Компресията като мярка на интелект
        <ol>
          <li>Ентропия на Шанън и оптималната компресия</li>
          <li>Сложност по Колмогоров и универсална индукция на Соломонов</li>
          <li>AIXI и AIXItl</li>
          <li>Теорема за най-бърз и най-кратък алгоритъм</li>
        </ol>
      </li>
      <li>
        Наборите от данни <i>enwikX</i>
        <ol>
          <li>MediaWiki page export format</li>
          <li>Wikitext</li>
          <li>Изследване на данните</li>
        </ol>
      </li>
      <li>
        Предложен подход за компресия на наборите от данни <i>enwikX</i>
        <ol>
          <li>Текущи най-добри резултати</li>
          <li>Kодове на Хъфман и аритметично кодиране</li>
          <li>
            Рекурентни невронни мрежи
            <ol>
              <li>Класически подход и проблеми</li>
              <li>Overfitting</li>
              <li>Feature Engineering</li>
            </ol>
          </li>
        </ol>
      </li>
      <li>
        Изследване на хиперпараметри
        <ol>
          <li>LSTM срещу GRU</li>
          <li>Размер на азбуката</li>
          <li>Размер на речник от под-думи</li>
          <li>Брой и размер на рекурентните слоеве</li>
        </ol>
      </li>
      <li>
        Методи и материали -- todo: дали не трябва да е по-напред?
        <ol>
          <li>Използвани инструменти</li>
          <li>Приложен метод</li>
        </ol>
      </li>
      <li>Резултати и дискусии</li>
      <li>Заключение</li>
    </ol>
  </div>

  <div class="page">

    <h2>Компресията като мярка на интелект</h2>

    <h3>Ентропия на Шанън и оптималната компресия</h3>

    <h3>Сложност по Колмогоров и универсална индукция на Соломонов</h3>

    [todo: да спомена, че не съществува оптимална не-ентропийна компресия]

    <h3>AIXI и AIXItl</h3>

    <!--p>
      Формализмът е
      обвързан с идеите за универсална индукция на Рей Соломонов и за сложност по Колмогоров. Но Хътър отбелязва, че
      въпреки възможността да се счете AIXI за формална дефиниция на математическо решение на ИИ, такова решение не е
      практично заради неизчислимостта му<a href="#reference-hutter-incomputablility"><sup>[x]</sup></a>.
    </p>

    <p>
      Като стъпка в посоката на практическа теория на ИИ Хътър предлага AIXItl &mdash; изчислим модел на AIXI, разглеждащ
      програми ограничени по размер и време за изчисление. Това се допълва от представената от него теорема за най-бърз
      и най-кратък алгоритъм<a href="#"><sup>[todo: цитат - страница 219]</sup></a>, която грубо казано твърди, че
      съществува единствена програма, която е най-бързата и най-кратката. Иначе казано - има единствена програма, която
      води до най-добро приближение на AIXItl до AIXI.
      [todo: малко повече пояснения, може би? Или мястото им не е в увода...]
    </p-->

    <h3>Теорема за най-бърз и най-кратък алгоритъм</h3>

    <h2>Наборите от данни <i>enwikX</i></h2>

    <p>
      Съществуват няколко набора от данни, носещи подобните имена: <i>enwik6</i>, <i>enwik7</i>, <i>enwik8</i>,
      <i>enwik9</i>, и <i>enwik10</i>. Общият вид на имената е enwikX, където X ∈ {5, 6, 7, 8, 9}. Всеки от тях
      представлява първите 10<sup>X</sup> байта от една определена текстова репрезентация на свободно достъпната
      енциклопедия Wikipedia. Данните са взети единствено от англоезичната версия.
    </p>

    <p>
      Въпросните набори от данни са широко използвани в
      изследвания в областта на ИИ, и по-конкретно - в областите на моделиране и синтез на естествени езици. Честият
      избор, на набори от данни базирани на Wikipedia, при тези изследвания, е продиктуван от няколко фактора:
      <ul>
        <li>
          Данните са достъпни под два лиценза - "GNU Free Documentation License" и "Creative Commons Attribution-ShareAlike".
          И двата позволяващи употребата и разпространението на данните с много малко ограничения.
        </li>
        <li>
          Данните имат относително високото качество, често с множество автори и редактори за всяка от статиите.
        </li>
        <li>
          Данните са взети от файл, чиито оригинален размер е 4.8 GiB - над 2 500 000 страници текст, при 2000
          символа на страница - безспорно значително количество текст.
        </li>
      </ul>
    </p>

    <p>
      Трябва да се отбележи, че данните са във вида, в който са съществували в Wikipedia на 3 март 2006. Към момента
      обемът на Wikipedia е над 4 пъти по-голям. Разумно е и да спекулираме, че качеството на статиите се е повишило.
      Но въпреки това, enwikX не се обновяват &ndash; това гарантира консистентност на изследванията във времето.
    </p>

    <h3>MediaWiki page export format</h3>

    <p>
      Преди започване на изследване на данните, е добре да се разгледа репрезентацията им. Вече беше споменато, че
      наборът от данни е текстов, но не беше уточнен конкретен формат и съдържание. Те ще бъдат предмет на интерес
      занапред, и заслужава да бъдат разгледани по-подробно. Първите 15 реда могат да бъдат получени с Unix
      командата "<i>head -n 15 enwik9</i>":

      <pre>
        <code>
&lt;mediawiki xmlns=&quot;http://www.mediawiki.org/xml/export-0.3/&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd&quot; version=&quot;0.3&quot; xml:lang=&quot;en&quot;&gt;
&lt;siteinfo&gt;
  &lt;sitename&gt;Wikipedia&lt;/sitename&gt;
  &lt;base&gt;http://en.wikipedia.org/wiki/Main_Page&lt;/base&gt;
  &lt;generator&gt;MediaWiki 1.6alpha&lt;/generator&gt;
  &lt;case&gt;first-letter&lt;/case&gt;
    &lt;namespaces&gt;
    &lt;namespace key=&quot;-2&quot;&gt;Media&lt;/namespace&gt;
    &lt;namespace key=&quot;-1&quot;&gt;Special&lt;/namespace&gt;
    &lt;namespace key=&quot;0&quot; /&gt;
    &lt;namespace key=&quot;1&quot;&gt;Talk&lt;/namespace&gt;
    &lt;namespace key=&quot;2&quot;&gt;User&lt;/namespace&gt;
    &lt;namespace key=&quot;3&quot;&gt;User talk&lt;/namespace&gt;
    &lt;namespace key=&quot;4&quot;&gt;Wikipedia&lt;/namespace&gt;
    &lt;namespace key=&quot;5&quot;&gt;Wikipedia talk&lt;/namespace&gt;
        </code>
      </pre>

      Моментално е видно, че става въпрос за XML-базиран формат. Форматът се нарича <i>MediaWiki page export format</i>
      (todo: превод в допълнение на официалното английско име на формата?), описан е в XML
      схема<a href="#reference-media-wiki-xml-xsd"><sup>[x]</sup></a>, и съдържа статии, метаданни за статиите,
      както и общи метаданни. Откъсът по-горе описва общи метаданни, и по-специално в случая &ndash; името на сайта,
      адреса му, версията на използвания формат, както и част от основните секции в Wikipedia.
    </p>

    <p>
      По-напред в съдържанието се намират данни и за самите статии. Например - редове от 120 до 140 могат да бъдат
      разгледани чрез "<i>head -n 120 enwik9 | tail -n 20</i>"

      <pre>
        <code>
&lt;/revision&gt;
&lt;/page&gt;
&lt;page&gt;
  &lt;title&gt;AdA&lt;/title&gt;
  &lt;id&gt;11&lt;/id&gt;
  &lt;revision&gt;
    &lt;id&gt;15898946&lt;/id&gt;
    &lt;timestamp&gt;2002-09-22T16:02:58Z&lt;/timestamp&gt;
    &lt;contributor&gt;
      &lt;username&gt;Andre Engels&lt;/username&gt;
      &lt;id&gt;300&lt;/id&gt;
    &lt;/contributor&gt;
    &lt;minor /&gt;
    &lt;text xml:space=&quot;preserve&quot;&gt;#REDIRECT [[Ada programming language]]&lt;/text&gt;
  &lt;/revision&gt;
&lt;/page&gt;
&lt;page&gt;
  &lt;title&gt;Anarchism&lt;/title&gt;
  &lt;id&gt;12&lt;/id&gt;
  &lt;revision&gt;
        </code>
      </pre>

      Това е статия за програмния език Ада. Като метаданни могат да бъдат открити името на статията и уникалния ѝ числов
      идентификатор. Прави впечатление XML етикета '&lt;revision&gt;'. Причината за съществуването му е, че Wikipedia
      поддържа история от редакциите на всяка статия, наричани ревизии. В <i>enwikX</i> наборите от данни,
      винаги присъства единствено последната ревизия. Това е и мястото, на което могат да бъдат открити някои от
      най-важните данни - уникален числов идентификатор на ревизията, време на създаване, автор, индикация дали
      редакцията е малка, коментар (липсващ тук), и текст на статията.
    </p>

    <h3>Wikitext</h3>

    <p>
      В примера по-горе, текстът на примерната статия е "#REDIRECT [[Ada programming language]]". Очевидно пунктуацията
      и специалните символи са повече, отколкото бихме очаквали, от текст на естествен език. Причината е, че текстът е
      във формат Wikitext &ndash; markup (todo: превод) език специално създаден за целите на Wikipedia. (todo: да разкажа за markup езици и за подобни формати, може би?)
    </p>

    <p>
      Форматът позволява декларирането на секции, подсекции, връзки към други статии и техни секции, автоматично
      пренасочване към други статии и техни секции, влагане на изображения, списъци с подточки, таблици и други.
      Между редове 135 и 140, например, може да бъде открито:

      <pre>
        <code>
          The word '''anarchism''' is [[etymology|derived from]] the [[Greek language|Greek]] ''[[Wiktionary:&amp;amp;#945;&amp;amp;#957;&amp;amp;#945;&amp;amp;#961;&amp;amp;#967;&amp;amp;#943;&amp;amp;#945;|&amp;amp;#945;&amp;amp;#957;&amp;amp;#945;&amp;amp;#961;&amp;amp;#967;&amp;amp;#943;&amp;amp;#945;]]'' (&amp;quot;without [[archon]]s (ruler, chief, king)&amp;quot;). Anarchism as a [[political philosophy]], is the belief that ''rulers'' are unnecessary and should be abolished, although there are differing interpretations of what this means. Anarchism also refers to related [[social movement]]s) that advocate the elimination of authoritarian institutions, particularly the [[state]].&amp;lt;ref&amp;gt;[http://en.wikiquote.org/wiki/Definitions_of_anarchism Definitions of anarchism] on Wikiquote, accessed 2006&amp;lt;/ref&amp;gt; The word &amp;quot;[[anarchy]],&amp;quot; as most anarchists use it, does not imply [[chaos]], [[nihilism]], or [[anomie]], but rather a harmonious [[anti-authoritarian]] society. In place of what are regarded as authoritarian political structures and coercive economic institutions, anarchists advocate social relations based upon [[voluntary association]] of autonomous individuals, [[mutual aid]], and [[self-governance]].

          While anarchism is most easily defined by what it is against, anarchists also offer positive visions of what they believe to be a truly free society. However, ideas about how an anarchist society might work vary considerably, especially with respect to economics; there is also disagreement about how a free society might be brought about.
        </code>
      </pre>

      Откъсът съдържа примери за връзки към други статии и техни секции (оградени с двойни квадратни скоби), удебелен
      текст (тройки кавички) и други. Присъствието на толкова много специални символи, поставя под въпрос
      съпоставимостта с базовата (todo: превод на baseline) оценка от между 0.6 и 1.3 бита за символ, която Шанън дава.
      Оценката, както вече беше споменато, е за азбука от 27 символа, която игнорира числа, пунктуация, големи и малки букви
      и други възможно специални или служебни символи. Това поставя въпроса - какво може да бъде видяно в данните?
    </p>

    <h3>Изследване на данните</h3>

    <h2>Предложен подход за компресия на наборите от данни <i>enwikX</i></h2>

    <h3>Текущи най-добри резултати</h3>

    <h3>Kодове на Хъфман и аритметично кодиране</h3>

    <h3>Рекурентни невронни мрежи</h3>

    <h4>Класически подход и проблеми</h4>

    <h4>Overfitting</h4>

    <h4>Feature Engineering</h4>

    <h2>Изследване на хиперпараметри</h2>

    <h3>LSTM срещу GRU</h3>

    <h3>Размер на азбуката</h3>

    <h3>Размер на речник от под-думи</h3>

    <h3>Брой и размер на рекурентните слоеве</h3>

    <h2>?. Методи и материали -- TODO</h2>

    <h3>Използвани инструменти -- TODO</h3>
    <p>
      Python 3.7
      <br>
      tensorflow 2.1, 2.2
      <br>
      tensorflow-datasets
      <br>
      jupyter
      <br>
      numpy
      <br>
      C++ 17
      <br>
      libstudxml - https://www.codesynthesis.com/projects/libstudxml/
      <br>
      NVIDIA GeForce RTX 2060 SUPER
      <br>
      Java, xjc, jaxb -- legacy
    </p>
  </div>

  <div class="page">
    <h2>Библиография</h2>
    <div style="font-size: x-small;">
      <div id="reference-turing-test">
        [x] Turing, Alan Mathison (July 1950). "COMPUTING MACHINERY AND INTELLIGENCE". Computing Machinery and Intelligence. Mind 49: 433-460.
      </div>
      <div id="reference-employment-test">
        [x] Nilsson, Nils John (WINTER 2005). "Human-Level Artificial Intelligence? Be Serious!". AI MAGAZINE
      </div>
      <div id="reference-robot-college-test">
        [x] Ben Goertzel (5 September 2012). "What counts as a conscious thinking machine?". NewScientist Magazine issue 2881;
        https://www.newscientist.com/article/mg21528813-600-what-counts-as-a-conscious-thinking-machine/
      </div>
      <div id="reference-mc-carthy-on-intelligence">
        [x] McCarthy, John (12 November 12 2007). "WHAT IS ARTIFICIAL INTELLIGENCE?".
        http://www-formal.stanford.edu/jmc/whatisai/node1.html
      </div>
      <div id="reference-hutter-ai-theory">
        Hutter, Marcus (September 2001). "Towards a Universal Theory of Artificial Intelligence based on Algorithmic Probability and Sequential Decisions",
        Lecture Notes in Artificial Intelligence (LNAI 2167), Proc. 12th Eurpean Conf. on Machine Learning, ECML (2001) 226--238
      </div>
      <div id="reference-mahoney-on-compression">
        Mahoney, Matt (20 August 2006). "Rationale for a Large Text Compression Benchmark",
        https://cs.fit.edu/~mmahoney/compression/rationale.html
      </div>
      <div id="reference-hutter-incomputablility">
        [x] Jan Leike and Marcus Hutter (20 October 2015). "On the Computability of AIXI". UAI 2015. arXiv:1510.05572
      </div>
      <div id="reference-unreasonable-rnn-effectiveness">
        [x] Karpathy, Andrej (21 May 2015). "The Unreasonable Effectiveness of Recurrent Neural Networks"
        http://karpathy.github.io/2015/05/21/rnn-effectiveness/
      </div>
      <div id="reference-media-wiki-xml-xsd">
        [x] Wikimedia Foundation. MediaWiki's page export format XML schema
        https://www.mediawiki.org/xml/export-0.10.xsd
      </div>
      <div id="reference-1">
        [1] Shannon, Claude Elwood (July 1948). "A Mathematical Theory of Communication". Bell System Technical Journal. 27 (3): 379–423
      </div>
      <div id="reference-2">
        [2] Shannon, Claude Elwood (October 1948). "A Mathematical Theory of Communication". Bell System Technical Journal. 27 (4): 623–666
      </div>
    </div>
  </div>
</body>
