{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import re\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "class SubalphabetSubwordTextEncoder:\n",
    "    HTML_ESCAPE_PATTERN = re.compile('&(((#\\d+)|([A-Za-z]+));&?)+')\n",
    "    SPECIAL_CHAR = 'X'\n",
    "\n",
    "    def __init__(self, subalphabet, vocab_list=None):\n",
    "        self._subword_text_encoder = tfds.features.text.SubwordTextEncoder(vocab_list)\n",
    "        self._subalphabet = subalphabet\n",
    "\n",
    "    def encode(self, s):\n",
    "        subword_max_code = len(self.subwords)\n",
    "        codes = self._build_encoding_dict(self._subalphabet)\n",
    "        preprocessed_string = self.preprocess_string(s)\n",
    "\n",
    "        result = []\n",
    "        unicode_buffer = bytearray()\n",
    "        for id in self._subword_text_encoder.encode(preprocessed_string):\n",
    "            if id <= subword_max_code:\n",
    "                result.append(id)\n",
    "            else:\n",
    "                unicode_buffer.append(id - subword_max_code - 1)\n",
    "                try:\n",
    "                    id = ord(unicode_buffer.decode())\n",
    "                    result.append(subword_max_code + codes[id] + 1)\n",
    "                    unicode_buffer = bytearray()\n",
    "                except UnicodeDecodeError:\n",
    "                    pass\n",
    "        return result\n",
    "\n",
    "    def decode(self, ids):\n",
    "        subword_max_code = len(self.subwords)\n",
    "\n",
    "        processed_ids = []\n",
    "        for id in ids:\n",
    "            if id <= subword_max_code:\n",
    "                processed_ids.append(id)\n",
    "            else:\n",
    "                char = self._subalphabet[id - subword_max_code - 1].encode()\n",
    "                processed_ids += [codepoint + subword_max_code + 1 for codepoint in char]\n",
    "\n",
    "        return self._subword_text_encoder.decode(processed_ids)\n",
    "    \n",
    "    @classmethod\n",
    "    def build_from_corpus(cls, corpus_generator, subalphabet_size, target_vocab_size, max_subword_length=20, max_corpus_chars=None, reserved_tokens=None):\n",
    "        simplified_strings = []\n",
    "        char_counts = defaultdict(lambda: 0)\n",
    "\n",
    "        for string in corpus_generator:\n",
    "            simplified_string = cls._unescape_string(string.lower())\n",
    "            simplified_strings.append(simplified_string)\n",
    "            for char in simplified_string:\n",
    "                char_counts[char] += 1\n",
    "\n",
    "        sorted_char_counts = sorted(char_counts.items(), key = lambda item: item[1], reverse = True)\n",
    "        chars = [char for char, char_count in sorted_char_counts[:subalphabet_size - 1]]\n",
    "        chars = list(reversed(chars)) # We reverse the chars so that the most common ones are last\n",
    "        chars = cls._ensure_special_character_presence(chars)\n",
    "\n",
    "        def simplified_corpus():\n",
    "            for string in simplified_strings:\n",
    "                yield cls._compact_string(chars, string)\n",
    "\n",
    "        subword_text_encoder = tfds.features.text.SubwordTextEncoder.build_from_corpus(simplified_corpus(), target_vocab_size + 256 - subalphabet_size)\n",
    "        \n",
    "        return cls(chars, subword_text_encoder.subwords)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_from_file(cls, filename_prefix):\n",
    "        subword_text_encoder = tfds.features.text.SubwordTextEncoder.load_from_file(filename_prefix)\n",
    "        with open(filename_prefix + '.subalphabet', 'rb') as text_file:\n",
    "            subalphabet = ['\\0', cls.SPECIAL_CHAR] + text_file.read().decode().split('\\0')\n",
    "\n",
    "        return cls(subalphabet, subword_text_encoder.subwords)\n",
    "    \n",
    "    def save_to_file(self, filename_prefix):\n",
    "        self._subword_text_encoder.save_to_file(filename_prefix)\n",
    "        with open(filename_prefix + '.subalphabet', 'wb') as text_file:\n",
    "            text_file.write('\\0'.join(self._subalphabet[2:]).encode())\n",
    "\n",
    "    def preprocess_string(self, string):\n",
    "        return self._compact_string(self._subalphabet, self._unescape_string(string.lower()))\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self._subword_text_encoder.vocab_size - 256 + len(self._subalphabet)\n",
    "    \n",
    "    @property\n",
    "    def subwords(self):\n",
    "        return self._subword_text_encoder.subwords\n",
    "\n",
    "    @classmethod\n",
    "    def _ensure_special_character_presence(cls, chars):\n",
    "        if '\\0' in chars:\n",
    "            chars = [char for char in chars if char != '\\0']\n",
    "        else:\n",
    "            chars = chars[1:]\n",
    "\n",
    "        return ['\\0', cls.SPECIAL_CHAR] + chars\n",
    "    \n",
    "    @classmethod\n",
    "    def _unescape_html_symbol(cls, escaped):\n",
    "        escaped_symbols = escaped.group(0).split(';')\n",
    "        escaped_symbols = escaped_symbols[:-1] # strip the last string empty string\n",
    "        escaped_symbols = (escaped_symbol + ';' for escaped_symbol in escaped_symbols)\n",
    "        result = ''\n",
    "\n",
    "        for escaped_symbol in escaped_symbols:\n",
    "            code = escaped_symbol[1:] if escaped_symbol[0] == '&' else escaped_symbol\n",
    "\n",
    "            if code[0] == '#':\n",
    "                result += chr(int(code[1:-1]))\n",
    "            elif code in html.entities.html5:\n",
    "                result += html.entities.html5[code]\n",
    "            else:\n",
    "                result += escaped_symbol\n",
    "\n",
    "        return result\n",
    "    \n",
    "    @classmethod\n",
    "    def _unescape_string(cls, string):\n",
    "        return cls.HTML_ESCAPE_PATTERN.sub(cls._unescape_html_symbol, string)\n",
    "    \n",
    "    @classmethod\n",
    "    def _compact_string(cls, chars, string):\n",
    "        if len(string) == 0:\n",
    "            return string\n",
    "        else:\n",
    "            compacted_string = string[0]\n",
    "\n",
    "            for char in string[1:]:\n",
    "                if char not in chars or char == cls.SPECIAL_CHAR:\n",
    "                    if compacted_string[-1] != cls.SPECIAL_CHAR:\n",
    "                        compacted_string += cls.SPECIAL_CHAR\n",
    "                else:\n",
    "                    compacted_string += char\n",
    "\n",
    "            return compacted_string\n",
    "    \n",
    "    @classmethod\n",
    "    def _build_encoding_dict(cls, chars):\n",
    "        return {ord(char): index for index, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SubalphabetSubwordTextEncoder.load_from_file('72_256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from checkpointing import checkpointable\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for physical_device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_device, enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE=np.int16\n",
    "\n",
    "class Articles:\n",
    "    EMPTY_ARTICLE = np.array([], dtype=TYPE) # used for padding\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        with open(path, 'rb') as text_file:\n",
    "            data = text_file.read()[:100000000].decode()\n",
    "\n",
    "        self.articles = sorted(set(data.split('\\0')), key=len)\n",
    "        self._encoded_articles = None\n",
    "\n",
    "    @property\n",
    "    def encoded_articles(self):\n",
    "        if self._encoded_articles == None:\n",
    "            self._encoded_articles = [np.array(encoder.encode(article), dtype=TYPE) for article in self.articles]\n",
    "        \n",
    "        return self._encoded_articles\n",
    "\n",
    "    def articles_generator(self, batch_size = 1, start = 0, end = None):\n",
    "        end = end or len(self.articles)\n",
    "\n",
    "        for _ in range(batch_size - ((end - start - 1) % batch_size + 1)):\n",
    "            yield self.EMPTY_ARTICLE\n",
    "\n",
    "        for article in itertools.islice(self.encoded_articles, start, end):\n",
    "            yield article\n",
    "\n",
    "    def subbatch_generator(self, batch_size, batch_length, start = 0, end = None):\n",
    "        end = end or len(self.articles)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_generator(self.articles_generator, args=(batch_size, start, end), output_types=TYPE)\n",
    "        dataset = dataset.padded_batch(batch_size, padded_shapes=([None]), drop_remainder=True)\n",
    "        dataset = dataset.shuffle(100)\n",
    "\n",
    "        for batch in dataset.as_numpy_iterator():\n",
    "            remaining = batch\n",
    "            while remaining.shape[1] > batch_length + 1:\n",
    "                yield remaining[:, :batch_length + 1]\n",
    "                remaining = remaining[:, batch_length:]\n",
    "\n",
    "            if remaining.shape[1] == batch_length + 1:\n",
    "                yield remaining\n",
    "                yield np.zeros((batch_size, batch_length + 1), dtype=TYPE)\n",
    "            else:\n",
    "                yield np.hstack([remaining, np.zeros([batch_size, batch_length - remaining.shape[1] + 1])])\n",
    "\n",
    "    def steps(self, batch_size, batch_length):\n",
    "        articles = self.articles_generator(batch_size, batch_length)\n",
    "        return sum(math.ceil(len(article) / batch_length + 1) for i, article in enumerate(articles) if (i + 1) % batch_size == 0)\n",
    "\n",
    "    def dataset(self, batch_size, batch_length, start = 0, end = None):\n",
    "        end = end or len(self.articles)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_generator(self.subbatch_generator, args=(batch_size, batch_length, start, end), output_types=TYPE, output_shapes=(batch_size, batch_length + 1))\n",
    "        return dataset.map(lambda batch: (batch[:, :-1], batch[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "def average_final_batch_ratio(true_labels, predictions):\n",
    "    return 0 ** tf.math.abs(true_labels[-1, -1])\n",
    "\n",
    "class ModelStateResetter(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.last_final_batch_count = 0\n",
    "        \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        average_final_batch_ratio = logs.get('average_final_batch_ratio', 0)\n",
    "        final_batch_count = int(round(average_final_batch_ratio * (batch + 1)))\n",
    "        is_final = final_batch_count - self.last_final_batch_count\n",
    "        self.last_final_batch_count = final_batch_count\n",
    "        \n",
    "        if is_final:\n",
    "            self.model.reset_states()\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, articles, checkpoint_dir, vocab_size, embedding_dim, rnn_units):\n",
    "        self._articles = articles\n",
    "        self._batch_size = None\n",
    "        self._batched_item_length = None\n",
    "        self._training_model = None\n",
    "        self._predicting_model = None\n",
    "        self._vocab_size = vocab_size\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._rnn_units = rnn_units\n",
    "\n",
    "        self._checkpoint_dir = checkpoint_dir\n",
    "        self._checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\") # Name of the checkpoint files\n",
    "\n",
    "    def training_model(self, batch_size, batched_item_length):\n",
    "        if self._training_model == None or batch_size != self._batch_size or batched_item_length != self._batched_item_length:\n",
    "            self._batch_size = batch_size\n",
    "            self._batched_item_length = batched_item_length\n",
    "            self._training_model = tf.keras.Sequential([\n",
    "                checkpointable(tf.keras.layers.Masking)(mask_value=0, batch_input_shape=[batch_size, batched_item_length]),\n",
    "                checkpointable(tf.keras.layers.Embedding)(self._vocab_size, self._embedding_dim),\n",
    "                checkpointable(tf.keras.layers.GRU)(self._rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                checkpointable(tf.keras.layers.GRU)(self._rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "                checkpointable(tf.keras.layers.Dense)(self._vocab_size),\n",
    "            ])\n",
    "\n",
    "            if os.path.isdir(self._checkpoint_dir):\n",
    "                self._training_model.load_weights(tf.train.latest_checkpoint(self._checkpoint_dir))\n",
    "\n",
    "            self._training_model.compile(optimizer='adam', loss=loss, metrics=[average_final_batch_ratio])\n",
    "            self._predicting_model = None\n",
    "        \n",
    "        return self._training_model\n",
    "\n",
    "    @property\n",
    "    def callbacks(self):\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=self._checkpoint_prefix, save_weights_only=True)\n",
    "        model_state_resetter_callback = ModelStateResetter()\n",
    "        \n",
    "        return [checkpoint_callback, model_state_resetter_callback]\n",
    "    \n",
    "    def train(self, batch_size, batched_item_length, epochs=1):\n",
    "        dataset = self._articles.dataset(batch_size, batched_item_length)\n",
    "\n",
    "        model = self.training_model(batch_size, batched_item_length)\n",
    "\n",
    "        model.fit(dataset, epochs=epochs, callbacks=self.callbacks)\n",
    "    \n",
    "    @property\n",
    "    def predicting_model(self):\n",
    "        if self._predicting_model == None:\n",
    "            self._predicting_model = tf.keras.Sequential([\n",
    "                checkpointable(tf.keras.layers.Masking)(mask_value=0, batch_input_shape=[1, 1]),\n",
    "                checkpointable(tf.keras.layers.Embedding)(self._vocab_size, self._embedding_dim),\n",
    "                checkpointable(tf.keras.layers.GRU)(self._rnn_units, stateful=True, return_sequences=True),\n",
    "                checkpointable(tf.keras.layers.GRU)(self._rnn_units, stateful=True, return_sequences=True),\n",
    "                checkpointable(tf.keras.layers.Dense)(self._vocab_size),\n",
    "            ])\n",
    "            \n",
    "            self._predicting_model.load_weights(tf.train.latest_checkpoint(self._checkpoint_dir))\n",
    "            self._training_model = None\n",
    "        \n",
    "        return self._predicting_model\n",
    "    \n",
    "    def predict(self, input_eval):\n",
    "        return self.predicting_model(input_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = Articles('page_revisions_text_with_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(articles, './training_checkpoints-32', vocab_size = encoder.vocab_size, embedding_dim=128, rnn_units=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (256, 256)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (256, 256, 128)           32640     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (256, 256, 512)           986112    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (256, 256, 512)           1575936   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (256, 256, 255)           130815    \n",
      "=================================================================\n",
      "Total params: 2,725,503\n",
      "Trainable params: 2,725,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.training_model(256, 256).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1672/1672 [==============================] - 466s 279ms/step - loss: 2.2504 - average_final_batch_ratio: 0.1561\n",
      "Epoch 2/10\n",
      "1672/1672 [==============================] - 318s 190ms/step - loss: 1.5829 - average_final_batch_ratio: 0.1561\n",
      "Epoch 3/10\n",
      "1672/1672 [==============================] - 322s 193ms/step - loss: 1.4825 - average_final_batch_ratio: 0.1561\n",
      "Epoch 4/10\n",
      "1672/1672 [==============================] - 325s 194ms/step - loss: 1.4354 - average_final_batch_ratio: 0.1561\n",
      "Epoch 5/10\n",
      "1672/1672 [==============================] - 327s 196ms/step - loss: 1.4017 - average_final_batch_ratio: 0.1561\n",
      "Epoch 6/10\n",
      "1672/1672 [==============================] - 315s 189ms/step - loss: 1.3785 - average_final_batch_ratio: 0.1561\n",
      "Epoch 7/10\n",
      "1672/1672 [==============================] - 315s 188ms/step - loss: 1.3648 - average_final_batch_ratio: 0.1561\n",
      "Epoch 8/10\n",
      "1672/1672 [==============================] - 315s 189ms/step - loss: 1.3510 - average_final_batch_ratio: 0.1561\n",
      "Epoch 9/10\n",
      "1672/1672 [==============================] - 314s 188ms/step - loss: 1.3360 - average_final_batch_ratio: 0.1561\n",
      "Epoch 10/10\n",
      "1672/1672 [==============================] - 320s 191ms/step - loss: 1.3318 - average_final_batch_ratio: 0.1561\n"
     ]
    }
   ],
   "source": [
    "model.train(224, 224, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "\n",
    "class Huffman:\n",
    "    huffman = ctypes.CDLL('x64/Release/huffman')\n",
    "    \n",
    "    huffman.create_tree.restype = ctypes.c_void_p\n",
    "    huffman.destroy_tree.restype = None\n",
    "    huffman.load_weights.restype = None\n",
    "    huffman.create_code_string.restype = ctypes.c_char_p\n",
    "    \n",
    "    def __init__(self, category_count):\n",
    "        self.category_count = category_count\n",
    "        self.tree = ctypes.c_void_p(self.huffman.create_tree(category_count))\n",
    "\n",
    "    def __del__(self):\n",
    "        self.huffman.destroy_tree(self.tree)\n",
    "        \n",
    "    def load_weights(self, weights):\n",
    "        self.huffman.load_weights(self.tree, weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float)))\n",
    "    \n",
    "    def get_code_length(self, category):\n",
    "        return self.huffman.get_code_length(self.tree, category)\n",
    "\n",
    "    def get_code_zero_count(self, category):\n",
    "        return self.huffman.get_code_zero_count(self.tree, category)\n",
    "    \n",
    "    def archive_size(self, model, text):\n",
    "        archived_size = math.ceil(math.log2(self.category_count))\n",
    "        input_eval = np.array([[text[0]]], dtype=TYPE)\n",
    "\n",
    "        model.predicting_model.reset_states()\n",
    "\n",
    "        for byte in text[1:]:\n",
    "            predictions = model.predict(input_eval)\n",
    "            predictions = tf.squeeze(predictions, 0) # remove the batch dimension\n",
    "\n",
    "            weights = tf.nn.softmax(predictions[0]).numpy()\n",
    "            self.load_weights(weights)\n",
    "            archived_size += self.get_code_length(byte.item())\n",
    "\n",
    "            input_eval = tf.expand_dims([byte], 0)\n",
    "\n",
    "        return archived_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0:\tLength: 12\tAvg Compression: 0.239583\n",
      "Article 1000:\tLength: 42\tAvg Compression: 0.342593\n",
      "Article 2000:\tLength: 51\tAvg Compression: 0.272619\n",
      "Article 3000:\tLength: 63\tAvg Compression: 0.241071\n",
      "Article 4000:\tLength: 387\tAvg Compression: 0.224324\n",
      "Article 5000:\tLength: 1254\tAvg Compression: 0.225124\n",
      "Article 6000:\tLength: 2360\tAvg Compression: 0.215309\n",
      "Article 7000:\tLength: 3684\tAvg Compression: 0.230533\n",
      "Article 8000:\tLength: 5357\tAvg Compression: 0.236913\n",
      "Article 9000:\tLength: 7861\tAvg Compression: 0.226200\n",
      "Article 10000:\tLength: 11179\tAvg Compression: 0.228717\n",
      "Article 11000:\tLength: 15815\tAvg Compression: 0.221533\n",
      "Article 12000:\tLength: 25299\tAvg Compression: 0.210647\n",
      "Article 13000:\tLength: 79403\tAvg Compression: 0.219174\n"
     ]
    }
   ],
   "source": [
    "total_raw = 0\n",
    "total_compressed = 0\n",
    "\n",
    "huffman = Huffman(encoder.vocab_size)\n",
    "for index, encoded_article in enumerate(articles.articles_generator(1)):\n",
    "    if index % 1000 == 0 and len(encoded_article) != 0:\n",
    "        article = encoder.decode(encoded_article)\n",
    "        total_raw += len(article) * 8\n",
    "        total_compressed += huffman.archive_size(model, encoded_article)\n",
    "        print('Article %d:\\tLength: %d\\tAvg Compression: %f' % (index, len(article), total_compressed/total_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1339/1339 [==============================] - 450s 336ms/step - loss: 1.2619 - average_final_batch_ratio: 0.1650\n"
     ]
    }
   ],
   "source": [
    "model.train(256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/74\n",
      "1339/1339 [==============================] - 308s 230ms/step - loss: 1.2569 - average_final_batch_ratio: 0.1650\n",
      "Epoch 2/74\n",
      "1339/1339 [==============================] - 313s 233ms/step - loss: 1.2504 - average_final_batch_ratio: 0.1650\n",
      "Epoch 3/74\n",
      "1339/1339 [==============================] - 311s 232ms/step - loss: 1.2468 - average_final_batch_ratio: 0.1650\n",
      "Epoch 4/74\n",
      "1339/1339 [==============================] - 318s 237ms/step - loss: 1.2448 - average_final_batch_ratio: 0.1650\n",
      "Epoch 5/74\n",
      "1339/1339 [==============================] - 314s 235ms/step - loss: 1.2411 - average_final_batch_ratio: 0.1650\n",
      "Epoch 6/74\n",
      "1339/1339 [==============================] - 316s 236ms/step - loss: 1.2354 - average_final_batch_ratio: 0.1650\n",
      "Epoch 7/74\n",
      "1339/1339 [==============================] - 317s 236ms/step - loss: 1.2343 - average_final_batch_ratio: 0.1650\n",
      "Epoch 8/74\n",
      "1339/1339 [==============================] - 311s 233ms/step - loss: 1.2308 - average_final_batch_ratio: 0.1650\n",
      "Epoch 9/74\n",
      "1339/1339 [==============================] - 317s 236ms/step - loss: 1.2275 - average_final_batch_ratio: 0.1650\n",
      "Epoch 10/74\n",
      "1339/1339 [==============================] - 311s 233ms/step - loss: 1.2269 - average_final_batch_ratio: 0.1650\n",
      "Epoch 11/74\n",
      "1339/1339 [==============================] - 316s 236ms/step - loss: 1.2267 - average_final_batch_ratio: 0.1650\n",
      "Epoch 12/74\n",
      "1339/1339 [==============================] - 311s 232ms/step - loss: 1.2234 - average_final_batch_ratio: 0.1650\n",
      "Epoch 13/74\n",
      "1339/1339 [==============================] - 313s 234ms/step - loss: 1.2190 - average_final_batch_ratio: 0.1650\n",
      "Epoch 14/74\n",
      "1339/1339 [==============================] - 314s 234ms/step - loss: 1.2196 - average_final_batch_ratio: 0.1650\n",
      "Epoch 15/74\n",
      "1339/1339 [==============================] - 308s 230ms/step - loss: 1.2188 - average_final_batch_ratio: 0.1650\n",
      "Epoch 16/74\n",
      "1339/1339 [==============================] - 311s 233ms/step - loss: 1.2168 - average_final_batch_ratio: 0.1650\n",
      "Epoch 17/74\n",
      "1339/1339 [==============================] - 306s 229ms/step - loss: 1.2165 - average_final_batch_ratio: 0.1650\n",
      "Epoch 18/74\n",
      "1339/1339 [==============================] - 315s 235ms/step - loss: 1.2105 - average_final_batch_ratio: 0.1650\n",
      "Epoch 19/74\n",
      "1339/1339 [==============================] - 310s 231ms/step - loss: 1.2108 - average_final_batch_ratio: 0.1650\n",
      "Epoch 20/74\n",
      "1339/1339 [==============================] - 312s 233ms/step - loss: 1.2133 - average_final_batch_ratio: 0.1650\n",
      "Epoch 21/74\n",
      "1339/1339 [==============================] - 312s 233ms/step - loss: 1.2110 - average_final_batch_ratio: 0.1650\n",
      "Epoch 22/74\n",
      "1339/1339 [==============================] - 306s 229ms/step - loss: 1.2087 - average_final_batch_ratio: 0.1650\n",
      "Epoch 23/74\n",
      "1339/1339 [==============================] - 310s 232ms/step - loss: 1.2058 - average_final_batch_ratio: 0.1650\n",
      "Epoch 24/74\n",
      "1339/1339 [==============================] - 306s 228ms/step - loss: 1.2066 - average_final_batch_ratio: 0.1650\n",
      "Epoch 25/74\n",
      "1339/1339 [==============================] - 315s 235ms/step - loss: 1.2080 - average_final_batch_ratio: 0.1650\n",
      "Epoch 26/74\n",
      "1339/1339 [==============================] - 315s 235ms/step - loss: 1.2054 - average_final_batch_ratio: 0.1650\n",
      "Epoch 27/74\n",
      "1339/1339 [==============================] - 317s 237ms/step - loss: 1.2021 - average_final_batch_ratio: 0.1650\n",
      "Epoch 28/74\n",
      "1339/1339 [==============================] - 318s 237ms/step - loss: 1.2024 - average_final_batch_ratio: 0.1650\n",
      "Epoch 29/74\n",
      "1339/1339 [==============================] - 312s 233ms/step - loss: 1.2046 - average_final_batch_ratio: 0.1650\n",
      "Epoch 30/74\n",
      "1339/1339 [==============================] - 312s 233ms/step - loss: 1.2035 - average_final_batch_ratio: 0.1650\n",
      "Epoch 31/74\n",
      "1339/1339 [==============================] - 306s 229ms/step - loss: 1.2023 - average_final_batch_ratio: 0.1650\n",
      "Epoch 32/74\n",
      "1339/1339 [==============================] - 315s 235ms/step - loss: 1.1993 - average_final_batch_ratio: 0.1650\n",
      "Epoch 33/74\n",
      "1339/1339 [==============================] - 311s 232ms/step - loss: 1.1996 - average_final_batch_ratio: 0.1650\n",
      "Epoch 34/74\n",
      "1339/1339 [==============================] - 313s 234ms/step - loss: 1.2006 - average_final_batch_ratio: 0.1650\n",
      "Epoch 35/74\n",
      "1339/1339 [==============================] - 314s 234ms/step - loss: 1.1978 - average_final_batch_ratio: 0.1650\n",
      "Epoch 36/74\n",
      "1339/1339 [==============================] - 308s 230ms/step - loss: 1.1994 - average_final_batch_ratio: 0.1650\n",
      "Epoch 37/74\n",
      "1339/1339 [==============================] - 311s 232ms/step - loss: 1.1977 - average_final_batch_ratio: 0.1650\n",
      "Epoch 38/74\n",
      "1339/1339 [==============================] - 306s 228ms/step - loss: 1.1956 - average_final_batch_ratio: 0.1650\n",
      "Epoch 39/74\n",
      "1339/1339 [==============================] - 314s 235ms/step - loss: 1.1970 - average_final_batch_ratio: 0.1650\n",
      "Epoch 40/74\n",
      "1339/1339 [==============================] - 310s 232ms/step - loss: 1.1965 - average_final_batch_ratio: 0.1650\n",
      "Epoch 41/74\n",
      "1339/1339 [==============================] - 312s 233ms/step - loss: 1.1967 - average_final_batch_ratio: 0.1650\n",
      "Epoch 42/74\n",
      "1339/1339 [==============================] - 312s 233ms/step - loss: 1.1963 - average_final_batch_ratio: 0.1650\n",
      "Epoch 43/74\n",
      "1339/1339 [==============================] - 306s 229ms/step - loss: 1.1924 - average_final_batch_ratio: 0.1650\n",
      "Epoch 44/74\n",
      "1339/1339 [==============================] - 310s 231ms/step - loss: 1.1953 - average_final_batch_ratio: 0.1650\n",
      "Epoch 45/74\n",
      "1339/1339 [==============================] - 306s 228ms/step - loss: 1.1917 - average_final_batch_ratio: 0.1650\n",
      "Epoch 46/74\n",
      "1339/1339 [==============================] - 314s 235ms/step - loss: 1.1943 - average_final_batch_ratio: 0.1650\n",
      "Epoch 47/74\n",
      "1339/1339 [==============================] - 310s 231ms/step - loss: 1.1935 - average_final_batch_ratio: 0.1650\n",
      "Epoch 48/74\n",
      "1339/1339 [==============================] - 312s 233ms/step - loss: 1.1931 - average_final_batch_ratio: 0.1650\n",
      "Epoch 49/74\n",
      "1339/1339 [==============================] - 317s 237ms/step - loss: 1.1927 - average_final_batch_ratio: 0.1650\n",
      "Epoch 50/74\n",
      "1339/1339 [==============================] - 313s 234ms/step - loss: 1.1915 - average_final_batch_ratio: 0.1650\n",
      "Epoch 51/74\n",
      "1339/1339 [==============================] - 317s 237ms/step - loss: 1.1900 - average_final_batch_ratio: 0.1650\n",
      "Epoch 52/74\n",
      "1339/1339 [==============================] - 311s 233ms/step - loss: 1.1916 - average_final_batch_ratio: 0.1650\n",
      "Epoch 53/74\n",
      "1339/1339 [==============================] - 316s 236ms/step - loss: 1.1888 - average_final_batch_ratio: 0.1650\n",
      "Epoch 54/74\n",
      "1339/1339 [==============================] - 311s 232ms/step - loss: 1.1913 - average_final_batch_ratio: 0.1650\n",
      "Epoch 55/74\n",
      "1339/1339 [==============================] - 313s 234ms/step - loss: 1.1887 - average_final_batch_ratio: 0.1650\n",
      "Epoch 56/74\n",
      "1339/1339 [==============================] - 314s 234ms/step - loss: 1.1897 - average_final_batch_ratio: 0.1650\n",
      "Epoch 57/74\n",
      "1339/1339 [==============================] - 308s 230ms/step - loss: 1.1887 - average_final_batch_ratio: 0.1650\n",
      "Epoch 58/74\n",
      "1339/1339 [==============================] - 312s 233ms/step - loss: 1.1893 - average_final_batch_ratio: 0.1650\n",
      "Epoch 59/74\n",
      "1339/1339 [==============================] - 306s 229ms/step - loss: 1.1881 - average_final_batch_ratio: 0.1650\n",
      "Epoch 60/74\n",
      "1339/1339 [==============================] - 315s 235ms/step - loss: 1.1878 - average_final_batch_ratio: 0.1650\n",
      "Epoch 61/74\n",
      "1339/1339 [==============================] - 310s 232ms/step - loss: 1.1886 - average_final_batch_ratio: 0.1650\n",
      "Epoch 62/74\n",
      "1339/1339 [==============================] - 312s 233ms/step - loss: 1.1891 - average_final_batch_ratio: 0.1650\n",
      "Epoch 63/74\n",
      "1339/1339 [==============================] - 313s 234ms/step - loss: 1.1865 - average_final_batch_ratio: 0.1650\n",
      "Epoch 64/74\n",
      "1339/1339 [==============================] - 307s 229ms/step - loss: 1.1872 - average_final_batch_ratio: 0.1650\n",
      "Epoch 65/74\n",
      "1339/1339 [==============================] - 311s 232ms/step - loss: 1.1873 - average_final_batch_ratio: 0.1650\n",
      "Epoch 66/74\n",
      "1339/1339 [==============================] - 306s 228ms/step - loss: 1.1852 - average_final_batch_ratio: 0.1650\n",
      "Epoch 67/74\n",
      "1339/1339 [==============================] - 315s 235ms/step - loss: 1.1872 - average_final_batch_ratio: 0.1650\n",
      "Epoch 68/74\n",
      "1339/1339 [==============================] - 310s 231ms/step - loss: 1.1859 - average_final_batch_ratio: 0.1650\n",
      "Epoch 69/74\n",
      "1339/1339 [==============================] - 312s 233ms/step - loss: 1.1870 - average_final_batch_ratio: 0.1650\n",
      "Epoch 70/74\n",
      "1339/1339 [==============================] - 313s 234ms/step - loss: 1.1839 - average_final_batch_ratio: 0.1650\n",
      "Epoch 71/74\n",
      "1339/1339 [==============================] - 307s 229ms/step - loss: 1.1854 - average_final_batch_ratio: 0.1650\n",
      "Epoch 72/74\n",
      "1339/1339 [==============================] - 314s 234ms/step - loss: 1.1840 - average_final_batch_ratio: 0.1650\n",
      "Epoch 73/74\n",
      "1339/1339 [==============================] - 310s 232ms/step - loss: 1.1838 - average_final_batch_ratio: 0.1650\n",
      "Epoch 74/74\n",
      "1339/1339 [==============================] - 319s 238ms/step - loss: 1.1845 - average_final_batch_ratio: 0.1650\n"
     ]
    }
   ],
   "source": [
    "model.train(256, 256, epochs=74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0:\tLength: 12\tAvg Compression: 0.229167\n",
      "Article 1000:\tLength: 42\tAvg Compression: 0.168981\n",
      "Article 2000:\tLength: 51\tAvg Compression: 0.166667\n",
      "Article 3000:\tLength: 63\tAvg Compression: 0.161458\n",
      "Article 4000:\tLength: 387\tAvg Compression: 0.202027\n",
      "Article 5000:\tLength: 1254\tAvg Compression: 0.212617\n",
      "Article 6000:\tLength: 2360\tAvg Compression: 0.204635\n",
      "Article 7000:\tLength: 3684\tAvg Compression: 0.218722\n",
      "Article 8000:\tLength: 5357\tAvg Compression: 0.226779\n",
      "Article 9000:\tLength: 7861\tAvg Compression: 0.216583\n",
      "Article 10000:\tLength: 11179\tAvg Compression: 0.217647\n",
      "Article 11000:\tLength: 15815\tAvg Compression: 0.212964\n",
      "Article 12000:\tLength: 25299\tAvg Compression: 0.203061\n",
      "Article 13000:\tLength: 79403\tAvg Compression: 0.210823\n"
     ]
    }
   ],
   "source": [
    "total_raw = 0\n",
    "total_compressed = 0\n",
    "\n",
    "huffman = Huffman(encoder.vocab_size)\n",
    "for index, encoded_article in enumerate(articles.articles_generator(1)):\n",
    "    if index % 1000 == 0 and len(encoded_article) != 0:\n",
    "        article = encoder.decode(encoded_article)\n",
    "        total_raw += len(article) * 8\n",
    "        total_compressed += huffman.archive_size(model, encoded_article)\n",
    "        print('Article %d:\\tLength: %d\\tAvg Compression: %f' % (index, len(article), total_compressed/total_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "1339/1339 [==============================] - 312s 233ms/step - loss: 1.1844 - average_final_batch_ratio: 0.1650\n",
      "Epoch 2/15\n",
      "1339/1339 [==============================] - 316s 236ms/step - loss: 1.1842 - average_final_batch_ratio: 0.1650\n",
      "Epoch 3/15\n",
      "1339/1339 [==============================] - 316s 236ms/step - loss: 1.1841 - average_final_batch_ratio: 0.1650\n",
      "Epoch 4/15\n",
      "1339/1339 [==============================] - 309s 231ms/step - loss: 1.1826 - average_final_batch_ratio: 0.1650\n",
      "Epoch 5/15\n",
      "1339/1339 [==============================] - 313s 234ms/step - loss: 1.1840 - average_final_batch_ratio: 0.1650\n",
      "Epoch 6/15\n",
      "1339/1339 [==============================] - 311s 232ms/step - loss: 1.1832 - average_final_batch_ratio: 0.1650\n",
      "Epoch 7/15\n",
      "1339/1339 [==============================] - 314s 235ms/step - loss: 1.1833 - average_final_batch_ratio: 0.1650\n",
      "Epoch 8/15\n",
      "1339/1339 [==============================] - 308s 230ms/step - loss: 1.1826 - average_final_batch_ratio: 0.1650\n",
      "Epoch 9/15\n",
      "1339/1339 [==============================] - 312s 233ms/step - loss: 1.1819 - average_final_batch_ratio: 0.1650\n",
      "Epoch 10/15\n",
      "1339/1339 [==============================] - 313s 234ms/step - loss: 1.1833 - average_final_batch_ratio: 0.1650\n",
      "Epoch 11/15\n",
      "1339/1339 [==============================] - 309s 231ms/step - loss: 1.1824 - average_final_batch_ratio: 0.1650\n",
      "Epoch 12/15\n",
      "1339/1339 [==============================] - 313s 233ms/step - loss: 1.1813 - average_final_batch_ratio: 0.1650\n",
      "Epoch 13/15\n",
      "1339/1339 [==============================] - 310s 232ms/step - loss: 1.1829 - average_final_batch_ratio: 0.1650\n",
      "Epoch 14/15\n",
      "1339/1339 [==============================] - 314s 234ms/step - loss: 1.1804 - average_final_batch_ratio: 0.1650\n",
      "Epoch 15/15\n",
      "1339/1339 [==============================] - 306s 229ms/step - loss: 1.1807 - average_final_batch_ratio: 0.1650\n"
     ]
    }
   ],
   "source": [
    "model.train(256, 256, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0:\tLength: 12\tAvg Compression: 0.239583\n",
      "Article 1000:\tLength: 42\tAvg Compression: 0.159722\n",
      "Article 2000:\tLength: 51\tAvg Compression: 0.154762\n",
      "Article 3000:\tLength: 63\tAvg Compression: 0.160714\n",
      "Article 4000:\tLength: 387\tAvg Compression: 0.207207\n",
      "Article 5000:\tLength: 1254\tAvg Compression: 0.216211\n",
      "Article 6000:\tLength: 2360\tAvg Compression: 0.205985\n",
      "Article 7000:\tLength: 3684\tAvg Compression: 0.220903\n",
      "Article 8000:\tLength: 5357\tAvg Compression: 0.228028\n",
      "Article 9000:\tLength: 7861\tAvg Compression: 0.217930\n",
      "Article 10000:\tLength: 11179\tAvg Compression: 0.220159\n",
      "Article 11000:\tLength: 15815\tAvg Compression: 0.215378\n",
      "Article 12000:\tLength: 25299\tAvg Compression: 0.204995\n",
      "Article 13000:\tLength: 79403\tAvg Compression: 0.212611\n"
     ]
    }
   ],
   "source": [
    "total_raw = 0\n",
    "total_compressed = 0\n",
    "\n",
    "huffman = Huffman(encoder.vocab_size)\n",
    "for index, encoded_article in enumerate(articles.articles_generator(1)):\n",
    "    if index % 1000 == 0 and len(encoded_article) != 0:\n",
    "        article = encoder.decode(encoded_article)\n",
    "        total_raw += len(article) * 8\n",
    "        total_compressed += huffman.archive_size(model, encoded_article)\n",
    "        print('Article %d:\\tLength: %d\\tAvg Compression: %f' % (index, len(article), total_compressed/total_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (512, 128)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (512, 128, 128)           32640     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (512, 128, 512)           986112    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (512, 128, 512)           1575936   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (512, 128, 255)           130815    \n",
      "=================================================================\n",
      "Total params: 2,725,503\n",
      "Trainable params: 2,725,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.training_model(512, 128).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1602/1602 [==============================] - 489s 305ms/step - loss: 0.9856 - average_final_batch_ratio: 0.1011\n",
      "Epoch 2/15\n",
      "   1/1602 [..............................] - ETA: 17:25"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:GPU:0 to /job:localhost/replica:0/task:0/device:CPU:0 in order to run Identity: GPU sync failed [Op:Identity]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a6e115990e5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-729f5d70ccd8>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, batch_size, batched_item_length, epochs)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched_item_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1036\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nEpoch %05d: saving model to %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1038\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1039\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave_weights\u001b[1;34m(self, filepath, overwrite, save_format)\u001b[0m\n\u001b[0;32m   1121\u001b[0m              'saved.\\n\\nConsider using a TensorFlow optimizer from `tf.train`.')\n\u001b[0;32m   1122\u001b[0m             % (optimizer,))\n\u001b[1;32m-> 1123\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1124\u001b[0m       \u001b[1;31m# Record this checkpoint so it's visible from tf.train.latest_checkpoint.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m       checkpoint_management.update_checkpoint_state_internal(\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix, checkpoint_number, session)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecursive_create_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m     save_path, new_feed_additions = self._save_cached_when_graph_building(\n\u001b[1;32m-> 1168\u001b[1;33m         file_prefix=file_prefix_tensor, object_graph_tensor=object_graph_tensor)\n\u001b[0m\u001b[0;32m   1169\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnew_feed_additions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m       \u001b[0mfeed_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_feed_additions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36m_save_cached_when_graph_building\u001b[1;34m(self, file_prefix, object_graph_tensor)\u001b[0m\n\u001b[0;32m   1114\u001b[0m         or context.executing_eagerly() or ops.inside_function()):\n\u001b[0;32m   1115\u001b[0m       \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctional_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiDeviceSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnamed_saveable_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1116\u001b[1;33m       \u001b[0msave_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1117\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msave_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;31m# _SingleDeviceSaver will use the CPU device when necessary, but initial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;31m# read operations should be placed on the SaveableObject's device.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[0msharded_saves\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshard_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msharded_saves\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix)\u001b[0m\n\u001b[0;32m     67\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mspec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msaveable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mtensor_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mtensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0mtensor_slices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\training\\saving\\saveable_object.py\u001b[0m in \u001b[0;36mtensor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\training\\saving\\saveable_object_util.py\u001b[0m in \u001b[0;36mf\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;31m# we copy them to CPU on the same machine first.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/device:CPU:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m               \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m   \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m   \u001b[1;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_handle_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   3824\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3825\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3826\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3827\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3828\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6604\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6605\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6606\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6607\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:GPU:0 to /job:localhost/replica:0/task:0/device:CPU:0 in order to run Identity: GPU sync failed [Op:Identity]"
     ]
    }
   ],
   "source": [
    "model.train(512, 128, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1422/1422 [==============================] - 547s 385ms/step - loss: 0.8838 - average_final_batch_ratio: 0.0992\n",
      "Epoch 2/4\n",
      "1422/1422 [==============================] - 403s 283ms/step - loss: 0.8855 - average_final_batch_ratio: 0.0992\n",
      "Epoch 3/4\n",
      "1422/1422 [==============================] - 401s 282ms/step - loss: 0.8875 - average_final_batch_ratio: 0.0992\n",
      "Epoch 4/4\n",
      "1422/1422 [==============================] - 396s 278ms/step - loss: 0.8872 - average_final_batch_ratio: 0.0992\n"
     ]
    }
   ],
   "source": [
    "model.train(640, 128, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0:\tLength: 12\tAvg Compression: 0.239583\n",
      "Article 1000:\tLength: 42\tAvg Compression: 0.199074\n",
      "Article 2000:\tLength: 51\tAvg Compression: 0.201190\n",
      "Article 3000:\tLength: 63\tAvg Compression: 0.173363\n",
      "Article 4000:\tLength: 387\tAvg Compression: 0.209234\n",
      "Article 5000:\tLength: 1254\tAvg Compression: 0.216763\n",
      "Article 6000:\tLength: 2360\tAvg Compression: 0.227932\n",
      "Article 7000:\tLength: 3684\tAvg Compression: 0.234083\n",
      "Article 8000:\tLength: 5357\tAvg Compression: 0.236289\n",
      "Article 9000:\tLength: 7861\tAvg Compression: 0.221377\n",
      "Article 10000:\tLength: 11179\tAvg Compression: 0.221105\n",
      "Article 11000:\tLength: 15815\tAvg Compression: 0.215679\n",
      "Article 12000:\tLength: 25299\tAvg Compression: 0.205101\n",
      "Article 13000:\tLength: 79403\tAvg Compression: 0.211797\n"
     ]
    }
   ],
   "source": [
    "total_raw = 0\n",
    "total_compressed = 0\n",
    "\n",
    "huffman = Huffman(encoder.vocab_size)\n",
    "for index, encoded_article in enumerate(articles.articles_generator(1)):\n",
    "    if index % 1000 == 0 and len(encoded_article) != 0:\n",
    "        article = encoder.decode(encoded_article)\n",
    "        total_raw += len(article) * 8\n",
    "        total_compressed += huffman.archive_size(model, encoded_article)\n",
    "        print('Article %d:\\tLength: %d\\tAvg Compression: %f' % (index, len(article), total_compressed/total_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "1290/1290 [==============================] - 427s 331ms/step - loss: 0.8118 - average_final_batch_ratio: 0.0969\n",
      "Epoch 2/5\n",
      "1290/1290 [==============================] - 425s 329ms/step - loss: 0.8144 - average_final_batch_ratio: 0.0969\n",
      "Epoch 3/5\n",
      "1290/1290 [==============================] - 422s 327ms/step - loss: 0.9594 - average_final_batch_ratio: 0.0969\n",
      "Epoch 4/5\n",
      "1290/1290 [==============================] - 421s 327ms/step - loss: 1.0268 - average_final_batch_ratio: 0.0969\n",
      "Epoch 5/5\n",
      "1290/1290 [==============================] - 417s 323ms/step - loss: 1.0107 - average_final_batch_ratio: 0.0969\n"
     ]
    }
   ],
   "source": [
    "model.train(768, 128, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0:\tLength: 12\tAvg Compression: 0.229167\n",
      "Article 1000:\tLength: 42\tAvg Compression: 0.187500\n",
      "Article 2000:\tLength: 51\tAvg Compression: 0.197619\n",
      "Article 3000:\tLength: 63\tAvg Compression: 0.177083\n",
      "Article 4000:\tLength: 387\tAvg Compression: 0.230856\n",
      "Article 5000:\tLength: 1254\tAvg Compression: 0.243988\n",
      "Article 6000:\tLength: 2360\tAvg Compression: 0.257436\n",
      "Article 7000:\tLength: 3684\tAvg Compression: 0.267843\n",
      "Article 8000:\tLength: 5357\tAvg Compression: 0.269938\n",
      "Article 9000:\tLength: 7861\tAvg Compression: 0.253583\n",
      "Article 10000:\tLength: 11179\tAvg Compression: 0.253112\n",
      "Article 11000:\tLength: 15815\tAvg Compression: 0.249144\n",
      "Article 12000:\tLength: 25299\tAvg Compression: 0.234899\n",
      "Article 13000:\tLength: 79403\tAvg Compression: 0.241570\n"
     ]
    }
   ],
   "source": [
    "total_raw = 0\n",
    "total_compressed = 0\n",
    "\n",
    "huffman = Huffman(encoder.vocab_size)\n",
    "for index, encoded_article in enumerate(articles.articles_generator(1)):\n",
    "    if index % 1000 == 0 and len(encoded_article) != 0:\n",
    "        article = encoder.decode(encoded_article)\n",
    "        total_raw += len(article) * 8\n",
    "        total_compressed += huffman.archive_size(model, encoded_article)\n",
    "        print('Article %d:\\tLength: %d\\tAvg Compression: %f' % (index, len(article), total_compressed/total_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1290/1290 [==============================] - 433s 335ms/step - loss: 0.9998 - average_final_batch_ratio: 0.0969\n",
      "Epoch 2/15\n",
      "1290/1290 [==============================] - 428s 332ms/step - loss: 0.9891 - average_final_batch_ratio: 0.0969\n",
      "Epoch 3/15\n",
      "1290/1290 [==============================] - 431s 334ms/step - loss: 0.9774 - average_final_batch_ratio: 0.0969\n",
      "Epoch 4/15\n",
      "1290/1290 [==============================] - 434s 337ms/step - loss: 0.9645 - average_final_batch_ratio: 0.0969\n",
      "Epoch 5/15\n",
      "1290/1290 [==============================] - 434s 337ms/step - loss: 0.9504 - average_final_batch_ratio: 0.0969\n",
      "Epoch 6/15\n",
      "1290/1290 [==============================] - 435s 337ms/step - loss: 0.9338 - average_final_batch_ratio: 0.0969\n",
      "Epoch 7/15\n",
      "1290/1290 [==============================] - 433s 336ms/step - loss: 0.9153 - average_final_batch_ratio: 0.0969\n",
      "Epoch 8/15\n",
      "1290/1290 [==============================] - 435s 337ms/step - loss: 0.8980 - average_final_batch_ratio: 0.0969\n",
      "Epoch 9/15\n",
      "1290/1290 [==============================] - 429s 333ms/step - loss: 0.8832 - average_final_batch_ratio: 0.0969\n",
      "Epoch 10/15\n",
      "1290/1290 [==============================] - 437s 338ms/step - loss: 0.8670 - average_final_batch_ratio: 0.0969\n",
      "Epoch 11/15\n",
      "1290/1290 [==============================] - 434s 336ms/step - loss: 0.8528 - average_final_batch_ratio: 0.0969\n",
      "Epoch 12/15\n",
      "1290/1290 [==============================] - 435s 337ms/step - loss: 0.8417 - average_final_batch_ratio: 0.0969\n",
      "Epoch 13/15\n",
      "1290/1290 [==============================] - 436s 338ms/step - loss: 0.8330 - average_final_batch_ratio: 0.0969\n",
      "Epoch 14/15\n",
      "1290/1290 [==============================] - 434s 336ms/step - loss: 0.8263 - average_final_batch_ratio: 0.0969\n",
      "Epoch 15/15\n",
      "1290/1290 [==============================] - 436s 338ms/step - loss: 0.8233 - average_final_batch_ratio: 0.0969\n"
     ]
    }
   ],
   "source": [
    "model.train(768, 128, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0:\tLength: 12\tAvg Compression: 0.270833\n",
      "Article 1000:\tLength: 42\tAvg Compression: 0.182870\n",
      "Article 2000:\tLength: 51\tAvg Compression: 0.194048\n",
      "Article 3000:\tLength: 63\tAvg Compression: 0.168155\n",
      "Article 4000:\tLength: 387\tAvg Compression: 0.200450\n",
      "Article 5000:\tLength: 1254\tAvg Compression: 0.209577\n",
      "Article 6000:\tLength: 2360\tAvg Compression: 0.221126\n",
      "Article 7000:\tLength: 3684\tAvg Compression: 0.229562\n",
      "Article 8000:\tLength: 5357\tAvg Compression: 0.231927\n",
      "Article 9000:\tLength: 7861\tAvg Compression: 0.216993\n",
      "Article 10000:\tLength: 11179\tAvg Compression: 0.217570\n",
      "Article 11000:\tLength: 15815\tAvg Compression: 0.213926\n",
      "Article 12000:\tLength: 25299\tAvg Compression: 0.203647\n",
      "Article 13000:\tLength: 79403\tAvg Compression: 0.211110\n"
     ]
    }
   ],
   "source": [
    "total_raw = 0\n",
    "total_compressed = 0\n",
    "\n",
    "huffman = Huffman(encoder.vocab_size)\n",
    "for index, encoded_article in enumerate(articles.articles_generator(1)):\n",
    "    if index % 1000 == 0 and len(encoded_article) != 0:\n",
    "        article = encoder.decode(encoded_article)\n",
    "        total_raw += len(article) * 8\n",
    "        total_compressed += huffman.archive_size(model, encoded_article)\n",
    "        print('Article %d:\\tLength: %d\\tAvg Compression: %f' % (index, len(article), total_compressed/total_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нека запишем примерно предвиждане на данни; ще го използваме за тестване на кодиране на huffman и аритметично кодиране"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1101"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles.encoded_articles[5449])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = articles.encoded_articles[5449]\n",
    "input_eval = np.array([[text[0]]], dtype=TYPE)\n",
    "\n",
    "model.predicting_model.reset_states()\n",
    "\n",
    "char_actual = []\n",
    "char_predictions = []\n",
    "\n",
    "for byte in text[1:]:\n",
    "    predictions = model.predict(input_eval)\n",
    "    predictions = tf.squeeze(predictions, 0) # remove the batch dimension\n",
    "\n",
    "    weights = tf.nn.softmax(predictions[0]).numpy()\n",
    "    char_actual.append(byte)\n",
    "    char_predictions.append(weights)\n",
    "\n",
    "    input_eval = tf.expand_dims([byte], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('char-actual', 'wb') as file:\n",
    "    file.write(np.array(char_actual).tobytes())\n",
    "\n",
    "with open('char-predictions', 'wb') as file:\n",
    "    file.write(np.array(char_predictions).tobytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Да продължим с тренирането..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1290/1290 [==============================] - 424s 329ms/step - loss: 0.8181 - average_final_batch_ratio: 0.0969\n",
      "Epoch 2/15\n",
      "1290/1290 [==============================] - 434s 336ms/step - loss: 0.8190 - average_final_batch_ratio: 0.0969\n",
      "Epoch 3/15\n",
      "1290/1290 [==============================] - 425s 330ms/step - loss: 0.8153 - average_final_batch_ratio: 0.0969\n",
      "Epoch 4/15\n",
      "1290/1290 [==============================] - 422s 327ms/step - loss: 0.8163 - average_final_batch_ratio: 0.0969\n",
      "Epoch 5/15\n",
      "1290/1290 [==============================] - 423s 328ms/step - loss: 0.8149 - average_final_batch_ratio: 0.0969\n",
      "Epoch 6/15\n",
      "1290/1290 [==============================] - 418s 324ms/step - loss: 0.8149 - average_final_batch_ratio: 0.0969\n",
      "Epoch 7/15\n",
      "1290/1290 [==============================] - 423s 328ms/step - loss: 0.8163 - average_final_batch_ratio: 0.0969\n",
      "Epoch 8/15\n",
      "1290/1290 [==============================] - 422s 327ms/step - loss: 0.8144 - average_final_batch_ratio: 0.0969\n",
      "Epoch 9/15\n",
      "1290/1290 [==============================] - 420s 325ms/step - loss: 0.8124 - average_final_batch_ratio: 0.0969\n",
      "Epoch 10/15\n",
      "1290/1290 [==============================] - 427s 331ms/step - loss: 0.8157 - average_final_batch_ratio: 0.0969\n",
      "Epoch 11/15\n",
      "1290/1290 [==============================] - 443s 343ms/step - loss: 0.8141 - average_final_batch_ratio: 0.0969\n",
      "Epoch 12/15\n",
      "1290/1290 [==============================] - 435s 337ms/step - loss: 0.8129 - average_final_batch_ratio: 0.0969\n",
      "Epoch 13/15\n",
      "1290/1290 [==============================] - 438s 340ms/step - loss: 0.8117 - average_final_batch_ratio: 0.0969\n",
      "Epoch 14/15\n",
      "1290/1290 [==============================] - 441s 342ms/step - loss: 0.8151 - average_final_batch_ratio: 0.0969\n",
      "Epoch 15/15\n",
      "1290/1290 [==============================] - 433s 335ms/step - loss: 0.8105 - average_final_batch_ratio: 0.0969\n"
     ]
    }
   ],
   "source": [
    "model.train(768, 128, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0:\tLength: 12\tAvg Compression: 0.270833\n",
      "Article 1000:\tLength: 42\tAvg Compression: 0.180556\n",
      "Article 2000:\tLength: 51\tAvg Compression: 0.200000\n",
      "Article 3000:\tLength: 63\tAvg Compression: 0.168899\n",
      "Article 4000:\tLength: 387\tAvg Compression: 0.205180\n",
      "Article 5000:\tLength: 1254\tAvg Compression: 0.217178\n",
      "Article 6000:\tLength: 2360\tAvg Compression: 0.231500\n",
      "Article 7000:\tLength: 3684\tAvg Compression: 0.237171\n",
      "Article 8000:\tLength: 5357\tAvg Compression: 0.238579\n",
      "Article 9000:\tLength: 7861\tAvg Compression: 0.224972\n",
      "Article 10000:\tLength: 11179\tAvg Compression: 0.226946\n",
      "Article 11000:\tLength: 15815\tAvg Compression: 0.221375\n",
      "Article 12000:\tLength: 25299\tAvg Compression: 0.210802\n",
      "Article 13000:\tLength: 79403\tAvg Compression: 0.217456\n"
     ]
    }
   ],
   "source": [
    "total_raw = 0\n",
    "total_compressed = 0\n",
    "\n",
    "huffman = Huffman(encoder.vocab_size)\n",
    "for index, encoded_article in enumerate(articles.articles_generator(1)):\n",
    "    if index % 1000 == 0 and len(encoded_article) != 0:\n",
    "        article = encoder.decode(encoded_article)\n",
    "        total_raw += len(article) * 8\n",
    "        total_compressed += huffman.archive_size(model, encoded_article)\n",
    "        print('Article %d:\\tLength: %d\\tAvg Compression: %f' % (index, len(article), total_compressed/total_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = articles.encoded_articles[5449]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([223,  32, 247, ..., 219, 184,  56], dtype=int16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=bernoulli's inequality=\n",
      "in [[mathematics]], '''bernoulli's inequality''' is an [[inequality]] that approximates [[exponentiation]]s of 1 X <i>x</i>.\n",
      "\n",
      "the inequality states that\n",
      ":<math>(1 X x)Xr \\geq 1 X rx\\!</math>\n",
      "for every [[integer]] <i>r</i> X 0 and every [[real number]] <i>x</i> X X1. if the exponent <i>r</i> is [[even number|even]], then the inequality is valid for ''all'' real numbers <i>x</i>. the strict version of the inequality reads\n",
      ":<math>(1 X x)Xr > 1 X rx\\!</math>\n",
      "for every integer <i>r</i> X 2 and every real number <i>x</i> X X1 with <i>x</i> X 0.\n",
      "\n",
      "bernoulli's inequality is often used as the crucial step in the [[proof (math)|proof]] of other inequalities. it can itself be proved using [[mathematical induction]]. \n",
      "\n",
      "the exponent <i>r</i> can be generalized to an arbitrary real number as follows: if <i>x</i> > X1, then\n",
      ":<math>(1 X x)Xr \\geq 1 X rx\\!</math>\n",
      "for <i>r</i> X 0 or <i>r</i> X 1, and \n",
      ":<math>(1 X x)Xr \\leq 1 X rx\\!</math>\n",
      "for 0 X <i>r</i> X 1.\n",
      "this generalization can be proved by comparing [[derivative]]s.\n",
      "again, the strict versions of these inequalities require <i>x</i> X 0 and <i>r</i> X 0, 1.\n",
      "\n",
      "== related inequalities ==\n",
      "the following inequality estimates the <i>r</i>-th power of 1 X <i>x</i> from the other side. for any real numbers <i>x</i>, <i>r</i> > 0, one has\n",
      ":<math>(1 X x)Xr < eX{rx},\\!</math>\n",
      "where <i>e</i> = [[e (number)|2.718...]].\n",
      "this may be proved using the inequality (1 X 1/<i>k</i>)<sup><i>k</i></sup> < <i>e</i>.\n",
      "\n",
      "[[category:inequalities]]\n",
      "\n",
      "[[de:bernoullische ungleichung]]\n",
      "[[fr:inégalité de bernoulli]]\n",
      "[[it:diseguaglianza di bernoulli]]\n",
      "[[pl:nierXwnoX bernoulliego]]\n",
      "[[ru:X X]]\n",
      "[[zh:X]]\n"
     ]
    }
   ],
   "source": [
    "print(encoder.decode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1653"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder.decode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
