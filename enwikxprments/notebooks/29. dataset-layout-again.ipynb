{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предишната тетрадка получихме доста високи стойности за `loss`. Преди това да се случи променихме две неща - започнахме да експериментираме с обучаване на целият набор данни. И също така направихме промени в реда на обхождане на статиите при обучение. Да проверим първото би отнело значително време. Затова можем да проверим второто.\n",
    "\n",
    "За целта ще вземем класа `Article` от предишната тетардка и ще го променим да прилича повече на предишни експерименти. След това ще преминем към обучение..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for physical_device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_device, enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE=np.int16\n",
    "\n",
    "subword_text_encoder = tfds.features.text.SubwordTextEncoder.load_from_file('vocab_4096')\n",
    "\n",
    "class Articles:\n",
    "    EMPTY_ARTICLE = np.array([], dtype=TYPE) # used for padding\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        with open(path, 'rb') as text_file:\n",
    "            data = text_file.read()\n",
    "\n",
    "        articles = set(data.split(b'\\0'))\n",
    "        articles = [np.array(subword_text_encoder.encode(article), dtype=TYPE) for article in articles]\n",
    "        self.articles = sorted(articles, key=len)\n",
    "\n",
    "    def articles_generator(self, batch_size = 1, start = 0, end = None):\n",
    "        end = end or len(self.articles)\n",
    "\n",
    "        for _ in range(batch_size - ((end - start - 1) % batch_size + 1)):\n",
    "            yield self.EMPTY_ARTICLE\n",
    "\n",
    "        for article in itertools.islice(self.articles, start, end):\n",
    "            yield article\n",
    "\n",
    "    def subbatch_generator(self, batch_size, batch_length, start = 0, end = None):\n",
    "        end = end or len(self.articles)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_generator(self.articles_generator, args=(batch_size, start, end), output_types=TYPE)\n",
    "        dataset = dataset.padded_batch(batch_size, padded_shapes=([None]), drop_remainder=True)\n",
    "        dataset = dataset.shuffle(100)\n",
    "\n",
    "        for batch in dataset.as_numpy_iterator():\n",
    "            remaining = batch\n",
    "            while remaining.shape[1] > batch_length + 1:\n",
    "                yield remaining[:, :batch_length + 1]\n",
    "                remaining = remaining[:, batch_length:]\n",
    "\n",
    "            yield remaining\n",
    "            if remaining.shape[1] == batch_length + 1:\n",
    "                yield np.zeros((batch_size, 2), dtype=TYPE)\n",
    "\n",
    "    def dataset(self, batch_size, batch_length, start = 0, end = None):\n",
    "        end = end or len(self.articles)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_generator(self.subbatch_generator, args=(batch_size, batch_length, start, end), output_types=TYPE, output_shapes=(batch_size, None))\n",
    "        return dataset.map(lambda batch: (batch[:, :-1], batch[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = Articles('page_revisions_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def batch_count(articles, batch_size, batched_item_length):\n",
    "    return sum(math.ceil(len(x) / batched_item_length) for i, x in enumerate(articles.articles_generator(batch_size, batched_item_length)) if (i + 1) % batch_size == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31302"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_count(articles, 4096, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:      4\t batch item length: 8192\tsteps per epoch:  54883\n",
      "batch size:      8\t batch item length: 4096\tsteps per epoch:  30600\n",
      "batch size:     16\t batch item length: 2048\tsteps per epoch:  19009\n",
      "batch size:     32\t batch item length: 1024\tsteps per epoch:  13899\n",
      "batch size:     64\t batch item length:  512\tsteps per epoch:  12037\n",
      "batch size:    128\t batch item length:  256\tsteps per epoch:  11533\n",
      "batch size:    256\t batch item length:  128\tsteps per epoch:  12121\n",
      "batch size:    512\t batch item length:   64\tsteps per epoch:  14172\n",
      "batch size:   1024\t batch item length:   32\tsteps per epoch:  18709\n",
      "batch size:   2048\t batch item length:   16\tsteps per epoch:  28057\n",
      "batch size:   4096\t batch item length:    8\tsteps per epoch:  46942\n",
      "batch size:   8192\t batch item length:    4\tsteps per epoch:  84947\n"
     ]
    }
   ],
   "source": [
    "steps = 12\n",
    "\n",
    "for i in range(steps):\n",
    "    batch_size = 4 * 2**i\n",
    "    batch_item_length = 4 * 2**(steps - i - 1)\n",
    "    count = batch_count(articles, batch_size, batch_item_length)\n",
    "    print(\"batch size: %6d\\t batch item length: %4d\\tsteps per epoch: %6d\" % (batch_size, batch_item_length, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1536\n",
    "BATCHED_ITEM_LENGTH = 32\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Masking(mask_value=0, batch_input_shape=[BATCH_SIZE, None]),\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "        tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size),\n",
    "    ])\n",
    "\n",
    "checkpoint_dir = './training_checkpoints-3' # Directory where the checkpoints will be saved\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\") # Name of the checkpoint files\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
    "\n",
    "class ModelStateResetter(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.last_total_length = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        average_batch_length = logs.get('average_batch_length', 0)\n",
    "        total_length = int(round(average_batch_length * (batch + 1)))\n",
    "        current_batch_length = total_length - self.last_total_length\n",
    "        self.last_total_length = total_length\n",
    "        \n",
    "        if current_batch_length < BATCHED_ITEM_LENGTH:\n",
    "            self.model.reset_states()\n",
    "        \n",
    "model_state_resetter_callback = ModelStateResetter()\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "def average_batch_length(true_labels, predictions):\n",
    "    return tf.shape(true_labels)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_2 (Masking)          (1536, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (1536, None, 512)         2072576   \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (1536, None, 1024)        4724736   \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (1536, None, 1024)        6297600   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1536, None, 4048)        4149200   \n",
      "=================================================================\n",
      "Total params: 17,244,112\n",
      "Trainable params: 17,244,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size = subword_text_encoder.vocab_size, embedding_dim=512, rnn_units=1024)\n",
    "# model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.compile(optimizer='adam', loss=loss, metrics=[average_batch_length])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   3892/Unknown - 3041s 781ms/step - loss: 3.4269 - average_batch_length: 31.5626"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b482f2d4ad0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCHED_ITEM_LENGTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_state_resetter_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(articles.dataset(BATCH_SIZE, BATCHED_ITEM_LENGTH), epochs=5, callbacks=[checkpoint_callback, model_state_resetter_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До тук добре. Числото изглежда много подобно на това, което сме виждали в предишни тетрадки с различен код, но при идентични параметри.\n",
    "\n",
    "Да видим до каква компресия ще доведе този `loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_predicting_model(vocab_size, embedding_dim, rnn_units):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Masking(mask_value=0, batch_input_shape=[1, 1]),\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "        #tf.keras.layers.GRU(rnn_units, return_sequences=True),\n",
    "        #tf.keras.layers.GRU(rnn_units, return_sequences=True),\n",
    "        #tf.keras.layers.GRU(rnn_units, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True),\n",
    "        tf.keras.layers.Dense(vocab_size),\n",
    "    ])\n",
    "\n",
    "model = build_predicting_model(vocab_size = subword_text_encoder.vocab_size, embedding_dim=512, rnn_units=1024)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "\n",
    "class Huffman:\n",
    "    huffman = ctypes.CDLL('x64/Release/huffman')\n",
    "    \n",
    "    huffman.create_tree.restype = ctypes.c_void_p\n",
    "    huffman.destroy_tree.restype = None\n",
    "    huffman.load_weights.restype = None\n",
    "    huffman.create_code_string.restype = ctypes.c_char_p\n",
    "    \n",
    "    def __init__(self, category_count):\n",
    "        self.tree = ctypes.c_void_p(self.huffman.create_tree(category_count))\n",
    "\n",
    "    def __del__(self):\n",
    "        self.huffman.destroy_tree(self.tree)\n",
    "        \n",
    "    def load_weights(self, weights):\n",
    "        self.huffman.load_weights(self.tree, weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float)))\n",
    "    \n",
    "    def get_code_length(self, category):\n",
    "        return self.huffman.get_code_length(self.tree, category)\n",
    "\n",
    "    def get_code_zero_count(self, category):\n",
    "        return self.huffman.get_code_zero_count(self.tree, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_archive_size(model, text):\n",
    "    archived_size = 0\n",
    "    zeros = 0\n",
    "    input_eval = np.array([[0]], dtype=TYPE)\n",
    "    huffman_tree = Huffman(subword_text_encoder.vocab_size)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    model.reset_states()\n",
    "\n",
    "    for index, byte in enumerate(text):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0) # remove the batch dimension\n",
    "\n",
    "        weights = tf.nn.softmax(predictions[0]).numpy()\n",
    "        huffman_tree.load_weights(weights)\n",
    "        zeros += huffman_tree.get_code_zero_count(byte.item())\n",
    "        archived_size += huffman_tree.get_code_length(byte.item())\n",
    "\n",
    "        input_eval = tf.expand_dims([byte], 0)\n",
    "  \n",
    "    return archived_size, zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0:\tLength: 56\tCompression: 1.035714\tAvg Compression: 1.035714\n",
      "Article 100:\tLength: 104\tCompression: 0.932692\tAvg Compression: 0.808464\n",
      "Article 200:\tLength: 72\tCompression: 1.069444\tAvg Compression: 0.794248\n",
      "Article 300:\tLength: 78112\tCompression: 0.786038\tAvg Compression: 0.792500\n",
      "Article 400:\tLength: 40\tCompression: 1.450000\tAvg Compression: 0.798137\n",
      "Article 500:\tLength: 3600\tCompression: 0.831667\tAvg Compression: 0.797820\n",
      "Article 600:\tLength: 88\tCompression: 0.818182\tAvg Compression: 0.796234\n",
      "Article 700:\tLength: 78144\tCompression: 0.794815\tAvg Compression: 0.796330\n",
      "Article 800:\tLength: 38392\tCompression: 0.731038\tAvg Compression: 0.797411\n",
      "Article 900:\tLength: 2840\tCompression: 0.779225\tAvg Compression: 0.798297\n",
      "Article 1000:\tLength: 5960\tCompression: 0.774832\tAvg Compression: 0.797415\n",
      "Article 1100:\tLength: 104\tCompression: 0.990385\tAvg Compression: 0.797880\n",
      "Article 1200:\tLength: 120\tCompression: 1.100000\tAvg Compression: 0.798434\n",
      "Article 1300:\tLength: 11192\tCompression: 0.745890\tAvg Compression: 0.799402\n",
      "Article 1400:\tLength: 6824\tCompression: 0.809936\tAvg Compression: 0.799358\n",
      "Article 1500:\tLength: 5976\tCompression: 0.694444\tAvg Compression: 0.801221\n",
      "Article 1600:\tLength: 13096\tCompression: 0.829337\tAvg Compression: 0.801838\n",
      "Article 1700:\tLength: 29104\tCompression: 0.841499\tAvg Compression: 0.802856\n",
      "Article 1800:\tLength: 5496\tCompression: 0.840429\tAvg Compression: 0.803578\n",
      "Article 1900:\tLength: 1488\tCompression: 0.761425\tAvg Compression: 0.804074\n",
      "Article 2000:\tLength: 88\tCompression: 0.931818\tAvg Compression: 0.804545\n",
      "Article 2100:\tLength: 7936\tCompression: 0.841482\tAvg Compression: 0.804864\n",
      "Article 2200:\tLength: 33688\tCompression: 0.881174\tAvg Compression: 0.805401\n",
      "Article 2300:\tLength: 80\tCompression: 1.037500\tAvg Compression: 0.805945\n",
      "Article 2400:\tLength: 42096\tCompression: 0.797487\tAvg Compression: 0.806018\n",
      "Article 2500:\tLength: 104\tCompression: 1.067308\tAvg Compression: 0.806294\n",
      "Article 2600:\tLength: 96\tCompression: 1.041667\tAvg Compression: 0.806115\n",
      "Article 2700:\tLength: 21048\tCompression: 0.807345\tAvg Compression: 0.806062\n",
      "Article 2800:\tLength: 56\tCompression: 1.160714\tAvg Compression: 0.805639\n",
      "Article 2900:\tLength: 96\tCompression: 1.197917\tAvg Compression: 0.805520\n",
      "Article 3000:\tLength: 53128\tCompression: 0.802722\tAvg Compression: 0.805856\n",
      "Article 3100:\tLength: 736\tCompression: 0.832880\tAvg Compression: 0.806077\n",
      "Article 3200:\tLength: 27680\tCompression: 0.798952\tAvg Compression: 0.806277\n",
      "Article 3300:\tLength: 64\tCompression: 1.062500\tAvg Compression: 0.806788\n",
      "Article 3400:\tLength: 13200\tCompression: 0.798864\tAvg Compression: 0.807526\n",
      "Article 3500:\tLength: 14440\tCompression: 0.760734\tAvg Compression: 0.808188\n",
      "Article 3600:\tLength: 1672\tCompression: 0.861244\tAvg Compression: 0.808258\n",
      "Article 3700:\tLength: 13976\tCompression: 0.757155\tAvg Compression: 0.808435\n",
      "Article 3800:\tLength: 2232\tCompression: 0.825717\tAvg Compression: 0.808846\n",
      "Article 3900:\tLength: 56\tCompression: 0.982143\tAvg Compression: 0.808603\n",
      "Article 4000:\tLength: 112\tCompression: 0.910714\tAvg Compression: 0.807833\n",
      "Article 4100:\tLength: 51352\tCompression: 0.781021\tAvg Compression: 0.807286\n",
      "Article 4200:\tLength: 23984\tCompression: 0.866703\tAvg Compression: 0.807035\n",
      "Article 4300:\tLength: 80\tCompression: 0.950000\tAvg Compression: 0.806900\n",
      "Article 4400:\tLength: 14080\tCompression: 0.826491\tAvg Compression: 0.806640\n",
      "Article 4500:\tLength: 72368\tCompression: 0.836046\tAvg Compression: 0.806562\n",
      "Article 4600:\tLength: 99848\tCompression: 0.791233\tAvg Compression: 0.806523\n",
      "Article 4700:\tLength: 112\tCompression: 1.000000\tAvg Compression: 0.806565\n",
      "Article 4800:\tLength: 80\tCompression: 0.975000\tAvg Compression: 0.806371\n",
      "Article 4900:\tLength: 72\tCompression: 1.027778\tAvg Compression: 0.806218\n",
      "Article 5000:\tLength: 1280\tCompression: 0.778125\tAvg Compression: 0.806290\n",
      "Article 5100:\tLength: 1032\tCompression: 0.875000\tAvg Compression: 0.805658\n",
      "Article 5200:\tLength: 46528\tCompression: 0.836077\tAvg Compression: 0.806049\n",
      "Article 5300:\tLength: 3184\tCompression: 0.835113\tAvg Compression: 0.806123\n",
      "Article 5400:\tLength: 60256\tCompression: 0.811123\tAvg Compression: 0.806209\n",
      "Article 5500:\tLength: 19576\tCompression: 0.701522\tAvg Compression: 0.806245\n",
      "Article 5600:\tLength: 11416\tCompression: 0.778732\tAvg Compression: 0.806164\n",
      "Article 5700:\tLength: 34920\tCompression: 0.823110\tAvg Compression: 0.806131\n",
      "Article 5800:\tLength: 18448\tCompression: 0.851095\tAvg Compression: 0.806287\n",
      "Article 5900:\tLength: 416\tCompression: 0.634615\tAvg Compression: 0.806268\n",
      "Article 6000:\tLength: 8600\tCompression: 0.803256\tAvg Compression: 0.806268\n",
      "Article 6100:\tLength: 7368\tCompression: 0.736971\tAvg Compression: 0.806605\n",
      "Article 6200:\tLength: 6432\tCompression: 0.734297\tAvg Compression: 0.806677\n",
      "Article 6300:\tLength: 30736\tCompression: 0.703995\tAvg Compression: 0.806543\n",
      "Article 6400:\tLength: 144\tCompression: 0.958333\tAvg Compression: 0.806343\n",
      "Article 6500:\tLength: 5824\tCompression: 0.743647\tAvg Compression: 0.806282\n",
      "Article 6600:\tLength: 976\tCompression: 0.788934\tAvg Compression: 0.806021\n",
      "Article 6700:\tLength: 6312\tCompression: 0.848384\tAvg Compression: 0.806106\n",
      "Article 6800:\tLength: 51744\tCompression: 0.783550\tAvg Compression: 0.806286\n",
      "Article 6900:\tLength: 88\tCompression: 1.159091\tAvg Compression: 0.806434\n",
      "Article 7000:\tLength: 51720\tCompression: 0.828422\tAvg Compression: 0.806387\n",
      "Article 7100:\tLength: 87840\tCompression: 0.846596\tAvg Compression: 0.806568\n",
      "Article 7200:\tLength: 113312\tCompression: 0.750953\tAvg Compression: 0.806661\n",
      "Article 7300:\tLength: 6072\tCompression: 0.809453\tAvg Compression: 0.806131\n",
      "Article 7400:\tLength: 3920\tCompression: 0.812755\tAvg Compression: 0.805839\n",
      "Article 7500:\tLength: 96\tCompression: 0.854167\tAvg Compression: 0.805751\n",
      "Article 7600:\tLength: 1048\tCompression: 0.773855\tAvg Compression: 0.805536\n",
      "Article 7700:\tLength: 10184\tCompression: 0.881874\tAvg Compression: 0.805605\n",
      "Article 7800:\tLength: 12392\tCompression: 0.797853\tAvg Compression: 0.805633\n",
      "Article 7900:\tLength: 10192\tCompression: 0.962520\tAvg Compression: 0.805801\n",
      "Article 8000:\tLength: 64\tCompression: 1.031250\tAvg Compression: 0.805730\n",
      "Article 8100:\tLength: 96\tCompression: 0.927083\tAvg Compression: 0.805905\n",
      "Article 8200:\tLength: 4320\tCompression: 0.713426\tAvg Compression: 0.805958\n",
      "Article 8300:\tLength: 64\tCompression: 0.984375\tAvg Compression: 0.805977\n",
      "Article 8400:\tLength: 13440\tCompression: 0.853051\tAvg Compression: 0.805896\n",
      "Article 8500:\tLength: 38008\tCompression: 0.824563\tAvg Compression: 0.805746\n",
      "Article 8600:\tLength: 56\tCompression: 1.160714\tAvg Compression: 0.805697\n",
      "Article 8700:\tLength: 15728\tCompression: 0.801564\tAvg Compression: 0.805160\n",
      "Article 8800:\tLength: 72\tCompression: 0.902778\tAvg Compression: 0.805138\n",
      "Article 8900:\tLength: 48\tCompression: 1.062500\tAvg Compression: 0.805126\n",
      "Article 9000:\tLength: 64\tCompression: 0.968750\tAvg Compression: 0.804042\n",
      "Article 9100:\tLength: 72\tCompression: 0.958333\tAvg Compression: 0.803966\n",
      "Article 9200:\tLength: 26992\tCompression: 0.842879\tAvg Compression: 0.803951\n",
      "Article 9300:\tLength: 152\tCompression: 0.763158\tAvg Compression: 0.804032\n",
      "Article 9400:\tLength: 88024\tCompression: 0.766109\tAvg Compression: 0.803749\n",
      "Article 9500:\tLength: 728\tCompression: 0.574176\tAvg Compression: 0.803720\n",
      "Article 9600:\tLength: 7456\tCompression: 0.818670\tAvg Compression: 0.803419\n",
      "Article 9700:\tLength: 216712\tCompression: 0.820033\tAvg Compression: 0.803366\n",
      "Article 9800:\tLength: 144\tCompression: 1.041667\tAvg Compression: 0.803217\n",
      "Article 9900:\tLength: 12216\tCompression: 0.769974\tAvg Compression: 0.803207\n"
     ]
    }
   ],
   "source": [
    "total_raw = 0\n",
    "total_compressed = 0\n",
    "\n",
    "for index, encoded_article in enumerate(articles.articles_generator(1)):\n",
    "    raw = len(encoded_article) * 8\n",
    "    if raw == 0:\n",
    "        continue\n",
    "    compressed, _ = huffman_archive_size(model, encoded_article)\n",
    "    total_raw += raw\n",
    "    total_compressed += compressed\n",
    "\n",
    "    if index % 100 == 0:\n",
    "        print('Article %d:\\tLength: %d\\tCompression: %f\\tAvg Compression: %f' % (index, raw, compressed/raw, total_compressed/total_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да разгледаме друг вариант за обхождане на статиите в dataset-а.\n",
    "\n",
    "Заради по-малко похабено място при padding, групирането на статии с еднакъв размер заедно води до по-бързи итерации. Което би било добре дошло.\n",
    "\n",
    "Също така прави броят на итерации предвидим. Което поради чисто имплементационни детайли работи по-добре с Keras.\n",
    "\n",
    "Това, обаче, ограничава възможността ни да разбъркваме входните данни. Нека разгледаме какво би било отражението на всичко това върху `loss` функцията и върху размерът на компресираните данни..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Articles:\n",
    "    EMPTY_ARTICLE = np.array([], dtype=TYPE) # used for padding\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        with open(path, 'rb') as text_file:\n",
    "            data = text_file.read()\n",
    "\n",
    "        articles = [np.array(subword_text_encoder.encode(article), dtype=TYPE) for article in data.split(b'\\0')[:10000]]\n",
    "        self.articles = sorted(articles, key=len)\n",
    "\n",
    "    def articles_generator(self, batch_size = 1, start = 0, end = None):\n",
    "        end = end or len(self.articles)\n",
    "\n",
    "        for _ in range(batch_size - ((end - start - 1) % batch_size + 1)):\n",
    "            yield self.EMPTY_ARTICLE\n",
    "\n",
    "        for article in itertools.islice(self.articles, start, end):\n",
    "            yield article\n",
    "\n",
    "    def subbatch_generator(self, batch_size, batch_length, start = 0, end = None):\n",
    "        end = end or len(self.articles)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_generator(self.articles_generator, args=(batch_size, start, end), output_types=TYPE)\n",
    "        dataset = dataset.padded_batch(batch_size, padded_shapes=([None]), drop_remainder=True)\n",
    "        dataset = dataset.shuffle(100)\n",
    "\n",
    "        for batch in dataset.as_numpy_iterator():\n",
    "            remaining = batch\n",
    "            while remaining.shape[1] > 1:\n",
    "                yield remaining[:, :batch_length + 1]\n",
    "                remaining = remaining[:, batch_length:]\n",
    "\n",
    "    def dataset(self, batch_size, batch_length, start = 0, end = None):\n",
    "        end = end or len(self.articles)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_generator(self.subbatch_generator, args=(batch_size, batch_length, start, end), output_types=TYPE, output_shapes=(batch_size, None))\n",
    "        return dataset.map(lambda batch: (batch[:, :-1], batch[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = Articles('page_revisions_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size = subword_text_encoder.vocab_size, embedding_dim=512, rnn_units=1024)\n",
    "model.compile(optimizer='adam', loss=loss, metrics=[average_batch_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "815/815 [==============================] - 765s 939ms/step - loss: 4.9605 - average_batch_length: 245.2761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f58037d1c8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(articles.dataset(BATCH_SIZE, BATCHED_ITEM_LENGTH), callbacks=[checkpoint_callback, model_state_resetter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 100:\tLength: 48\tCompression: 1.229167\tAvg Compression: 1.286173\n",
      "Article 200:\tLength: 56\tCompression: 1.250000\tAvg Compression: 1.264769\n",
      "Article 300:\tLength: 56\tCompression: 1.303571\tAvg Compression: 1.263903\n",
      "Article 400:\tLength: 64\tCompression: 1.265625\tAvg Compression: 1.254929\n",
      "Article 500:\tLength: 64\tCompression: 1.390625\tAvg Compression: 1.254209\n",
      "Article 600:\tLength: 64\tCompression: 1.250000\tAvg Compression: 1.252549\n",
      "Article 700:\tLength: 64\tCompression: 0.937500\tAvg Compression: 1.242738\n",
      "Article 800:\tLength: 72\tCompression: 1.138889\tAvg Compression: 1.232653\n",
      "Article 900:\tLength: 72\tCompression: 1.208333\tAvg Compression: 1.232040\n",
      "Article 1000:\tLength: 72\tCompression: 1.194444\tAvg Compression: 1.233190\n",
      "Article 1100:\tLength: 72\tCompression: 1.402778\tAvg Compression: 1.234418\n",
      "Article 1200:\tLength: 80\tCompression: 1.137500\tAvg Compression: 1.232741\n",
      "Article 1300:\tLength: 80\tCompression: 1.075000\tAvg Compression: 1.232191\n",
      "Article 1400:\tLength: 80\tCompression: 1.125000\tAvg Compression: 1.230236\n",
      "Article 1500:\tLength: 80\tCompression: 1.125000\tAvg Compression: 1.230809\n",
      "Article 1600:\tLength: 80\tCompression: 1.175000\tAvg Compression: 1.233082\n",
      "Article 1700:\tLength: 88\tCompression: 1.125000\tAvg Compression: 1.229259\n",
      "Article 1800:\tLength: 88\tCompression: 1.272727\tAvg Compression: 1.227179\n",
      "Article 1900:\tLength: 88\tCompression: 1.238636\tAvg Compression: 1.223007\n",
      "Article 2000:\tLength: 88\tCompression: 1.375000\tAvg Compression: 1.223912\n",
      "Article 2100:\tLength: 96\tCompression: 1.395833\tAvg Compression: 1.223427\n",
      "Article 2200:\tLength: 96\tCompression: 1.166667\tAvg Compression: 1.223612\n",
      "Article 2300:\tLength: 104\tCompression: 1.125000\tAvg Compression: 1.223272\n",
      "Article 2400:\tLength: 104\tCompression: 1.230769\tAvg Compression: 1.223272\n",
      "Article 2500:\tLength: 112\tCompression: 1.178571\tAvg Compression: 1.222682\n",
      "Article 2600:\tLength: 120\tCompression: 1.150000\tAvg Compression: 1.223195\n",
      "Article 2700:\tLength: 128\tCompression: 1.132812\tAvg Compression: 1.223081\n",
      "Article 2800:\tLength: 136\tCompression: 1.198529\tAvg Compression: 1.222784\n",
      "Article 2900:\tLength: 176\tCompression: 1.045455\tAvg Compression: 1.220311\n",
      "Article 3000:\tLength: 696\tCompression: 1.156609\tAvg Compression: 1.205933\n",
      "Article 3100:\tLength: 1200\tCompression: 1.053333\tAvg Compression: 1.179100\n",
      "Article 3200:\tLength: 1456\tCompression: 1.098901\tAvg Compression: 1.166661\n",
      "Article 3300:\tLength: 1768\tCompression: 1.167986\tAvg Compression: 1.152454\n",
      "Article 3400:\tLength: 2136\tCompression: 1.139981\tAvg Compression: 1.142065\n",
      "Article 3500:\tLength: 2360\tCompression: 1.044915\tAvg Compression: 1.133852\n",
      "Article 3600:\tLength: 2688\tCompression: 1.145461\tAvg Compression: 1.128542\n",
      "Article 3700:\tLength: 2952\tCompression: 1.069106\tAvg Compression: 1.125058\n",
      "Article 3800:\tLength: 3272\tCompression: 1.057152\tAvg Compression: 1.123648\n",
      "Article 3900:\tLength: 3640\tCompression: 1.112637\tAvg Compression: 1.123963\n",
      "Article 4000:\tLength: 4056\tCompression: 1.129684\tAvg Compression: 1.124604\n",
      "Article 4100:\tLength: 4392\tCompression: 1.141849\tAvg Compression: 1.124591\n",
      "Article 4200:\tLength: 4872\tCompression: 1.135468\tAvg Compression: 1.124628\n",
      "Article 4300:\tLength: 5296\tCompression: 1.138784\tAvg Compression: 1.125716\n",
      "Article 4400:\tLength: 5664\tCompression: 1.141066\tAvg Compression: 1.126723\n",
      "Article 4500:\tLength: 6096\tCompression: 1.147146\tAvg Compression: 1.127675\n",
      "Article 4600:\tLength: 6464\tCompression: 1.118038\tAvg Compression: 1.128492\n",
      "Article 4700:\tLength: 6856\tCompression: 1.096995\tAvg Compression: 1.129670\n",
      "Article 4800:\tLength: 7328\tCompression: 1.059907\tAvg Compression: 1.130309\n",
      "Article 4900:\tLength: 7752\tCompression: 1.112745\tAvg Compression: 1.131200\n",
      "Article 5000:\tLength: 8128\tCompression: 1.117249\tAvg Compression: 1.132535\n",
      "Article 5100:\tLength: 8688\tCompression: 1.081607\tAvg Compression: 1.133360\n",
      "Article 5200:\tLength: 9248\tCompression: 1.172362\tAvg Compression: 1.134351\n",
      "Article 5300:\tLength: 9752\tCompression: 1.165197\tAvg Compression: 1.135339\n",
      "Article 5400:\tLength: 10272\tCompression: 1.171826\tAvg Compression: 1.136272\n",
      "Article 5500:\tLength: 10760\tCompression: 1.160409\tAvg Compression: 1.137056\n",
      "Article 5600:\tLength: 11352\tCompression: 1.171159\tAvg Compression: 1.137795\n",
      "Article 5700:\tLength: 11928\tCompression: 1.145875\tAvg Compression: 1.139110\n",
      "Article 5800:\tLength: 12440\tCompression: 1.136656\tAvg Compression: 1.139496\n",
      "Article 5900:\tLength: 13152\tCompression: 1.178908\tAvg Compression: 1.140259\n",
      "Article 6000:\tLength: 13720\tCompression: 1.138484\tAvg Compression: 1.140849\n",
      "Article 6100:\tLength: 14496\tCompression: 1.176531\tAvg Compression: 1.141340\n",
      "Article 6200:\tLength: 15176\tCompression: 1.179626\tAvg Compression: 1.141821\n",
      "Article 6300:\tLength: 15792\tCompression: 1.144187\tAvg Compression: 1.142516\n",
      "Article 6400:\tLength: 16600\tCompression: 1.190482\tAvg Compression: 1.142843\n",
      "Article 6500:\tLength: 17512\tCompression: 1.198949\tAvg Compression: 1.143199\n",
      "Article 6600:\tLength: 18576\tCompression: 1.151001\tAvg Compression: 1.144058\n",
      "Article 6700:\tLength: 19608\tCompression: 1.159578\tAvg Compression: 1.144562\n",
      "Article 6800:\tLength: 20472\tCompression: 1.106633\tAvg Compression: 1.144943\n",
      "Article 6900:\tLength: 21312\tCompression: 1.129176\tAvg Compression: 1.145649\n",
      "Article 7000:\tLength: 22360\tCompression: 1.161494\tAvg Compression: 1.146276\n",
      "Article 7100:\tLength: 23448\tCompression: 1.138988\tAvg Compression: 1.147181\n",
      "Article 7200:\tLength: 24448\tCompression: 1.159440\tAvg Compression: 1.147925\n",
      "Article 7300:\tLength: 25696\tCompression: 1.171311\tAvg Compression: 1.148526\n",
      "Article 7400:\tLength: 26680\tCompression: 1.140292\tAvg Compression: 1.148710\n",
      "Article 7500:\tLength: 28000\tCompression: 1.106429\tAvg Compression: 1.149129\n",
      "Article 7600:\tLength: 29592\tCompression: 1.172479\tAvg Compression: 1.149649\n"
     ]
    }
   ],
   "source": [
    "total_raw = 0\n",
    "total_compressed = 0\n",
    "\n",
    "for index, encoded_article in enumerate(articles.articles_generator(1)):\n",
    "    raw = len(encoded_article) * 8\n",
    "    if raw == 0:\n",
    "        continue\n",
    "    compressed, _ = huffman_archive_size(model, encoded_article)\n",
    "    total_raw += raw\n",
    "    total_compressed += compressed\n",
    "\n",
    "    if index % 100 == 0:\n",
    "        print('Article %d:\\tLength: %d\\tCompression: %f\\tAvg Compression: %f' % (index, raw, compressed/raw, total_compressed/total_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 99:\tLength: 31016\tCompression: 1.159724\tAvg Compression: 1.160828\n",
      "Article 199:\tLength: 32480\tCompression: 1.164070\tAvg Compression: 1.157721\n",
      "Article 299:\tLength: 34064\tCompression: 1.218853\tAvg Compression: 1.157919\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Article 399:\tLength: 35456\tCompression: 1.174272\tAvg Compression: 1.158292\n",
      "Article 499:\tLength: 37096\tCompression: 1.180370\tAvg Compression: 1.159346\n",
      "Article 599:\tLength: 38712\tCompression: 1.095268\tAvg Compression: 1.159092\n",
      "Article 699:\tLength: 40576\tCompression: 1.146712\tAvg Compression: 1.158533\n",
      "Article 799:\tLength: 42376\tCompression: 1.173424\tAvg Compression: 1.158731\n",
      "Article 899:\tLength: 44472\tCompression: 1.187466\tAvg Compression: 1.158734\n",
      "Article 999:\tLength: 46896\tCompression: 1.138263\tAvg Compression: 1.158351\n",
      "Article 1099:\tLength: 49352\tCompression: 1.139549\tAvg Compression: 1.159018\n",
      "Article 1199:\tLength: 51952\tCompression: 1.154239\tAvg Compression: 1.159643\n",
      "Article 1299:\tLength: 55392\tCompression: 1.160041\tAvg Compression: 1.160231\n",
      "Article 1399:\tLength: 58712\tCompression: 1.152712\tAvg Compression: 1.161304\n",
      "Article 1499:\tLength: 62608\tCompression: 1.164500\tAvg Compression: 1.161700\n",
      "Article 1599:\tLength: 67248\tCompression: 1.158399\tAvg Compression: 1.162794\n",
      "Article 1699:\tLength: 72680\tCompression: 1.187356\tAvg Compression: 1.163388\n",
      "Article 1799:\tLength: 78872\tCompression: 1.183056\tAvg Compression: 1.163776\n",
      "Article 1899:\tLength: 86288\tCompression: 1.165712\tAvg Compression: 1.164276\n",
      "Article 1999:\tLength: 95784\tCompression: 1.145724\tAvg Compression: 1.164481\n",
      "Article 2099:\tLength: 107688\tCompression: 1.209290\tAvg Compression: 1.165309\n",
      "Article 2199:\tLength: 121784\tCompression: 1.151416\tAvg Compression: 1.166364\n",
      "Article 2299:\tLength: 147040\tCompression: 1.181536\tAvg Compression: 1.166791\n"
     ]
    }
   ],
   "source": [
    "total_raw = 0\n",
    "total_compressed = 0\n",
    "\n",
    "for index, encoded_article in enumerate(itertools.islice(articles.articles_generator(1), 7601, 10000)):\n",
    "    raw = len(encoded_article) * 8\n",
    "    if raw == 0:\n",
    "        continue\n",
    "    compressed, _ = huffman_archive_size(model, encoded_article)\n",
    "    total_raw += raw\n",
    "    total_compressed += compressed\n",
    "\n",
    "    if (7601 + index) % 100 == 0:\n",
    "        print('Article %d:\\tLength: %d\\tCompression: %f\\tAvg Compression: %f' % (index, raw, compressed/raw, total_compressed/total_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 2398:\tLength: 606424\tCompression: 1.209142\tAvg Compression: 1.168509\n"
     ]
    }
   ],
   "source": [
    "print('Article %d:\\tLength: %d\\tCompression: %f\\tAvg Compression: %f' % (index, raw, compressed/raw, total_compressed/total_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE=np.int16\n",
    "\n",
    "subword_text_encoder = tfds.features.text.SubwordTextEncoder.load_from_file('vocab_4096')\n",
    "\n",
    "class Articles:\n",
    "    EMPTY_ARTICLE = np.array([], dtype=TYPE) # used for padding\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        with open(path, 'rb') as text_file:\n",
    "            data = text_file.read()\n",
    "\n",
    "        articles = set(data.split(b'\\0')[:10000])\n",
    "        articles = [np.array(subword_text_encoder.encode(article), dtype=TYPE) for article in articles]\n",
    "        self.articles = sorted(articles, key=len)\n",
    "\n",
    "    def articles_generator(self, batch_size = 1, start = 0, end = None):\n",
    "        end = end or len(self.articles)\n",
    "\n",
    "        for _ in range(batch_size - ((end - start - 1) % batch_size + 1)):\n",
    "            yield self.EMPTY_ARTICLE\n",
    "\n",
    "        for article in itertools.islice(self.articles, start, end):\n",
    "            yield article\n",
    "\n",
    "    def subbatch_generator(self, batch_size, batch_length, start = 0, end = None):\n",
    "        end = end or len(self.articles)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_generator(self.articles_generator, args=(batch_size, start, end), output_types=TYPE)\n",
    "        dataset = dataset.padded_batch(batch_size, padded_shapes=([None]), drop_remainder=True)\n",
    "        dataset = dataset.shuffle(100)\n",
    "\n",
    "        for batch in dataset.as_numpy_iterator():\n",
    "            remaining = batch\n",
    "            while remaining.shape[1] > 1:\n",
    "                yield remaining[:, :batch_length + 1]\n",
    "                remaining = remaining[:, batch_length:]\n",
    "\n",
    "    def dataset(self, batch_size, batch_length, start = 0, end = None):\n",
    "        end = end or len(self.articles)\n",
    "\n",
    "        dataset = tf.data.Dataset.from_generator(self.subbatch_generator, args=(batch_size, batch_length, start, end), output_types=TYPE, output_shapes=(batch_size, None))\n",
    "        return dataset.map(lambda batch: (batch[:, :-1], batch[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = Articles('page_revisions_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 192\n",
    "BATCHED_ITEM_LENGTH = 256\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Masking(mask_value=0, batch_input_shape=[BATCH_SIZE, None]),\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "        # tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size),\n",
    "    ])\n",
    "\n",
    "checkpoint_dir = './training_checkpoints-3' # Directory where the checkpoints will be saved\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\") # Name of the checkpoint files\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
    "\n",
    "class ModelStateResetter(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.last_total_length = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        average_batch_length = logs.get('average_batch_length', 0)\n",
    "        total_length = int(round(average_batch_length * (batch + 1)))\n",
    "        current_batch_length = total_length - self.last_total_length\n",
    "        self.last_total_length = total_length\n",
    "        \n",
    "        if current_batch_length < BATCHED_ITEM_LENGTH:\n",
    "            self.model.reset_states()\n",
    "        \n",
    "model_state_resetter_callback = ModelStateResetter()\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "def average_batch_length(true_labels, predictions):\n",
    "    return tf.shape(true_labels)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size = subword_text_encoder.vocab_size, embedding_dim=512, rnn_units=1024)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.compile(optimizer='adam', loss=loss, metrics=[average_batch_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "    730/Unknown - 689s 944ms/step - loss: 3.6365 - average_batch_length: 246.9233"
     ]
    }
   ],
   "source": [
    "dataset = articles.dataset(BATCH_SIZE, BATCHED_ITEM_LENGTH)\n",
    "model.fit(dataset, epochs=6, callbacks=[checkpoint_callback, model_state_resetter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "812/812 [==============================] - 774s 954ms/step - loss: 3.7543 - average_batch_length: 246.1207\n",
      "Epoch 2/5\n",
      "812/812 [==============================] - 788s 970ms/step - loss: 3.2118 - average_batch_length: 246.1207\n",
      "Epoch 3/5\n",
      "812/812 [==============================] - 767s 945ms/step - loss: 2.8895 - average_batch_length: 246.1207\n",
      "Epoch 4/5\n",
      "812/812 [==============================] - 768s 945ms/step - loss: 2.7013 - average_batch_length: 246.1207\n",
      "Epoch 5/5\n",
      " 75/812 [=>............................] - ETA: 11:21 - loss: 3.5320 - average_batch_length: 237.5200"
     ]
    }
   ],
   "source": [
    "dataset = articles.dataset(BATCH_SIZE, BATCHED_ITEM_LENGTH)\n",
    "model.fit(dataset, epochs=5, callbacks=[checkpoint_callback, model_state_resetter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    121/Unknown - 116s 957ms/step - loss: 3.5313 - average_batch_length: 242.2562"
     ]
    }
   ],
   "source": [
    "dataset = articles.dataset(BATCH_SIZE, BATCHED_ITEM_LENGTH)\n",
    "model.fit(dataset, callbacks=[checkpoint_callback, model_state_resetter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 100:\tLength: 56\tCompression: 1.178571\tAvg Compression: 1.145208\n",
      "Article 200:\tLength: 56\tCompression: 1.178571\tAvg Compression: 1.114327\n",
      "Article 300:\tLength: 64\tCompression: 1.031250\tAvg Compression: 1.094505\n",
      "Article 400:\tLength: 64\tCompression: 1.015625\tAvg Compression: 1.071536\n",
      "Article 500:\tLength: 64\tCompression: 0.937500\tAvg Compression: 1.055387\n",
      "Article 600:\tLength: 64\tCompression: 0.937500\tAvg Compression: 1.047612\n",
      "Article 700:\tLength: 72\tCompression: 0.916667\tAvg Compression: 1.038866\n",
      "Article 800:\tLength: 72\tCompression: 0.944444\tAvg Compression: 1.033067\n",
      "Article 900:\tLength: 72\tCompression: 1.138889\tAvg Compression: 1.026084\n",
      "Article 1000:\tLength: 80\tCompression: 0.950000\tAvg Compression: 1.021565\n",
      "Article 1100:\tLength: 80\tCompression: 1.025000\tAvg Compression: 1.014067\n",
      "Article 1200:\tLength: 80\tCompression: 1.125000\tAvg Compression: 1.007953\n",
      "Article 1300:\tLength: 80\tCompression: 0.925000\tAvg Compression: 1.001440\n",
      "Article 1400:\tLength: 88\tCompression: 0.829545\tAvg Compression: 0.992802\n",
      "Article 1500:\tLength: 88\tCompression: 0.750000\tAvg Compression: 0.984432\n",
      "Article 1600:\tLength: 88\tCompression: 0.931818\tAvg Compression: 0.977547\n",
      "Article 1700:\tLength: 88\tCompression: 0.750000\tAvg Compression: 0.970883\n",
      "Article 1800:\tLength: 96\tCompression: 1.041667\tAvg Compression: 0.966033\n",
      "Article 1900:\tLength: 96\tCompression: 0.812500\tAvg Compression: 0.960933\n",
      "Article 2000:\tLength: 104\tCompression: 0.750000\tAvg Compression: 0.954787\n",
      "Article 2100:\tLength: 112\tCompression: 0.857143\tAvg Compression: 0.950677\n",
      "Article 2200:\tLength: 120\tCompression: 0.916667\tAvg Compression: 0.945892\n",
      "Article 2300:\tLength: 128\tCompression: 0.867188\tAvg Compression: 0.943206\n",
      "Article 2400:\tLength: 160\tCompression: 0.887500\tAvg Compression: 0.938343\n",
      "Article 2500:\tLength: 488\tCompression: 0.750000\tAvg Compression: 0.919154\n",
      "Article 2600:\tLength: 1000\tCompression: 0.671000\tAvg Compression: 0.864738\n",
      "Article 2700:\tLength: 1456\tCompression: 0.699863\tAvg Compression: 0.821001\n",
      "Article 2800:\tLength: 1760\tCompression: 0.700000\tAvg Compression: 0.786288\n",
      "Article 2900:\tLength: 2136\tCompression: 0.685861\tAvg Compression: 0.767525\n",
      "Article 3000:\tLength: 2352\tCompression: 0.716837\tAvg Compression: 0.753540\n",
      "Article 3100:\tLength: 2672\tCompression: 0.574850\tAvg Compression: 0.739974\n",
      "Article 3200:\tLength: 2952\tCompression: 0.682927\tAvg Compression: 0.731736\n",
      "Article 3300:\tLength: 3264\tCompression: 0.735294\tAvg Compression: 0.726719\n",
      "Article 3400:\tLength: 3640\tCompression: 0.689560\tAvg Compression: 0.723033\n",
      "Article 3500:\tLength: 4048\tCompression: 0.650692\tAvg Compression: 0.721941\n",
      "Article 3600:\tLength: 4384\tCompression: 0.757755\tAvg Compression: 0.720951\n",
      "Article 3700:\tLength: 4872\tCompression: 0.579844\tAvg Compression: 0.719650\n",
      "Article 3800:\tLength: 5296\tCompression: 0.649169\tAvg Compression: 0.719015\n",
      "Article 3900:\tLength: 5664\tCompression: 0.795904\tAvg Compression: 0.717494\n",
      "Article 4000:\tLength: 6096\tCompression: 0.760499\tAvg Compression: 0.716943\n",
      "Article 4100:\tLength: 6456\tCompression: 0.682156\tAvg Compression: 0.717420\n",
      "Article 4200:\tLength: 6840\tCompression: 0.709649\tAvg Compression: 0.717358\n"
     ]
    }
   ],
   "source": [
    "total_raw = 0\n",
    "total_compressed = 0\n",
    "\n",
    "for index, encoded_article in enumerate(articles.articles_generator(1)):\n",
    "    raw = len(encoded_article) * 8\n",
    "    if raw == 0:\n",
    "        continue\n",
    "    compressed, _ = huffman_archive_size(model, encoded_article)\n",
    "    total_raw += raw\n",
    "    total_compressed += compressed\n",
    "\n",
    "    if index % 100 == 0:\n",
    "        print('Article %d:\\tLength: %d\\tCompression: %f\\tAvg Compression: %f' % (index, raw, compressed/raw, total_compressed/total_raw))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
