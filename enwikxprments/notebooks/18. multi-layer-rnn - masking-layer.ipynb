{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def articles():\n",
    "    with open('page_revisions_text', 'rb') as text_file:\n",
    "        pending_article_data = b''\n",
    "        while True:\n",
    "            data = text_file.read(1024 * 1024)\n",
    "            if len(data) == 0:\n",
    "                break\n",
    "\n",
    "            articles = data.split(b'\\0')\n",
    "            articles[0] = pending_article_data + articles[0]\n",
    "            for index, article in enumerate(articles):\n",
    "                if index + 1 == len(articles):\n",
    "                    pending_article_data = article\n",
    "                else:\n",
    "                    yield article\n",
    "\n",
    "        print(pending_article_data)\n",
    "        if len(pending_article_data) != 0:\n",
    "            yield pending_article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword_text_encoder = tfds.features.text.SubwordTextEncoder.load_from_file('vocab_4096')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да видим как би изглеждало обучение с кодираните статии..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((128, None), (128, None)), types: (tf.int16, tf.int16)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "BATCHED_ITEM_LENGTH = 256\n",
    "BUFFER_SIZE = 256\n",
    "TYPE=np.int16\n",
    "\n",
    "def articles_generator():\n",
    "    for index, article in enumerate(itertools.islice(articles(), 0, 2000)):\n",
    "        yield np.array(subword_text_encoder.encode(article + b'\\0'), dtype=TYPE)\n",
    "\n",
    "    # Pad the article count to the batch size\n",
    "    # We do this to ensure that no data is dropped\n",
    "    index += 1\n",
    "    while index % BATCH_SIZE != 0:\n",
    "        yield np.array([0], dtype=TYPE)\n",
    "        index += 1\n",
    "\n",
    "def subbatches():\n",
    "    dataset = tf.data.Dataset.from_generator(articles_generator, output_types=TYPE)\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "    dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=([None]), drop_remainder=True)\n",
    "\n",
    "    for batch in dataset.as_numpy_iterator():\n",
    "        remaining = batch\n",
    "        while remaining.shape[1] > 1:\n",
    "            yield remaining[:, :BATCHED_ITEM_LENGTH + 1]\n",
    "            remaining = remaining[:, BATCHED_ITEM_LENGTH:]\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(subbatches, output_types=TYPE, output_shapes=(BATCH_SIZE, None))\n",
    "dataset = dataset.map(lambda batch: (batch[:, :-1], batch[:, 1:]))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Masking(mask_value=0, batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "def average_batch_length(true_labels, predictions):\n",
    "    return tf.shape(true_labels)[1]\n",
    "\n",
    "model = build_model(vocab_size = subword_text_encoder.vocab_size, embedding_dim=512, rnn_units=1024, batch_size=BATCH_SIZE)\n",
    "model.compile(optimizer='adam', loss=loss, metrics=[average_batch_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints' # Directory where the checkpoints will be saved\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\") # Name of the checkpoint files\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelStateResetter(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.last_total_length = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        average_batch_length = logs.get('average_batch_length', 0)\n",
    "        total_length = int(round(average_batch_length * (batch + 1)))\n",
    "        current_batch_length = total_length - self.last_total_length\n",
    "        self.last_total_length = total_length\n",
    "        \n",
    "        if current_batch_length < BATCHED_ITEM_LENGTH:\n",
    "            self.model.reset_states()\n",
    "        \n",
    "model_state_resetter_callback = ModelStateResetter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1286/1286 [==============================] - 851s 662ms/step - loss: 0.7893 - average_batch_length: 254.4277\n",
      "Epoch 2/10\n",
      "1371/1371 [==============================] - 904s 659ms/step - loss: 0.5686 - average_batch_length: 254.4048\n",
      "Epoch 3/10\n",
      "1286/1286 [==============================] - 873s 678ms/step - loss: 0.5481 - average_batch_length: 254.2162\n",
      "Epoch 4/10\n",
      "1302/1302 [==============================] - 893s 686ms/step - loss: 0.4961 - average_batch_length: 254.5200\n",
      "Epoch 5/10\n",
      "1317/1317 [==============================] - 900s 683ms/step - loss: 0.4583 - average_batch_length: 254.6014\n",
      "Epoch 6/10\n",
      "1364/1364 [==============================] - 924s 678ms/step - loss: 0.4205 - average_batch_length: 254.8145\n",
      "Epoch 7/10\n",
      "1362/1362 [==============================] - 921s 676ms/step - loss: 0.4033 - average_batch_length: 254.4435\n",
      "Epoch 8/10\n",
      "1279/1279 [==============================] - 883s 690ms/step - loss: 0.4139 - average_batch_length: 254.5981\n",
      "Epoch 9/10\n",
      "1302/1302 [==============================] - 884s 679ms/step - loss: 0.3934 - average_batch_length: 254.5046\n",
      "Epoch 10/10\n",
      "1275/1275 [==============================] - 851s 668ms/step - loss: 0.3903 - average_batch_length: 254.4196\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 10\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    print('Epoch %d/%d' % (epoch + 1, total_epochs))\n",
    "    model.fit(dataset, callbacks=[checkpoint_callback, model_state_resetter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1295/1295 [==============================] - 855s 660ms/step - loss: 0.3739 - average_batch_length: 254.3290\n",
      "Epoch 2/7\n",
      "1313/1313 [==============================] - 869s 662ms/step - loss: 0.3597 - average_batch_length: 254.4128\n",
      "Epoch 3/7\n",
      "1342/1342 [==============================] - 895s 667ms/step - loss: 0.3436 - average_batch_length: 254.6602\n",
      "Epoch 4/7\n",
      "1252/1252 [==============================] - 833s 665ms/step - loss: 0.3599 - average_batch_length: 254.6318\n",
      "Epoch 5/7\n",
      "1267/1267 [==============================] - 844s 666ms/step - loss: 0.3476 - average_batch_length: 254.5738\n",
      "Epoch 6/7\n",
      "1307/1307 [==============================] - 871s 666ms/step - loss: 0.3299 - average_batch_length: 254.6679\n",
      "Epoch 7/7\n",
      "1313/1313 [==============================] - 872s 664ms/step - loss: 0.3215 - average_batch_length: 254.3602\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 7\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    print('Epoch %d/%d' % (epoch + 1, total_epochs))\n",
    "    model.fit(dataset, callbacks=[checkpoint_callback, model_state_resetter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: 25541\n",
      "Encoded: 8222\n"
     ]
    }
   ],
   "source": [
    "with open('page_revisions_text', 'rb') as text_file:\n",
    "    data = text_file.read()\n",
    "\n",
    "article = data.split(b'\\0')[120]\n",
    "del data\n",
    "\n",
    "encoded_article = np.array(subword_text_encoder.encode(article + b'\\0'), dtype=TYPE)\n",
    "\n",
    "print('Raw:', len(article))\n",
    "print('Encoded:', len(encoded_article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "\n",
    "class Huffman:\n",
    "    huffman = ctypes.CDLL('x64/Release/huffman')\n",
    "    \n",
    "    huffman.create_tree.restype = ctypes.c_void_p\n",
    "    huffman.destroy_tree.restype = None\n",
    "    huffman.load_weights.restype = None\n",
    "    huffman.create_code_string.restype = ctypes.c_char_p\n",
    "    \n",
    "    def __init__(self, category_count):\n",
    "        self.tree = ctypes.c_void_p(self.huffman.create_tree(category_count))\n",
    "\n",
    "    def __del__(self):\n",
    "        self.huffman.destroy_tree(self.tree)\n",
    "        \n",
    "    def load_weights(self, weights):\n",
    "        self.huffman.load_weights(self.tree, weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float)))\n",
    "    \n",
    "    def get_code_length(self, category):\n",
    "        return self.huffman.get_code_length(self.tree, category)\n",
    "\n",
    "    def get_code_zero_count(self, category):\n",
    "        return self.huffman.get_code_zero_count(self.tree, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_archive_size(model, text):\n",
    "    archived_size = 0\n",
    "    zeros = 0\n",
    "    input_eval = np.array([[0]], dtype=TYPE)\n",
    "    huffman_tree = Huffman(subword_text_encoder.vocab_size)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    model.reset_states()\n",
    "\n",
    "    for index, byte in enumerate(text):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0) # remove the batch dimension\n",
    "\n",
    "        weights = tf.nn.softmax(predictions[0]).numpy()\n",
    "        huffman_tree.load_weights(weights)\n",
    "        zeros += huffman_tree.get_code_zero_count(byte.item())\n",
    "        archived_size += huffman_tree.get_code_length(byte.item())\n",
    "\n",
    "        input_eval = tf.expand_dims([byte], 0)\n",
    "  \n",
    "    return archived_size, zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocab_size = subword_text_encoder.vocab_size, embedding_dim=512, rnn_units=1024, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0:\tCompression: 0.430556\tAvg Compression: 0.430556\n",
      "Article 1:\tCompression: 0.255952\tAvg Compression: 0.308333\n",
      "Article 2:\tCompression: 0.216837\tAvg Compression: 0.267202\n",
      "Article 3:\tCompression: 0.291667\tAvg Compression: 0.272482\n",
      "Article 4:\tCompression: 0.314286\tAvg Compression: 0.280891\n",
      "Article 5:\tCompression: 0.185897\tAvg Compression: 0.263498\n",
      "Article 6:\tCompression: 0.193331\tAvg Compression: 0.193585\n",
      "Article 7:\tCompression: 0.192568\tAvg Compression: 0.193585\n",
      "Article 8:\tCompression: 0.200000\tAvg Compression: 0.193589\n",
      "Article 9:\tCompression: 0.181548\tAvg Compression: 0.193580\n",
      "Article 10:\tCompression: 0.220395\tAvg Compression: 0.193598\n",
      "Article 11:\tCompression: 0.170455\tAvg Compression: 0.193580\n",
      "Article 12:\tCompression: 0.176136\tAvg Compression: 0.193567\n",
      "Article 13:\tCompression: 0.203947\tAvg Compression: 0.193574\n",
      "Article 14:\tCompression: 0.178191\tAvg Compression: 0.193562\n",
      "Article 15:\tCompression: 0.317708\tAvg Compression: 0.193612\n",
      "Article 16:\tCompression: 0.282143\tAvg Compression: 0.193665\n",
      "Article 17:\tCompression: 0.359375\tAvg Compression: 0.193732\n",
      "Article 18:\tCompression: 0.186571\tAvg Compression: 0.190480\n",
      "Article 19:\tCompression: 0.227273\tAvg Compression: 0.190491\n",
      "Article 20:\tCompression: 0.214286\tAvg Compression: 0.190499\n",
      "Article 21:\tCompression: 0.203947\tAvg Compression: 0.190504\n",
      "Article 22:\tCompression: 0.416667\tAvg Compression: 0.190566\n",
      "Article 23:\tCompression: 0.274038\tAvg Compression: 0.190586\n",
      "Article 24:\tCompression: 0.235294\tAvg Compression: 0.190600\n",
      "Article 25:\tCompression: 0.242424\tAvg Compression: 0.190616\n",
      "Article 26:\tCompression: 0.336957\tAvg Compression: 0.190647\n",
      "Article 27:\tCompression: 0.208559\tAvg Compression: 0.191999\n",
      "Article 28:\tCompression: 0.199324\tAvg Compression: 0.192001\n",
      "Article 29:\tCompression: 0.245690\tAvg Compression: 0.192028\n",
      "Article 30:\tCompression: 0.201374\tAvg Compression: 0.192795\n",
      "Article 31:\tCompression: 0.322917\tAvg Compression: 0.192819\n",
      "Article 32:\tCompression: 0.346591\tAvg Compression: 0.192845\n",
      "Article 33:\tCompression: 0.304348\tAvg Compression: 0.192865\n",
      "Article 34:\tCompression: 0.380682\tAvg Compression: 0.192897\n",
      "Article 35:\tCompression: 0.363636\tAvg Compression: 0.192927\n",
      "Article 36:\tCompression: 0.291667\tAvg Compression: 0.192945\n",
      "Article 37:\tCompression: 0.363095\tAvg Compression: 0.192973\n",
      "Article 38:\tCompression: 0.351190\tAvg Compression: 0.192999\n",
      "Article 39:\tCompression: 0.272321\tAvg Compression: 0.193016\n",
      "Article 40:\tCompression: 0.279167\tAvg Compression: 0.193036\n",
      "Article 41:\tCompression: 0.322917\tAvg Compression: 0.193061\n",
      "Article 42:\tCompression: 0.254902\tAvg Compression: 0.193085\n",
      "Article 43:\tCompression: 0.197674\tAvg Compression: 0.193087\n",
      "Article 44:\tCompression: 0.178571\tAvg Compression: 0.193082\n",
      "Article 45:\tCompression: 0.178571\tAvg Compression: 0.193077\n",
      "Article 46:\tCompression: 0.217949\tAvg Compression: 0.193085\n",
      "Article 47:\tCompression: 0.201923\tAvg Compression: 0.193088\n",
      "Article 48:\tCompression: 0.225806\tAvg Compression: 0.193095\n",
      "Article 49:\tCompression: 0.276786\tAvg Compression: 0.193114\n",
      "Article 50:\tCompression: 0.320652\tAvg Compression: 0.193136\n",
      "Article 51:\tCompression: 0.303030\tAvg Compression: 0.193165\n",
      "Article 52:\tCompression: 0.333333\tAvg Compression: 0.193194\n",
      "Article 53:\tCompression: 0.361607\tAvg Compression: 0.193231\n",
      "Article 54:\tCompression: 0.285000\tAvg Compression: 0.193248\n",
      "Article 55:\tCompression: 0.246622\tAvg Compression: 0.193264\n",
      "Article 56:\tCompression: 0.328125\tAvg Compression: 0.193289\n",
      "Article 57:\tCompression: 0.327381\tAvg Compression: 0.193311\n",
      "Article 58:\tCompression: 0.258065\tAvg Compression: 0.193326\n",
      "Article 59:\tCompression: 0.322917\tAvg Compression: 0.193350\n",
      "Article 60:\tCompression: 0.203571\tAvg Compression: 0.193353\n",
      "Article 61:\tCompression: 0.265152\tAvg Compression: 0.193371\n",
      "Article 62:\tCompression: 0.415179\tAvg Compression: 0.193419\n",
      "Article 63:\tCompression: 0.294355\tAvg Compression: 0.193444\n",
      "Article 64:\tCompression: 0.328125\tAvg Compression: 0.193469\n",
      "Article 65:\tCompression: 0.323864\tAvg Compression: 0.193491\n",
      "Article 66:\tCompression: 0.359375\tAvg Compression: 0.193522\n",
      "Article 67:\tCompression: 0.263393\tAvg Compression: 0.193537\n",
      "Article 68:\tCompression: 0.250000\tAvg Compression: 0.193550\n",
      "Article 69:\tCompression: 0.180556\tAvg Compression: 0.193545\n",
      "Article 70:\tCompression: 0.258333\tAvg Compression: 0.193560\n",
      "Article 71:\tCompression: 0.285156\tAvg Compression: 0.193583\n",
      "Article 72:\tCompression: 0.338710\tAvg Compression: 0.193617\n",
      "Article 73:\tCompression: 0.215909\tAvg Compression: 0.193623\n",
      "Article 74:\tCompression: 0.264535\tAvg Compression: 0.193647\n",
      "Article 75:\tCompression: 0.262097\tAvg Compression: 0.193663\n",
      "Article 76:\tCompression: 0.331522\tAvg Compression: 0.193688\n",
      "Article 77:\tCompression: 0.228571\tAvg Compression: 0.193697\n",
      "Article 78:\tCompression: 0.190789\tAvg Compression: 0.193696\n",
      "Article 79:\tCompression: 0.216912\tAvg Compression: 0.193702\n",
      "Article 80:\tCompression: 0.227273\tAvg Compression: 0.193711\n",
      "Article 81:\tCompression: 0.227564\tAvg Compression: 0.193721\n",
      "Article 82:\tCompression: 0.318750\tAvg Compression: 0.193740\n",
      "Article 83:\tCompression: 0.220238\tAvg Compression: 0.193749\n",
      "Article 84:\tCompression: 0.207166\tAvg Compression: 0.194796\n",
      "Article 85:\tCompression: 0.272059\tAvg Compression: 0.194815\n",
      "Article 86:\tCompression: 0.250000\tAvg Compression: 0.194828\n",
      "Article 87:\tCompression: 0.220238\tAvg Compression: 0.194835\n",
      "Article 88:\tCompression: 0.301724\tAvg Compression: 0.194857\n",
      "Article 89:\tCompression: 0.218750\tAvg Compression: 0.194864\n",
      "Article 90:\tCompression: 0.336207\tAvg Compression: 0.194893\n",
      "Article 91:\tCompression: 0.291667\tAvg Compression: 0.194911\n",
      "Article 92:\tCompression: 0.160467\tAvg Compression: 0.190513\n",
      "Article 93:\tCompression: 0.312500\tAvg Compression: 0.190530\n",
      "Article 94:\tCompression: 0.216197\tAvg Compression: 0.193404\n",
      "Article 95:\tCompression: 0.253676\tAvg Compression: 0.193415\n",
      "Article 96:\tCompression: 0.183647\tAvg Compression: 0.190425\n",
      "Article 97:\tCompression: 0.183765\tAvg Compression: 0.189461\n",
      "Article 98:\tCompression: 0.204973\tAvg Compression: 0.189518\n",
      "Article 99:\tCompression: 0.318750\tAvg Compression: 0.189526\n",
      "Article 100:\tCompression: 0.180556\tAvg Compression: 0.189525\n",
      "Article 101:\tCompression: 0.196023\tAvg Compression: 0.189526\n",
      "Article 102:\tCompression: 0.188889\tAvg Compression: 0.189525\n",
      "Article 103:\tCompression: 0.157407\tAvg Compression: 0.189520\n",
      "Article 104:\tCompression: 0.156250\tAvg Compression: 0.189514\n",
      "Article 105:\tCompression: 0.169118\tAvg Compression: 0.189510\n",
      "Article 106:\tCompression: 0.223214\tAvg Compression: 0.189515\n",
      "Article 107:\tCompression: 0.241667\tAvg Compression: 0.189525\n",
      "Article 108:\tCompression: 0.173077\tAvg Compression: 0.189522\n",
      "Article 109:\tCompression: 0.209135\tAvg Compression: 0.189526\n",
      "Article 110:\tCompression: 0.296512\tAvg Compression: 0.189541\n",
      "Article 111:\tCompression: 0.270455\tAvg Compression: 0.189555\n",
      "Article 112:\tCompression: 0.166704\tAvg Compression: 0.188829\n",
      "Article 113:\tCompression: 0.291667\tAvg Compression: 0.188838\n",
      "Article 114:\tCompression: 0.318750\tAvg Compression: 0.188846\n",
      "Article 115:\tCompression: 0.307065\tAvg Compression: 0.188863\n",
      "Article 116:\tCompression: 0.337500\tAvg Compression: 0.188873\n",
      "Article 117:\tCompression: 0.217217\tAvg Compression: 0.188972\n",
      "Article 118:\tCompression: 0.282895\tAvg Compression: 0.188983\n",
      "Article 119:\tCompression: 0.201007\tAvg Compression: 0.189117\n"
     ]
    }
   ],
   "source": [
    "total_raw = 0\n",
    "total_compressed = 0\n",
    "\n",
    "for index, article in enumerate(itertools.islice(articles(), 0, 120)):\n",
    "    raw = (len(article) + 1) * 8\n",
    "    encoded_article = np.array(subword_text_encoder.encode(article + b'\\0'), dtype=TYPE)\n",
    "    compressed, _ = huffman_archive_size(model, encoded_article)\n",
    "    total_raw += raw\n",
    "    total_compressed += compressed\n",
    "    print('Article %d:\\tCompression: %f\\tAvg Compression: %f' % (index, compressed/raw, total_compressed/total_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Това определено изглежда доста по-разумно. При това със стойност за loss около `0.32`. Да видим колко компресия можем да постигнем след още трениране..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size = subword_text_encoder.vocab_size, embedding_dim=512, rnn_units=1024, batch_size=BATCH_SIZE)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.compile(optimizer='adam', loss=loss, metrics=[average_batch_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1356/1356 [==============================] - 906s 668ms/step - loss: 0.3048 - average_batch_length: 254.5737\n",
      "Epoch 2/10\n",
      "1295/1295 [==============================] - 870s 672ms/step - loss: 0.3126 - average_batch_length: 254.5537\n",
      "Epoch 3/10\n",
      "1231/1231 [==============================] - 830s 674ms/step - loss: 0.3221 - average_batch_length: 254.2128\n",
      "Epoch 4/10\n",
      "1317/1317 [==============================] - 889s 675ms/step - loss: 0.2952 - average_batch_length: 254.5368\n",
      "Epoch 5/10\n",
      "1322/1322 [==============================] - 880s 666ms/step - loss: 0.2884 - average_batch_length: 254.5605\n",
      "Epoch 6/10\n",
      "1377/1377 [==============================] - 917s 666ms/step - loss: 0.2717 - average_batch_length: 254.5977\n",
      "Epoch 7/10\n",
      "1282/1282 [==============================] - 864s 674ms/step - loss: 0.2862 - average_batch_length: 254.2707\n",
      "Epoch 8/10\n",
      "1322/1322 [==============================] - 874s 661ms/step - loss: 0.2714 - average_batch_length: 254.4637\n",
      "Epoch 9/10\n",
      "1357/1357 [==============================] - 903s 665ms/step - loss: 0.2597 - average_batch_length: 254.4178\n",
      "Epoch 10/10\n",
      "1332/1332 [==============================] - 882s 662ms/step - loss: 0.2593 - average_batch_length: 254.2883\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 10\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    print('Epoch %d/%d' % (epoch + 1, total_epochs))\n",
    "    model.fit(dataset, callbacks=[checkpoint_callback, model_state_resetter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1349/1349 [==============================] - 899s 666ms/step - loss: 0.2512 - average_batch_length: 254.8688\n",
      "Epoch 2/8\n",
      "1345/1345 [==============================] - 893s 664ms/step - loss: 0.2469 - average_batch_length: 254.5353\n",
      "Epoch 3/8\n",
      "1393/1393 [==============================] - 926s 664ms/step - loss: 0.2336 - average_batch_length: 254.3345\n",
      "Epoch 4/8\n",
      "1396/1396 [==============================] - 928s 665ms/step - loss: 0.2293 - average_batch_length: 254.6390\n",
      "Epoch 5/8\n",
      "1296/1296 [==============================] - 860s 664ms/step - loss: 0.2417 - average_batch_length: 254.5015\n",
      "Epoch 6/8\n",
      "1311/1311 [==============================] - 865s 660ms/step - loss: 0.2340 - average_batch_length: 254.6590\n",
      "Epoch 7/8\n",
      "1327/1327 [==============================] - 875s 659ms/step - loss: 0.2268 - average_batch_length: 254.3821\n",
      "Epoch 8/8\n",
      "1318/1318 [==============================] - 870s 660ms/step - loss: 0.2242 - average_batch_length: 254.4029\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 8\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    print('Epoch %d/%d' % (epoch + 1, total_epochs))\n",
    "    model.fit(dataset, callbacks=[checkpoint_callback, model_state_resetter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 887s 660ms/step - loss: 0.2162 - average_batch_length: 254.6850\n",
      "Epoch 2/5\n",
      "1349/1349 [==============================] - 891s 660ms/step - loss: 0.2119 - average_batch_length: 254.8762\n",
      "Epoch 3/5\n",
      "1292/1292 [==============================] - 853s 660ms/step - loss: 0.2166 - average_batch_length: 254.5263\n",
      "Epoch 4/5\n",
      "1297/1297 [==============================] - 857s 661ms/step - loss: 0.2122 - average_batch_length: 254.7009\n",
      "Epoch 5/5\n",
      "1313/1313 [==============================] - 866s 659ms/step - loss: 0.2058 - average_batch_length: 254.4699\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 5\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    print('Epoch %d/%d' % (epoch + 1, total_epochs))\n",
    "    model.fit(dataset, callbacks=[checkpoint_callback, model_state_resetter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1334/1334 [==============================] - 883s 662ms/step - loss: 0.1990 - average_batch_length: 254.5322\n",
      "Epoch 2/5\n",
      "1355/1355 [==============================] - 901s 665ms/step - loss: 0.1928 - average_batch_length: 254.6635\n",
      "Epoch 3/5\n",
      "1258/1258 [==============================] - 836s 664ms/step - loss: 0.2041 - average_batch_length: 254.5445\n",
      "Epoch 4/5\n",
      "1274/1274 [==============================] - 849s 666ms/step - loss: 0.1975 - average_batch_length: 254.5078\n",
      "Epoch 5/5\n",
      "1283/1283 [==============================] - 862s 672ms/step - loss: 0.1927 - average_batch_length: 254.6313\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 5\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    print('Epoch %d/%d' % (epoch + 1, total_epochs))\n",
    "    model.fit(dataset, callbacks=[checkpoint_callback, model_state_resetter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1368/1368 [==============================] - 909s 664ms/step - loss: 0.1786 - average_batch_length: 254.6038\n",
      "Epoch 2/5\n",
      "1408/1408 [==============================] - 933s 663ms/step - loss: 0.1714 - average_batch_length: 254.5391\n",
      "Epoch 3/5\n",
      "1330/1330 [==============================] - 883s 664ms/step - loss: 0.1791 - average_batch_length: 254.6609\n",
      "Epoch 4/5\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "1411/1411 [==============================] - 949s 673ms/step - loss: 0.1664 - average_batch_length: 254.5904\n",
      "Epoch 5/5\n",
      "1390/1390 [==============================] - 935s 673ms/step - loss: 0.1668 - average_batch_length: 254.8230\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 5\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    print('Epoch %d/%d' % (epoch + 1, total_epochs))\n",
    "    model.fit(dataset, callbacks=[checkpoint_callback, model_state_resetter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1312/1312 [==============================] - 869s 662ms/step - loss: 0.1733 - average_batch_length: 254.3811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2731c070c08>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, callbacks=[checkpoint_callback, model_state_resetter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365/1365 [==============================] - 909s 666ms/step - loss: 0.1644 - average_batch_length: 254.5311\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataset, callbacks=[checkpoint_callback, model_state_resetter_callback]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1282/1282 [==============================] - 861s 672ms/step - loss: 0.1723 - average_batch_length: 254.4251\n",
      "Epoch 2/3\n",
      "1260/1260 [==============================] - 865s 686ms/step - loss: 0.1720 - average_batch_length: 254.5318\n",
      "Epoch 3/3\n",
      "1301/1301 [==============================] - 867s 666ms/step - loss: 0.1643 - average_batch_length: 254.4873\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 3\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    print('Epoch %d/%d' % (epoch + 1, total_epochs))\n",
    "    model.fit(dataset, callbacks=[checkpoint_callback, model_state_resetter_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нека видим какво е научила невронната мрежа..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocab_size = subword_text_encoder.vocab_size, embedding_dim=512, rnn_units=1024, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0:\tCompression: 0.604167\tAvg Compression: 0.604167\n",
      "Article 1:\tCompression: 0.372024\tAvg Compression: 0.441667\n",
      "Article 2:\tCompression: 0.295918\tAvg Compression: 0.376147\n",
      "Article 3:\tCompression: 0.412500\tAvg Compression: 0.383993\n",
      "Article 4:\tCompression: 0.425000\tAvg Compression: 0.392241\n",
      "Article 5:\tCompression: 0.272436\tAvg Compression: 0.370305\n",
      "Article 6:\tCompression: 0.131288\tAvg Compression: 0.132154\n",
      "Article 7:\tCompression: 0.277027\tAvg Compression: 0.132245\n",
      "Article 8:\tCompression: 0.300000\tAvg Compression: 0.132359\n",
      "Article 9:\tCompression: 0.267857\tAvg Compression: 0.132456\n",
      "Article 10:\tCompression: 0.328947\tAvg Compression: 0.132582\n",
      "Article 11:\tCompression: 0.272727\tAvg Compression: 0.132687\n",
      "Article 12:\tCompression: 0.250000\tAvg Compression: 0.132774\n",
      "Article 13:\tCompression: 0.302632\tAvg Compression: 0.132884\n",
      "Article 14:\tCompression: 0.263298\tAvg Compression: 0.132987\n",
      "Article 15:\tCompression: 0.458333\tAvg Compression: 0.133119\n",
      "Article 16:\tCompression: 0.428571\tAvg Compression: 0.133294\n",
      "Article 17:\tCompression: 0.484375\tAvg Compression: 0.133437\n",
      "Article 18:\tCompression: 0.125579\tAvg Compression: 0.129868\n",
      "Article 19:\tCompression: 0.325758\tAvg Compression: 0.129928\n",
      "Article 20:\tCompression: 0.314286\tAvg Compression: 0.129988\n",
      "Article 21:\tCompression: 0.292763\tAvg Compression: 0.130045\n",
      "Article 22:\tCompression: 0.475000\tAvg Compression: 0.130140\n",
      "Article 23:\tCompression: 0.427885\tAvg Compression: 0.130211\n",
      "Article 24:\tCompression: 0.334559\tAvg Compression: 0.130275\n",
      "Article 25:\tCompression: 0.329545\tAvg Compression: 0.130336\n",
      "Article 26:\tCompression: 0.407609\tAvg Compression: 0.130394\n",
      "Article 27:\tCompression: 0.144143\tAvg Compression: 0.131432\n",
      "Article 28:\tCompression: 0.304054\tAvg Compression: 0.131486\n",
      "Article 29:\tCompression: 0.310345\tAvg Compression: 0.131575\n",
      "Article 30:\tCompression: 0.133822\tAvg Compression: 0.131759\n",
      "Article 31:\tCompression: 0.447917\tAvg Compression: 0.131818\n",
      "Article 32:\tCompression: 0.500000\tAvg Compression: 0.131881\n",
      "Article 33:\tCompression: 0.456522\tAvg Compression: 0.131940\n",
      "Article 34:\tCompression: 0.528409\tAvg Compression: 0.132008\n",
      "Article 35:\tCompression: 0.528409\tAvg Compression: 0.132076\n",
      "Article 36:\tCompression: 0.369792\tAvg Compression: 0.132120\n",
      "Article 37:\tCompression: 0.482143\tAvg Compression: 0.132177\n",
      "Article 38:\tCompression: 0.511905\tAvg Compression: 0.132239\n",
      "Article 39:\tCompression: 0.383929\tAvg Compression: 0.132294\n",
      "Article 40:\tCompression: 0.366667\tAvg Compression: 0.132349\n",
      "Article 41:\tCompression: 0.484375\tAvg Compression: 0.132415\n",
      "Article 42:\tCompression: 0.316176\tAvg Compression: 0.132488\n",
      "Article 43:\tCompression: 0.273256\tAvg Compression: 0.132535\n",
      "Article 44:\tCompression: 0.252976\tAvg Compression: 0.132574\n",
      "Article 45:\tCompression: 0.252976\tAvg Compression: 0.132614\n",
      "Article 46:\tCompression: 0.262821\tAvg Compression: 0.132653\n",
      "Article 47:\tCompression: 0.272436\tAvg Compression: 0.132695\n",
      "Article 48:\tCompression: 0.387097\tAvg Compression: 0.132757\n",
      "Article 49:\tCompression: 0.401786\tAvg Compression: 0.132815\n",
      "Article 50:\tCompression: 0.483696\tAvg Compression: 0.132878\n",
      "Article 51:\tCompression: 0.382576\tAvg Compression: 0.132942\n",
      "Article 52:\tCompression: 0.407407\tAvg Compression: 0.132999\n",
      "Article 53:\tCompression: 0.450893\tAvg Compression: 0.133068\n",
      "Article 54:\tCompression: 0.425000\tAvg Compression: 0.133125\n",
      "Article 55:\tCompression: 0.287162\tAvg Compression: 0.133169\n",
      "Article 56:\tCompression: 0.473958\tAvg Compression: 0.133233\n",
      "Article 57:\tCompression: 0.500000\tAvg Compression: 0.133292\n",
      "Article 58:\tCompression: 0.370968\tAvg Compression: 0.133350\n",
      "Article 59:\tCompression: 0.473958\tAvg Compression: 0.133413\n",
      "Article 60:\tCompression: 0.300000\tAvg Compression: 0.133458\n",
      "Article 61:\tCompression: 0.363636\tAvg Compression: 0.133517\n",
      "Article 62:\tCompression: 0.598214\tAvg Compression: 0.133618\n",
      "Article 63:\tCompression: 0.467742\tAvg Compression: 0.133698\n",
      "Article 64:\tCompression: 0.447917\tAvg Compression: 0.133756\n",
      "Article 65:\tCompression: 0.460227\tAvg Compression: 0.133812\n",
      "Article 66:\tCompression: 0.416667\tAvg Compression: 0.133865\n",
      "Article 67:\tCompression: 0.392857\tAvg Compression: 0.133921\n",
      "Article 68:\tCompression: 0.344828\tAvg Compression: 0.133968\n",
      "Article 69:\tCompression: 0.252778\tAvg Compression: 0.134009\n",
      "Article 70:\tCompression: 0.333333\tAvg Compression: 0.134056\n",
      "Article 71:\tCompression: 0.355469\tAvg Compression: 0.134110\n",
      "Article 72:\tCompression: 0.403226\tAvg Compression: 0.134175\n",
      "Article 73:\tCompression: 0.321970\tAvg Compression: 0.134223\n",
      "Article 74:\tCompression: 0.311047\tAvg Compression: 0.134281\n",
      "Article 75:\tCompression: 0.375000\tAvg Compression: 0.134339\n",
      "Article 76:\tCompression: 0.483696\tAvg Compression: 0.134401\n",
      "Article 77:\tCompression: 0.342857\tAvg Compression: 0.134457\n",
      "Article 78:\tCompression: 0.299342\tAvg Compression: 0.134506\n",
      "Article 79:\tCompression: 0.301471\tAvg Compression: 0.134549\n",
      "Article 80:\tCompression: 0.329545\tAvg Compression: 0.134599\n",
      "Article 81:\tCompression: 0.275641\tAvg Compression: 0.134641\n",
      "Article 82:\tCompression: 0.518750\tAvg Compression: 0.134701\n",
      "Article 83:\tCompression: 0.398810\tAvg Compression: 0.134786\n",
      "Article 84:\tCompression: 0.127366\tAvg Compression: 0.134207\n",
      "Article 85:\tCompression: 0.327206\tAvg Compression: 0.134253\n",
      "Article 86:\tCompression: 0.363636\tAvg Compression: 0.134307\n",
      "Article 87:\tCompression: 0.398810\tAvg Compression: 0.134386\n",
      "Article 88:\tCompression: 0.456897\tAvg Compression: 0.134452\n",
      "Article 89:\tCompression: 0.350694\tAvg Compression: 0.134508\n",
      "Article 90:\tCompression: 0.413793\tAvg Compression: 0.134565\n",
      "Article 91:\tCompression: 0.495370\tAvg Compression: 0.134634\n",
      "Article 92:\tCompression: 0.100429\tAvg Compression: 0.130267\n",
      "Article 93:\tCompression: 0.460227\tAvg Compression: 0.130311\n",
      "Article 94:\tCompression: 0.146580\tAvg Compression: 0.132133\n",
      "Article 95:\tCompression: 0.356618\tAvg Compression: 0.132175\n",
      "Article 96:\tCompression: 0.120817\tAvg Compression: 0.128698\n",
      "Article 97:\tCompression: 0.129828\tAvg Compression: 0.128862\n",
      "Article 98:\tCompression: 0.148297\tAvg Compression: 0.128932\n",
      "Article 99:\tCompression: 0.518750\tAvg Compression: 0.128957\n",
      "Article 100:\tCompression: 0.252778\tAvg Compression: 0.128975\n",
      "Article 101:\tCompression: 0.284091\tAvg Compression: 0.128998\n",
      "Article 102:\tCompression: 0.266667\tAvg Compression: 0.129018\n",
      "Article 103:\tCompression: 0.236111\tAvg Compression: 0.129036\n",
      "Article 104:\tCompression: 0.223214\tAvg Compression: 0.129054\n",
      "Article 105:\tCompression: 0.227941\tAvg Compression: 0.129070\n",
      "Article 106:\tCompression: 0.270833\tAvg Compression: 0.129089\n",
      "Article 107:\tCompression: 0.343750\tAvg Compression: 0.129131\n",
      "Article 108:\tCompression: 0.240385\tAvg Compression: 0.129150\n",
      "Article 109:\tCompression: 0.237981\tAvg Compression: 0.129168\n",
      "Article 110:\tCompression: 0.447674\tAvg Compression: 0.129213\n",
      "Article 111:\tCompression: 0.325000\tAvg Compression: 0.129247\n",
      "Article 112:\tCompression: 0.104395\tAvg Compression: 0.128458\n",
      "Article 113:\tCompression: 0.495370\tAvg Compression: 0.128489\n",
      "Article 114:\tCompression: 0.518750\tAvg Compression: 0.128514\n",
      "Article 115:\tCompression: 0.434783\tAvg Compression: 0.128558\n",
      "Article 116:\tCompression: 0.531250\tAvg Compression: 0.128583\n",
      "Article 117:\tCompression: 0.157226\tAvg Compression: 0.128684\n",
      "Article 118:\tCompression: 0.322368\tAvg Compression: 0.128707\n",
      "Article 119:\tCompression: 0.143993\tAvg Compression: 0.128877\n"
     ]
    }
   ],
   "source": [
    "total_raw = 0\n",
    "total_compressed = 0\n",
    "\n",
    "for index, article in enumerate(itertools.islice(articles(), 0, 120)):\n",
    "    raw = (len(article) + 1) * 8\n",
    "    encoded_article = np.array(subword_text_encoder.encode(article + b'\\0'), dtype=TYPE)\n",
    "    compressed, _ = huffman_archive_size(model, encoded_article)\n",
    "    total_raw += raw\n",
    "    total_compressed += compressed\n",
    "    print('Article %d:\\tCompression: %f\\tAvg Compression: %f' % (index, compressed/raw, total_compressed/total_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Може да е само илюзия, но изглежда, че компресията става по-добра с напредването на статиите. Некато го проверим, гледайки компресирания размер на последните 100 статии, използвани за обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0:\tCompression: 0.113915\tAvg Compression: 0.113915\n",
      "Article 1:\tCompression: 0.111272\tAvg Compression: 0.111688\n",
      "Article 2:\tCompression: 0.136296\tAvg Compression: 0.119879\n",
      "Article 3:\tCompression: 0.099103\tAvg Compression: 0.101286\n",
      "Article 4:\tCompression: 0.123359\tAvg Compression: 0.102694\n",
      "Article 5:\tCompression: 0.113522\tAvg Compression: 0.103155\n",
      "Article 6:\tCompression: 0.128168\tAvg Compression: 0.105730\n",
      "Article 7:\tCompression: 0.111517\tAvg Compression: 0.106563\n",
      "Article 8:\tCompression: 0.133822\tAvg Compression: 0.110462\n",
      "Article 9:\tCompression: 0.545455\tAvg Compression: 0.110580\n",
      "Article 10:\tCompression: 0.128954\tAvg Compression: 0.110909\n",
      "Article 11:\tCompression: 0.137648\tAvg Compression: 0.111675\n",
      "Article 12:\tCompression: 0.200331\tAvg Compression: 0.111987\n",
      "Article 13:\tCompression: 0.119649\tAvg Compression: 0.112366\n",
      "Article 14:\tCompression: 0.102357\tAvg Compression: 0.112189\n",
      "Article 15:\tCompression: 0.097865\tAvg Compression: 0.111682\n",
      "Article 16:\tCompression: 0.094076\tAvg Compression: 0.111489\n",
      "Article 17:\tCompression: 0.104930\tAvg Compression: 0.111403\n",
      "Article 18:\tCompression: 0.111288\tAvg Compression: 0.111399\n",
      "Article 19:\tCompression: 0.449074\tAvg Compression: 0.111489\n",
      "Article 20:\tCompression: 0.558824\tAvg Compression: 0.111639\n",
      "Article 21:\tCompression: 0.130750\tAvg Compression: 0.111810\n",
      "Article 22:\tCompression: 0.123510\tAvg Compression: 0.112008\n",
      "Article 23:\tCompression: 0.119603\tAvg Compression: 0.112265\n",
      "Article 24:\tCompression: 0.085024\tAvg Compression: 0.110088\n",
      "Article 25:\tCompression: 0.125746\tAvg Compression: 0.110567\n",
      "Article 26:\tCompression: 0.143745\tAvg Compression: 0.110933\n",
      "Article 27:\tCompression: 0.164662\tAvg Compression: 0.111270\n",
      "Article 28:\tCompression: 0.115680\tAvg Compression: 0.111330\n",
      "Article 29:\tCompression: 0.115489\tAvg Compression: 0.111455\n",
      "Article 30:\tCompression: 0.113324\tAvg Compression: 0.111606\n",
      "Article 31:\tCompression: 0.107339\tAvg Compression: 0.111458\n",
      "Article 32:\tCompression: 0.108705\tAvg Compression: 0.111255\n",
      "Article 33:\tCompression: 0.144521\tAvg Compression: 0.111583\n",
      "Article 34:\tCompression: 0.516667\tAvg Compression: 0.111660\n",
      "Article 35:\tCompression: 0.122019\tAvg Compression: 0.111955\n",
      "Article 36:\tCompression: 0.081309\tAvg Compression: 0.110549\n",
      "Article 37:\tCompression: 0.094697\tAvg Compression: 0.110448\n",
      "Article 38:\tCompression: 0.106959\tAvg Compression: 0.110358\n",
      "Article 39:\tCompression: 0.120261\tAvg Compression: 0.110405\n",
      "Article 40:\tCompression: 0.115815\tAvg Compression: 0.110881\n",
      "Article 41:\tCompression: 0.103657\tAvg Compression: 0.109491\n",
      "Article 42:\tCompression: 0.112840\tAvg Compression: 0.109639\n",
      "Article 43:\tCompression: 0.123006\tAvg Compression: 0.109672\n",
      "Article 44:\tCompression: 0.085163\tAvg Compression: 0.108846\n",
      "Article 45:\tCompression: 0.103320\tAvg Compression: 0.108805\n",
      "Article 46:\tCompression: 0.285256\tAvg Compression: 0.108831\n",
      "Article 47:\tCompression: 0.133035\tAvg Compression: 0.109034\n",
      "Article 48:\tCompression: 0.809783\tAvg Compression: 0.109095\n",
      "Article 49:\tCompression: 0.120537\tAvg Compression: 0.109333\n",
      "Article 50:\tCompression: 0.099421\tAvg Compression: 0.109113\n",
      "Article 51:\tCompression: 0.138683\tAvg Compression: 0.109185\n",
      "Article 52:\tCompression: 0.101010\tAvg Compression: 0.109118\n",
      "Article 53:\tCompression: 0.093110\tAvg Compression: 0.108677\n",
      "Article 54:\tCompression: 0.144832\tAvg Compression: 0.108807\n",
      "Article 55:\tCompression: 0.140713\tAvg Compression: 0.109196\n",
      "Article 56:\tCompression: 0.116613\tAvg Compression: 0.109468\n",
      "Article 57:\tCompression: 0.103626\tAvg Compression: 0.109417\n",
      "Article 58:\tCompression: 0.110408\tAvg Compression: 0.109441\n",
      "Article 59:\tCompression: 0.105714\tAvg Compression: 0.109363\n",
      "Article 60:\tCompression: 0.405000\tAvg Compression: 0.109386\n",
      "Article 61:\tCompression: 0.128272\tAvg Compression: 0.109607\n",
      "Article 62:\tCompression: 0.443750\tAvg Compression: 0.109628\n",
      "Article 63:\tCompression: 0.727273\tAvg Compression: 0.109670\n",
      "Article 64:\tCompression: 0.131963\tAvg Compression: 0.109747\n",
      "Article 65:\tCompression: 0.137741\tAvg Compression: 0.109907\n",
      "Article 66:\tCompression: 0.095568\tAvg Compression: 0.109266\n",
      "Article 67:\tCompression: 0.092230\tAvg Compression: 0.108661\n",
      "Article 68:\tCompression: 0.088360\tAvg Compression: 0.107732\n",
      "Article 69:\tCompression: 0.090257\tAvg Compression: 0.107415\n",
      "Article 70:\tCompression: 0.103998\tAvg Compression: 0.107344\n",
      "Article 71:\tCompression: 0.129449\tAvg Compression: 0.107426\n",
      "Article 72:\tCompression: 0.146385\tAvg Compression: 0.107682\n",
      "Article 73:\tCompression: 0.135569\tAvg Compression: 0.107780\n",
      "Article 74:\tCompression: 0.352273\tAvg Compression: 0.107801\n",
      "Article 75:\tCompression: 0.380000\tAvg Compression: 0.107818\n",
      "Article 76:\tCompression: 0.097655\tAvg Compression: 0.107679\n",
      "Article 77:\tCompression: 0.446429\tAvg Compression: 0.107703\n",
      "Article 78:\tCompression: 0.103812\tAvg Compression: 0.107665\n",
      "Article 79:\tCompression: 0.442073\tAvg Compression: 0.107699\n",
      "Article 80:\tCompression: 0.103107\tAvg Compression: 0.107647\n",
      "Article 81:\tCompression: 0.390152\tAvg Compression: 0.107670\n",
      "Article 82:\tCompression: 0.106866\tAvg Compression: 0.107610\n",
      "Article 83:\tCompression: 0.105182\tAvg Compression: 0.107574\n",
      "Article 84:\tCompression: 0.114686\tAvg Compression: 0.107627\n",
      "Article 85:\tCompression: 0.115148\tAvg Compression: 0.107675\n",
      "Article 86:\tCompression: 0.135303\tAvg Compression: 0.107820\n",
      "Article 87:\tCompression: 0.466912\tAvg Compression: 0.107847\n",
      "Article 88:\tCompression: 0.520833\tAvg Compression: 0.107874\n",
      "Article 89:\tCompression: 0.105768\tAvg Compression: 0.107822\n",
      "Article 90:\tCompression: 0.131747\tAvg Compression: 0.107874\n",
      "Article 91:\tCompression: 0.108294\tAvg Compression: 0.107879\n",
      "Article 92:\tCompression: 0.100279\tAvg Compression: 0.107813\n",
      "Article 93:\tCompression: 0.242754\tAvg Compression: 0.107833\n",
      "Article 94:\tCompression: 0.140551\tAvg Compression: 0.107978\n",
      "Article 95:\tCompression: 0.106589\tAvg Compression: 0.107945\n",
      "Article 96:\tCompression: 0.514286\tAvg Compression: 0.107975\n",
      "Article 97:\tCompression: 0.130564\tAvg Compression: 0.108077\n",
      "Article 98:\tCompression: 0.110178\tAvg Compression: 0.108092\n",
      "Article 99:\tCompression: 0.110542\tAvg Compression: 0.108177\n"
     ]
    }
   ],
   "source": [
    "total_raw = 0\n",
    "total_compressed = 0\n",
    "\n",
    "for index, article in enumerate(itertools.islice(articles(), 1900, 2000)):\n",
    "    raw = (len(article) + 1) * 8\n",
    "    encoded_article = np.array(subword_text_encoder.encode(article + b'\\0'), dtype=TYPE)\n",
    "    compressed, _ = huffman_archive_size(model, encoded_article)\n",
    "    total_raw += raw\n",
    "    total_compressed += compressed\n",
    "    print('Article %d:\\tCompression: %f\\tAvg Compression: %f' % (index, compressed/raw, total_compressed/total_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, определено. Трябва да се поработи над dataset-а."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
