{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def articles():\n",
    "    with open('page_revisions_text', 'rb') as text_file:\n",
    "        pending_article_data = b''\n",
    "        while True:\n",
    "            data = text_file.read(1024 ** 2)\n",
    "            if len(data) == 0:\n",
    "                break\n",
    "\n",
    "            articles = data.split(b'\\0')\n",
    "            articles[0] = pending_article_data + articles[0]\n",
    "            for index, article in enumerate(articles):\n",
    "                if index + 1 == len(articles):\n",
    "                    pending_article_data = article\n",
    "                else:\n",
    "                    yield article\n",
    "\n",
    "        print(pending_article_data)\n",
    "        if len(pending_article_data) != 0:\n",
    "            yield pending_article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword_text_encoder = tfds.features.text.SubwordTextEncoder.load_from_file('vocab_4096')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((192, None), (192, None)), types: (tf.int16, tf.int16)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 192\n",
    "BATCHED_ITEM_LENGTH = 256\n",
    "BUFFER_SIZE = 1024\n",
    "TYPE=np.int16\n",
    "\n",
    "def articles_generator():\n",
    "    for index, article in enumerate(itertools.islice(articles(), 0, 10000)):\n",
    "        yield np.array(subword_text_encoder.encode(article + b'\\0'), dtype=TYPE)\n",
    "\n",
    "    # Pad the article count to the batch size\n",
    "    # We do this to ensure that no data is dropped\n",
    "    index += 1\n",
    "    while index % BATCH_SIZE != 0:\n",
    "        yield np.array([0], dtype=TYPE)\n",
    "        index += 1\n",
    "\n",
    "def subbatches():\n",
    "    dataset = tf.data.Dataset.from_generator(articles_generator, output_types=TYPE)\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "    dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=([None]), drop_remainder=True)\n",
    "    dataset = dataset.shuffle(2000)\n",
    "\n",
    "    for batch in dataset.as_numpy_iterator():\n",
    "        remaining = batch\n",
    "        while remaining.shape[1] > 1:\n",
    "            yield remaining[:, :BATCHED_ITEM_LENGTH + 1]\n",
    "            remaining = remaining[:, BATCHED_ITEM_LENGTH:]\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(subbatches, output_types=TYPE, output_shapes=(BATCH_SIZE, None))\n",
    "dataset = dataset.map(lambda batch: (batch[:, :-1], batch[:, 1:]))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Masking(mask_value=0, batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "def average_batch_length(true_labels, predictions):\n",
    "    return tf.shape(true_labels)[1]\n",
    "\n",
    "model = build_model(vocab_size = subword_text_encoder.vocab_size, embedding_dim=512, rnn_units=1024, batch_size=BATCH_SIZE)\n",
    "model.compile(optimizer='adam', loss=loss, metrics=[average_batch_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints-1' # Directory where the checkpoints will be saved\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\") # Name of the checkpoint files\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelStateResetter(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.last_total_length = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        average_batch_length = logs.get('average_batch_length', 0)\n",
    "        total_length = int(round(average_batch_length * (batch + 1)))\n",
    "        current_batch_length = total_length - self.last_total_length\n",
    "        self.last_total_length = total_length\n",
    "        \n",
    "        if current_batch_length < BATCHED_ITEM_LENGTH:\n",
    "            self.model.reset_states()\n",
    "        \n",
    "model_state_resetter_callback = ModelStateResetter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5032/5032 [==============================] - 4809s 956ms/step - loss: 0.6076 - average_batch_length: 254.7204\n",
      "Epoch 2/10\n",
      "4927/4927 [==============================] - 4720s 958ms/step - loss: 0.4464 - average_batch_length: 254.5784\n",
      "Epoch 3/10\n",
      "5031/5031 [==============================] - 4818s 958ms/step - loss: 0.3901 - average_batch_length: 254.4858\n",
      "Epoch 4/10\n",
      "5007/5007 [==============================] - 4801s 959ms/step - loss: 0.3683 - average_batch_length: 254.7234\n",
      "Epoch 5/10\n",
      "5007/5007 [==============================] - 4848s 968ms/step - loss: 0.3532 - average_batch_length: 254.6295\n",
      "Epoch 6/10\n",
      "4919/4919 [==============================] - 4738s 963ms/step - loss: 0.3485 - average_batch_length: 254.6438\n",
      "Epoch 7/10\n",
      "5058/5058 [==============================] - 4879s 965ms/step - loss: 0.3305 - average_batch_length: 254.6433\n",
      "Epoch 8/10\n",
      "4943/4943 [==============================] - 4737s 958ms/step - loss: 0.3307 - average_batch_length: 254.6599\n",
      "Epoch 9/10\n",
      "5030/5030 [==============================] - 4837s 962ms/step - loss: 0.3197 - average_batch_length: 254.6779\n",
      "Epoch 10/10\n",
      "4900/4900 [==============================] - 4867s 993ms/step - loss: 0.3227 - average_batch_length: 254.5914\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 10\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    print('Epoch %d/%d' % (epoch + 1, total_epochs))\n",
    "    model.fit(dataset, callbacks=[checkpoint_callback, model_state_resetter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4956/4956 [==============================] - 4851s 979ms/step - loss: 0.3146 - average_batch_length: 254.7042\n",
      "Epoch 2/10\n",
      "4900/4900 [==============================] - 4878s 995ms/step - loss: 0.3145 - average_batch_length: 254.6269\n",
      "Epoch 3/10\n",
      "4991/4991 [==============================] - 4838s 969ms/step - loss: 0.3050 - average_batch_length: 254.5005\n",
      "Epoch 4/10\n",
      "4990/4990 [==============================] - 4910s 984ms/step - loss: 0.3018 - average_batch_length: 254.5413\n",
      "Epoch 5/10\n",
      "4973/4973 [==============================] - 4839s 973ms/step - loss: 0.2999 - average_batch_length: 254.5230\n",
      "Epoch 6/10\n",
      "4964/4964 [==============================] - 4754s 958ms/step - loss: 0.2976 - average_batch_length: 254.7611\n",
      "Epoch 7/10\n",
      "4951/4951 [==============================] - 4736s 956ms/step - loss: 0.2957 - average_batch_length: 254.5595\n",
      "Epoch 8/10\n",
      "4998/4998 [==============================] - 4781s 956ms/step - loss: 0.2905 - average_batch_length: 254.7919\n",
      "Epoch 9/10\n",
      "4905/4905 [==============================] - 4687s 956ms/step - loss: 0.2939 - average_batch_length: 254.6856\n",
      "Epoch 10/10\n",
      "4955/4955 [==============================] - 4738s 956ms/step - loss: 0.2888 - average_batch_length: 254.6494\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 10\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    print('Epoch %d/%d' % (epoch + 1, total_epochs))\n",
    "    model.fit(dataset, callbacks=[checkpoint_callback, model_state_resetter_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: 25541\n",
      "Encoded: 8222\n"
     ]
    }
   ],
   "source": [
    "with open('page_revisions_text', 'rb') as text_file:\n",
    "    data = text_file.read()\n",
    "\n",
    "article = data.split(b'\\0')[120]\n",
    "del data\n",
    "\n",
    "encoded_article = np.array(subword_text_encoder.encode(article + b'\\0'), dtype=TYPE)\n",
    "\n",
    "print('Raw:', len(article))\n",
    "print('Encoded:', len(encoded_article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "\n",
    "class Huffman:\n",
    "    huffman = ctypes.CDLL('x64/Release/huffman')\n",
    "    \n",
    "    huffman.create_tree.restype = ctypes.c_void_p\n",
    "    huffman.destroy_tree.restype = None\n",
    "    huffman.load_weights.restype = None\n",
    "    huffman.create_code_string.restype = ctypes.c_char_p\n",
    "    \n",
    "    def __init__(self, category_count):\n",
    "        self.tree = ctypes.c_void_p(self.huffman.create_tree(category_count))\n",
    "\n",
    "    def __del__(self):\n",
    "        self.huffman.destroy_tree(self.tree)\n",
    "        \n",
    "    def load_weights(self, weights):\n",
    "        self.huffman.load_weights(self.tree, weights.ctypes.data_as(ctypes.POINTER(ctypes.c_float)))\n",
    "    \n",
    "    def get_code_length(self, category):\n",
    "        return self.huffman.get_code_length(self.tree, category)\n",
    "\n",
    "    def get_code_zero_count(self, category):\n",
    "        return self.huffman.get_code_zero_count(self.tree, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huffman_archive_size(model, text):\n",
    "    archived_size = 0\n",
    "    zeros = 0\n",
    "    input_eval = np.array([[0]], dtype=TYPE)\n",
    "    huffman_tree = Huffman(subword_text_encoder.vocab_size)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    model.reset_states()\n",
    "\n",
    "    for index, byte in enumerate(text):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0) # remove the batch dimension\n",
    "\n",
    "        weights = tf.nn.softmax(predictions[0]).numpy()\n",
    "        huffman_tree.load_weights(weights)\n",
    "        zeros += huffman_tree.get_code_zero_count(byte.item())\n",
    "        archived_size += huffman_tree.get_code_length(byte.item())\n",
    "\n",
    "        input_eval = tf.expand_dims([byte], 0)\n",
    "  \n",
    "    return archived_size, zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(vocab_size = subword_text_encoder.vocab_size, embedding_dim=512, rnn_units=1024, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0:\tCompression: 0.430556\tAvg Compression: 0.430556\n",
      "Article 1:\tCompression: 0.250000\tAvg Compression: 0.304167\n",
      "Article 2:\tCompression: 0.229592\tAvg Compression: 0.270642\n",
      "Article 3:\tCompression: 0.283333\tAvg Compression: 0.273381\n",
      "Article 4:\tCompression: 0.314286\tAvg Compression: 0.281609\n",
      "Article 5:\tCompression: 0.217949\tAvg Compression: 0.269953\n",
      "Article 6:\tCompression: 0.176171\tAvg Compression: 0.176511\n",
      "Article 7:\tCompression: 0.209459\tAvg Compression: 0.176531\n",
      "Article 8:\tCompression: 0.246875\tAvg Compression: 0.176579\n",
      "Article 9:\tCompression: 0.187500\tAvg Compression: 0.176587\n",
      "Article 10:\tCompression: 0.236842\tAvg Compression: 0.176626\n",
      "Article 11:\tCompression: 0.187500\tAvg Compression: 0.176634\n",
      "Article 12:\tCompression: 0.184659\tAvg Compression: 0.176640\n",
      "Article 13:\tCompression: 0.220395\tAvg Compression: 0.176668\n",
      "Article 14:\tCompression: 0.172872\tAvg Compression: 0.176665\n",
      "Article 15:\tCompression: 0.375000\tAvg Compression: 0.176746\n",
      "Article 16:\tCompression: 0.314286\tAvg Compression: 0.176827\n",
      "Article 17:\tCompression: 0.375000\tAvg Compression: 0.176907\n",
      "Article 18:\tCompression: 0.175486\tAvg Compression: 0.176262\n",
      "Article 19:\tCompression: 0.250000\tAvg Compression: 0.176285\n",
      "Article 20:\tCompression: 0.235714\tAvg Compression: 0.176304\n",
      "Article 21:\tCompression: 0.207237\tAvg Compression: 0.176315\n",
      "Article 22:\tCompression: 0.504167\tAvg Compression: 0.176405\n",
      "Article 23:\tCompression: 0.331731\tAvg Compression: 0.176442\n",
      "Article 24:\tCompression: 0.235294\tAvg Compression: 0.176461\n",
      "Article 25:\tCompression: 0.234848\tAvg Compression: 0.176479\n",
      "Article 26:\tCompression: 0.402174\tAvg Compression: 0.176526\n",
      "Article 27:\tCompression: 0.179710\tAvg Compression: 0.176767\n",
      "Article 28:\tCompression: 0.229730\tAvg Compression: 0.176783\n",
      "Article 29:\tCompression: 0.174569\tAvg Compression: 0.176782\n",
      "Article 30:\tCompression: 0.196750\tAvg Compression: 0.178420\n",
      "Article 31:\tCompression: 0.390625\tAvg Compression: 0.178460\n",
      "Article 32:\tCompression: 0.420455\tAvg Compression: 0.178502\n",
      "Article 33:\tCompression: 0.364130\tAvg Compression: 0.178535\n",
      "Article 34:\tCompression: 0.460227\tAvg Compression: 0.178583\n",
      "Article 35:\tCompression: 0.448864\tAvg Compression: 0.178629\n",
      "Article 36:\tCompression: 0.338542\tAvg Compression: 0.178659\n",
      "Article 37:\tCompression: 0.410714\tAvg Compression: 0.178697\n",
      "Article 38:\tCompression: 0.404762\tAvg Compression: 0.178734\n",
      "Article 39:\tCompression: 0.321429\tAvg Compression: 0.178765\n",
      "Article 40:\tCompression: 0.300000\tAvg Compression: 0.178794\n",
      "Article 41:\tCompression: 0.348958\tAvg Compression: 0.178826\n",
      "Article 42:\tCompression: 0.227941\tAvg Compression: 0.178845\n",
      "Article 43:\tCompression: 0.180233\tAvg Compression: 0.178846\n",
      "Article 44:\tCompression: 0.202381\tAvg Compression: 0.178853\n",
      "Article 45:\tCompression: 0.202381\tAvg Compression: 0.178861\n",
      "Article 46:\tCompression: 0.230769\tAvg Compression: 0.178877\n",
      "Article 47:\tCompression: 0.214744\tAvg Compression: 0.178888\n",
      "Article 48:\tCompression: 0.229839\tAvg Compression: 0.178900\n",
      "Article 49:\tCompression: 0.303571\tAvg Compression: 0.178927\n",
      "Article 50:\tCompression: 0.407609\tAvg Compression: 0.178968\n",
      "Article 51:\tCompression: 0.318182\tAvg Compression: 0.179003\n",
      "Article 52:\tCompression: 0.407407\tAvg Compression: 0.179051\n",
      "Article 53:\tCompression: 0.361607\tAvg Compression: 0.179091\n",
      "Article 54:\tCompression: 0.380000\tAvg Compression: 0.179130\n",
      "Article 55:\tCompression: 0.277027\tAvg Compression: 0.179158\n",
      "Article 56:\tCompression: 0.411458\tAvg Compression: 0.179201\n",
      "Article 57:\tCompression: 0.410714\tAvg Compression: 0.179239\n",
      "Article 58:\tCompression: 0.318548\tAvg Compression: 0.179272\n",
      "Article 59:\tCompression: 0.364583\tAvg Compression: 0.179307\n",
      "Article 60:\tCompression: 0.239286\tAvg Compression: 0.179323\n",
      "Article 61:\tCompression: 0.310606\tAvg Compression: 0.179357\n",
      "Article 62:\tCompression: 0.383929\tAvg Compression: 0.179401\n",
      "Article 63:\tCompression: 0.266129\tAvg Compression: 0.179422\n",
      "Article 64:\tCompression: 0.291667\tAvg Compression: 0.179443\n",
      "Article 65:\tCompression: 0.329545\tAvg Compression: 0.179468\n",
      "Article 66:\tCompression: 0.317708\tAvg Compression: 0.179494\n",
      "Article 67:\tCompression: 0.339286\tAvg Compression: 0.179529\n",
      "Article 68:\tCompression: 0.267241\tAvg Compression: 0.179548\n",
      "Article 69:\tCompression: 0.186111\tAvg Compression: 0.179551\n",
      "Article 70:\tCompression: 0.287500\tAvg Compression: 0.179576\n",
      "Article 71:\tCompression: 0.304688\tAvg Compression: 0.179607\n",
      "Article 72:\tCompression: 0.358871\tAvg Compression: 0.179650\n",
      "Article 73:\tCompression: 0.231061\tAvg Compression: 0.179663\n",
      "Article 74:\tCompression: 0.244186\tAvg Compression: 0.179684\n",
      "Article 75:\tCompression: 0.318548\tAvg Compression: 0.179717\n",
      "Article 76:\tCompression: 0.358696\tAvg Compression: 0.179749\n",
      "Article 77:\tCompression: 0.253571\tAvg Compression: 0.179769\n",
      "Article 78:\tCompression: 0.200658\tAvg Compression: 0.179775\n",
      "Article 79:\tCompression: 0.231618\tAvg Compression: 0.179789\n",
      "Article 80:\tCompression: 0.219697\tAvg Compression: 0.179799\n",
      "Article 81:\tCompression: 0.221154\tAvg Compression: 0.179811\n",
      "Article 82:\tCompression: 0.356250\tAvg Compression: 0.179839\n",
      "Article 83:\tCompression: 0.211310\tAvg Compression: 0.179849\n",
      "Article 84:\tCompression: 0.165423\tAvg Compression: 0.178723\n",
      "Article 85:\tCompression: 0.261029\tAvg Compression: 0.178742\n",
      "Article 86:\tCompression: 0.268939\tAvg Compression: 0.178764\n",
      "Article 87:\tCompression: 0.211310\tAvg Compression: 0.178773\n",
      "Article 88:\tCompression: 0.275862\tAvg Compression: 0.178793\n",
      "Article 89:\tCompression: 0.270833\tAvg Compression: 0.178817\n",
      "Article 90:\tCompression: 0.362069\tAvg Compression: 0.178854\n",
      "Article 91:\tCompression: 0.300926\tAvg Compression: 0.178878\n",
      "Article 92:\tCompression: 0.149196\tAvg Compression: 0.175088\n",
      "Article 93:\tCompression: 0.352273\tAvg Compression: 0.175112\n",
      "Article 94:\tCompression: 0.201044\tAvg Compression: 0.178015\n",
      "Article 95:\tCompression: 0.253676\tAvg Compression: 0.178030\n",
      "Article 96:\tCompression: 0.174317\tAvg Compression: 0.176893\n",
      "Article 97:\tCompression: 0.167899\tAvg Compression: 0.175592\n",
      "Article 98:\tCompression: 0.182684\tAvg Compression: 0.175618\n",
      "Article 99:\tCompression: 0.356250\tAvg Compression: 0.175629\n",
      "Article 100:\tCompression: 0.186111\tAvg Compression: 0.175631\n",
      "Article 101:\tCompression: 0.213068\tAvg Compression: 0.175636\n",
      "Article 102:\tCompression: 0.191667\tAvg Compression: 0.175639\n",
      "Article 103:\tCompression: 0.164352\tAvg Compression: 0.175637\n",
      "Article 104:\tCompression: 0.160714\tAvg Compression: 0.175634\n",
      "Article 105:\tCompression: 0.181373\tAvg Compression: 0.175635\n",
      "Article 106:\tCompression: 0.187500\tAvg Compression: 0.175637\n",
      "Article 107:\tCompression: 0.202083\tAvg Compression: 0.175642\n",
      "Article 108:\tCompression: 0.163462\tAvg Compression: 0.175640\n",
      "Article 109:\tCompression: 0.199519\tAvg Compression: 0.175644\n",
      "Article 110:\tCompression: 0.258721\tAvg Compression: 0.175655\n",
      "Article 111:\tCompression: 0.238636\tAvg Compression: 0.175667\n",
      "Article 112:\tCompression: 0.155932\tAvg Compression: 0.175040\n",
      "Article 113:\tCompression: 0.300926\tAvg Compression: 0.175050\n",
      "Article 114:\tCompression: 0.356250\tAvg Compression: 0.175062\n",
      "Article 115:\tCompression: 0.328804\tAvg Compression: 0.175084\n",
      "Article 116:\tCompression: 0.337500\tAvg Compression: 0.175094\n",
      "Article 117:\tCompression: 0.221677\tAvg Compression: 0.175257\n",
      "Article 118:\tCompression: 0.223684\tAvg Compression: 0.175263\n",
      "Article 119:\tCompression: 0.178924\tAvg Compression: 0.175304\n",
      "Article 120:\tCompression: 0.172887\tAvg Compression: 0.175127\n",
      "Article 121:\tCompression: 0.182297\tAvg Compression: 0.175227\n",
      "Article 122:\tCompression: 0.324074\tAvg Compression: 0.175238\n",
      "Article 123:\tCompression: 0.165112\tAvg Compression: 0.174182\n",
      "Article 124:\tCompression: 0.157566\tAvg Compression: 0.174107\n",
      "Article 125:\tCompression: 0.253676\tAvg Compression: 0.174114\n",
      "Article 126:\tCompression: 0.314516\tAvg Compression: 0.174125\n",
      "Article 127:\tCompression: 0.157741\tAvg Compression: 0.173949\n",
      "Article 128:\tCompression: 0.261364\tAvg Compression: 0.173956\n",
      "Article 129:\tCompression: 0.242857\tAvg Compression: 0.173962\n",
      "Article 130:\tCompression: 0.210526\tAvg Compression: 0.173966\n",
      "Article 131:\tCompression: 0.238971\tAvg Compression: 0.173971\n",
      "Article 132:\tCompression: 0.170663\tAvg Compression: 0.173897\n",
      "Article 133:\tCompression: 0.221875\tAvg Compression: 0.173901\n",
      "Article 134:\tCompression: 0.232143\tAvg Compression: 0.173906\n",
      "Article 135:\tCompression: 0.253676\tAvg Compression: 0.173913\n",
      "Article 136:\tCompression: 0.214674\tAvg Compression: 0.173917\n",
      "Article 137:\tCompression: 0.274306\tAvg Compression: 0.173926\n",
      "Article 138:\tCompression: 0.175403\tAvg Compression: 0.173926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 139:\tCompression: 0.173697\tAvg Compression: 0.173913\n",
      "Article 140:\tCompression: 0.180190\tAvg Compression: 0.174328\n",
      "Article 141:\tCompression: 0.189798\tAvg Compression: 0.174445\n",
      "Article 142:\tCompression: 0.161267\tAvg Compression: 0.174222\n",
      "Article 143:\tCompression: 0.174685\tAvg Compression: 0.174226\n",
      "Article 144:\tCompression: 0.230769\tAvg Compression: 0.174231\n",
      "Article 145:\tCompression: 0.175165\tAvg Compression: 0.174264\n",
      "Article 146:\tCompression: 0.182842\tAvg Compression: 0.174405\n",
      "Article 147:\tCompression: 0.188155\tAvg Compression: 0.174774\n",
      "Article 148:\tCompression: 0.155593\tAvg Compression: 0.173658\n",
      "Article 149:\tCompression: 0.165172\tAvg Compression: 0.173123\n",
      "Article 150:\tCompression: 0.328571\tAvg Compression: 0.173132\n",
      "Article 151:\tCompression: 0.155807\tAvg Compression: 0.172928\n",
      "Article 152:\tCompression: 0.177003\tAvg Compression: 0.173301\n",
      "Article 153:\tCompression: 0.189852\tAvg Compression: 0.173681\n",
      "Article 154:\tCompression: 0.336957\tAvg Compression: 0.173686\n",
      "Article 155:\tCompression: 0.164526\tAvg Compression: 0.173286\n",
      "Article 156:\tCompression: 0.154096\tAvg Compression: 0.172052\n",
      "Article 157:\tCompression: 0.185000\tAvg Compression: 0.172287\n",
      "Article 158:\tCompression: 0.189324\tAvg Compression: 0.172310\n",
      "Article 159:\tCompression: 0.168701\tAvg Compression: 0.172286\n",
      "Article 160:\tCompression: 0.375000\tAvg Compression: 0.172292\n",
      "Article 161:\tCompression: 0.186008\tAvg Compression: 0.172366\n",
      "Article 162:\tCompression: 0.189382\tAvg Compression: 0.172960\n",
      "Article 163:\tCompression: 0.176713\tAvg Compression: 0.172974\n",
      "Article 164:\tCompression: 0.145848\tAvg Compression: 0.171358\n",
      "Article 165:\tCompression: 0.328488\tAvg Compression: 0.171366\n",
      "Article 166:\tCompression: 0.170982\tAvg Compression: 0.171365\n",
      "Article 167:\tCompression: 0.330000\tAvg Compression: 0.171369\n",
      "Article 168:\tCompression: 0.330000\tAvg Compression: 0.171374\n",
      "Article 169:\tCompression: 0.172802\tAvg Compression: 0.171392\n",
      "Article 170:\tCompression: 0.210565\tAvg Compression: 0.172640\n",
      "Article 171:\tCompression: 0.185814\tAvg Compression: 0.173301\n",
      "Article 172:\tCompression: 0.246429\tAvg Compression: 0.173304\n",
      "Article 173:\tCompression: 0.175028\tAvg Compression: 0.173312\n",
      "Article 174:\tCompression: 0.246795\tAvg Compression: 0.173315\n",
      "Article 175:\tCompression: 0.216755\tAvg Compression: 0.173826\n",
      "Article 176:\tCompression: 0.154561\tAvg Compression: 0.173543\n",
      "Article 177:\tCompression: 0.246212\tAvg Compression: 0.173545\n",
      "Article 178:\tCompression: 0.235714\tAvg Compression: 0.173547\n",
      "Article 179:\tCompression: 0.200658\tAvg Compression: 0.173548\n",
      "Article 180:\tCompression: 0.231618\tAvg Compression: 0.173550\n",
      "Article 181:\tCompression: 0.219697\tAvg Compression: 0.173552\n",
      "Article 182:\tCompression: 0.340909\tAvg Compression: 0.173556\n",
      "Article 183:\tCompression: 0.221875\tAvg Compression: 0.173558\n",
      "Article 184:\tCompression: 0.193750\tAvg Compression: 0.173558\n",
      "Article 185:\tCompression: 0.177326\tAvg Compression: 0.173559\n",
      "Article 186:\tCompression: 0.177326\tAvg Compression: 0.173559\n",
      "Article 187:\tCompression: 0.177326\tAvg Compression: 0.173559\n",
      "Article 188:\tCompression: 0.179680\tAvg Compression: 0.173584\n",
      "Article 189:\tCompression: 0.147134\tAvg Compression: 0.173515\n",
      "Article 190:\tCompression: 0.295139\tAvg Compression: 0.173520\n",
      "Article 191:\tCompression: 0.170300\tAvg Compression: 0.173519\n",
      "Article 192:\tCompression: 0.180804\tAvg Compression: 0.173519\n",
      "Article 193:\tCompression: 0.176520\tAvg Compression: 0.173680\n",
      "Article 194:\tCompression: 0.192535\tAvg Compression: 0.174139\n",
      "Article 195:\tCompression: 0.187835\tAvg Compression: 0.174250\n",
      "Article 196:\tCompression: 0.352273\tAvg Compression: 0.174254\n",
      "Article 197:\tCompression: 0.368421\tAvg Compression: 0.174257\n",
      "Article 198:\tCompression: 0.168870\tAvg Compression: 0.174086\n",
      "Article 199:\tCompression: 0.156531\tAvg Compression: 0.174025\n",
      "Article 200:\tCompression: 0.349138\tAvg Compression: 0.174029\n",
      "Article 201:\tCompression: 0.183285\tAvg Compression: 0.174238\n",
      "Article 202:\tCompression: 0.170227\tAvg Compression: 0.174174\n",
      "Article 203:\tCompression: 0.320312\tAvg Compression: 0.174178\n",
      "Article 204:\tCompression: 0.186036\tAvg Compression: 0.174200\n",
      "Article 205:\tCompression: 0.141390\tAvg Compression: 0.174143\n",
      "Article 206:\tCompression: 0.185153\tAvg Compression: 0.174284\n",
      "Article 207:\tCompression: 0.175959\tAvg Compression: 0.174288\n",
      "Article 208:\tCompression: 0.277778\tAvg Compression: 0.174291\n",
      "Article 209:\tCompression: 0.257143\tAvg Compression: 0.174294\n",
      "Article 210:\tCompression: 0.257143\tAvg Compression: 0.174296\n",
      "Article 211:\tCompression: 0.257143\tAvg Compression: 0.174299\n",
      "Article 212:\tCompression: 0.170131\tAvg Compression: 0.174167\n",
      "Article 213:\tCompression: 0.169133\tAvg Compression: 0.174121\n",
      "Article 214:\tCompression: 0.189529\tAvg Compression: 0.174149\n",
      "Article 215:\tCompression: 0.482143\tAvg Compression: 0.174155\n",
      "Article 216:\tCompression: 0.225694\tAvg Compression: 0.174156\n",
      "Article 217:\tCompression: 0.225694\tAvg Compression: 0.174158\n",
      "Article 218:\tCompression: 0.257812\tAvg Compression: 0.174160\n",
      "Article 219:\tCompression: 0.356250\tAvg Compression: 0.174163\n",
      "Article 220:\tCompression: 0.194037\tAvg Compression: 0.174195\n",
      "Article 221:\tCompression: 0.338542\tAvg Compression: 0.174199\n",
      "Article 222:\tCompression: 0.207501\tAvg Compression: 0.174444\n",
      "Article 223:\tCompression: 0.246429\tAvg Compression: 0.174446\n",
      "Article 224:\tCompression: 0.197929\tAvg Compression: 0.174647\n",
      "Article 225:\tCompression: 0.168656\tAvg Compression: 0.174645\n",
      "Article 226:\tCompression: 0.193627\tAvg Compression: 0.174818\n",
      "Article 227:\tCompression: 0.169331\tAvg Compression: 0.174758\n",
      "Article 228:\tCompression: 0.186511\tAvg Compression: 0.174787\n",
      "Article 229:\tCompression: 0.253378\tAvg Compression: 0.174789\n",
      "Article 230:\tCompression: 0.159639\tAvg Compression: 0.174745\n",
      "Article 231:\tCompression: 0.179817\tAvg Compression: 0.174912\n",
      "Article 232:\tCompression: 0.155394\tAvg Compression: 0.174868\n",
      "Article 233:\tCompression: 0.183287\tAvg Compression: 0.175041\n",
      "Article 234:\tCompression: 0.180662\tAvg Compression: 0.175231\n",
      "Article 235:\tCompression: 0.183871\tAvg Compression: 0.175294\n",
      "Article 236:\tCompression: 0.185559\tAvg Compression: 0.175321\n",
      "Article 237:\tCompression: 0.168429\tAvg Compression: 0.175300\n",
      "Article 238:\tCompression: 0.192221\tAvg Compression: 0.175320\n",
      "Article 239:\tCompression: 0.355000\tAvg Compression: 0.175323\n",
      "Article 240:\tCompression: 0.160195\tAvg Compression: 0.175126\n",
      "Article 241:\tCompression: 0.175921\tAvg Compression: 0.175128\n",
      "Article 242:\tCompression: 0.178265\tAvg Compression: 0.175145\n",
      "Article 243:\tCompression: 0.190694\tAvg Compression: 0.175166\n",
      "Article 244:\tCompression: 0.187252\tAvg Compression: 0.175224\n",
      "Article 245:\tCompression: 0.187272\tAvg Compression: 0.175259\n",
      "Article 246:\tCompression: 0.361607\tAvg Compression: 0.175263\n",
      "Article 247:\tCompression: 0.170176\tAvg Compression: 0.175260\n",
      "Article 248:\tCompression: 0.189662\tAvg Compression: 0.175325\n",
      "Article 249:\tCompression: 0.182417\tAvg Compression: 0.175336\n",
      "Article 250:\tCompression: 0.204176\tAvg Compression: 0.175433\n",
      "Article 251:\tCompression: 0.178086\tAvg Compression: 0.175435\n",
      "Article 252:\tCompression: 0.365741\tAvg Compression: 0.175439\n",
      "Article 253:\tCompression: 0.205646\tAvg Compression: 0.175876\n",
      "Article 254:\tCompression: 0.292857\tAvg Compression: 0.175879\n",
      "Article 255:\tCompression: 0.277778\tAvg Compression: 0.175881\n",
      "Article 256:\tCompression: 0.162441\tAvg Compression: 0.175665\n",
      "Article 257:\tCompression: 0.152751\tAvg Compression: 0.175469\n",
      "Article 258:\tCompression: 0.161295\tAvg Compression: 0.175318\n",
      "Article 259:\tCompression: 0.425000\tAvg Compression: 0.175321\n",
      "Article 260:\tCompression: 0.260638\tAvg Compression: 0.175323\n",
      "Article 261:\tCompression: 0.179397\tAvg Compression: 0.175351\n",
      "Article 262:\tCompression: 0.225271\tAvg Compression: 0.175401\n",
      "Article 263:\tCompression: 0.177215\tAvg Compression: 0.175424\n",
      "Article 264:\tCompression: 0.162697\tAvg Compression: 0.175234\n",
      "Article 265:\tCompression: 0.369792\tAvg Compression: 0.175237\n",
      "Article 266:\tCompression: 0.173540\tAvg Compression: 0.175209\n",
      "Article 267:\tCompression: 0.151617\tAvg Compression: 0.174906\n",
      "Article 268:\tCompression: 0.246094\tAvg Compression: 0.174907\n",
      "Article 269:\tCompression: 0.179732\tAvg Compression: 0.174955\n",
      "Article 270:\tCompression: 0.147677\tAvg Compression: 0.174868\n",
      "Article 271:\tCompression: 0.153298\tAvg Compression: 0.174812\n",
      "Article 272:\tCompression: 0.174898\tAvg Compression: 0.174812\n",
      "Article 273:\tCompression: 0.159359\tAvg Compression: 0.174802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 274:\tCompression: 0.156826\tAvg Compression: 0.174778\n",
      "Article 275:\tCompression: 0.167002\tAvg Compression: 0.174773\n",
      "Article 276:\tCompression: 0.135851\tAvg Compression: 0.174727\n",
      "Article 277:\tCompression: 0.165159\tAvg Compression: 0.174672\n",
      "Article 278:\tCompression: 0.198008\tAvg Compression: 0.174806\n",
      "Article 279:\tCompression: 0.193740\tAvg Compression: 0.174925\n",
      "Article 280:\tCompression: 0.317708\tAvg Compression: 0.174927\n",
      "Article 281:\tCompression: 0.172388\tAvg Compression: 0.174874\n",
      "Article 282:\tCompression: 0.184721\tAvg Compression: 0.174896\n",
      "Article 283:\tCompression: 0.195087\tAvg Compression: 0.175084\n",
      "Article 284:\tCompression: 0.190909\tAvg Compression: 0.175290\n",
      "Article 285:\tCompression: 0.217391\tAvg Compression: 0.175291\n",
      "Article 286:\tCompression: 0.235714\tAvg Compression: 0.175292\n",
      "Article 287:\tCompression: 0.235294\tAvg Compression: 0.175382\n",
      "Article 288:\tCompression: 0.175918\tAvg Compression: 0.175383\n",
      "Article 289:\tCompression: 0.235714\tAvg Compression: 0.175384\n",
      "Article 290:\tCompression: 0.213095\tAvg Compression: 0.175479\n",
      "Article 291:\tCompression: 0.352273\tAvg Compression: 0.175481\n",
      "Article 292:\tCompression: 0.168680\tAvg Compression: 0.175244\n",
      "Article 293:\tCompression: 0.172873\tAvg Compression: 0.175173\n",
      "Article 294:\tCompression: 0.162182\tAvg Compression: 0.175025\n",
      "Article 295:\tCompression: 0.198198\tAvg Compression: 0.175211\n",
      "Article 296:\tCompression: 0.168347\tAvg Compression: 0.175101\n",
      "Article 297:\tCompression: 0.355000\tAvg Compression: 0.175103\n",
      "Article 298:\tCompression: 0.257143\tAvg Compression: 0.175104\n",
      "Article 299:\tCompression: 0.158455\tAvg Compression: 0.174817\n",
      "Article 300:\tCompression: 0.163237\tAvg Compression: 0.174666\n",
      "Article 301:\tCompression: 0.159158\tAvg Compression: 0.174520\n",
      "Article 302:\tCompression: 0.178511\tAvg Compression: 0.174549\n",
      "Article 303:\tCompression: 0.331731\tAvg Compression: 0.174551\n",
      "Article 304:\tCompression: 0.203324\tAvg Compression: 0.174949\n",
      "Article 305:\tCompression: 0.174980\tAvg Compression: 0.174950\n",
      "Article 306:\tCompression: 0.195927\tAvg Compression: 0.175041\n",
      "Article 307:\tCompression: 0.235294\tAvg Compression: 0.175042\n",
      "Article 308:\tCompression: 0.157258\tAvg Compression: 0.175042\n",
      "Article 309:\tCompression: 0.206250\tAvg Compression: 0.175042\n",
      "Article 310:\tCompression: 0.253676\tAvg Compression: 0.175044\n",
      "Article 311:\tCompression: 0.191860\tAvg Compression: 0.175044\n",
      "Article 312:\tCompression: 0.207237\tAvg Compression: 0.175045\n",
      "Article 313:\tCompression: 0.152344\tAvg Compression: 0.175044\n",
      "Article 314:\tCompression: 0.234848\tAvg Compression: 0.175045\n",
      "Article 315:\tCompression: 0.225000\tAvg Compression: 0.175046\n",
      "Article 316:\tCompression: 0.191860\tAvg Compression: 0.175046\n",
      "Article 317:\tCompression: 0.191872\tAvg Compression: 0.175068\n",
      "Article 318:\tCompression: 0.183617\tAvg Compression: 0.175290\n",
      "Article 319:\tCompression: 0.191495\tAvg Compression: 0.175306\n",
      "Article 320:\tCompression: 0.345833\tAvg Compression: 0.175308\n",
      "Article 321:\tCompression: 0.192626\tAvg Compression: 0.175314\n",
      "Article 322:\tCompression: 0.172166\tAvg Compression: 0.175301\n",
      "Article 323:\tCompression: 0.162394\tAvg Compression: 0.175075\n",
      "Article 324:\tCompression: 0.169218\tAvg Compression: 0.174957\n",
      "Article 325:\tCompression: 0.178483\tAvg Compression: 0.174962\n",
      "Article 326:\tCompression: 0.174603\tAvg Compression: 0.174955\n",
      "Article 327:\tCompression: 0.155365\tAvg Compression: 0.174770\n",
      "Article 328:\tCompression: 0.223684\tAvg Compression: 0.174771\n",
      "Article 329:\tCompression: 0.201335\tAvg Compression: 0.174784\n",
      "Article 330:\tCompression: 0.238119\tAvg Compression: 0.174797\n",
      "Article 331:\tCompression: 0.177623\tAvg Compression: 0.174800\n",
      "Article 332:\tCompression: 0.215480\tAvg Compression: 0.174823\n",
      "Article 333:\tCompression: 0.183068\tAvg Compression: 0.175020\n",
      "Article 334:\tCompression: 0.176521\tAvg Compression: 0.175025\n",
      "Article 335:\tCompression: 0.176057\tAvg Compression: 0.175030\n",
      "Article 336:\tCompression: 0.176333\tAvg Compression: 0.175032\n",
      "Article 337:\tCompression: 0.198977\tAvg Compression: 0.175059\n",
      "Article 338:\tCompression: 0.172543\tAvg Compression: 0.175058\n",
      "Article 339:\tCompression: 0.180710\tAvg Compression: 0.175067\n",
      "Article 340:\tCompression: 0.187249\tAvg Compression: 0.175212\n",
      "Article 341:\tCompression: 0.166933\tAvg Compression: 0.175207\n",
      "Article 342:\tCompression: 0.203038\tAvg Compression: 0.175234\n",
      "Article 343:\tCompression: 0.309028\tAvg Compression: 0.175236\n",
      "Article 344:\tCompression: 0.156150\tAvg Compression: 0.175157\n",
      "Article 345:\tCompression: 0.156987\tAvg Compression: 0.175056\n",
      "Article 346:\tCompression: 0.183904\tAvg Compression: 0.175147\n",
      "Article 347:\tCompression: 0.210692\tAvg Compression: 0.175149\n",
      "Article 348:\tCompression: 0.201676\tAvg Compression: 0.175351\n",
      "Article 349:\tCompression: 0.174722\tAvg Compression: 0.175344\n",
      "Article 350:\tCompression: 0.393750\tAvg Compression: 0.175346\n",
      "Article 351:\tCompression: 0.414474\tAvg Compression: 0.175348\n",
      "Article 352:\tCompression: 0.202138\tAvg Compression: 0.175681\n",
      "Article 353:\tCompression: 0.225000\tAvg Compression: 0.175682\n",
      "Article 354:\tCompression: 0.176006\tAvg Compression: 0.175687\n",
      "Article 355:\tCompression: 0.188713\tAvg Compression: 0.175715\n",
      "Article 356:\tCompression: 0.178910\tAvg Compression: 0.175720\n",
      "Article 357:\tCompression: 0.275943\tAvg Compression: 0.175722\n",
      "Article 358:\tCompression: 0.146429\tAvg Compression: 0.175721\n",
      "Article 359:\tCompression: 0.152778\tAvg Compression: 0.175720\n",
      "Article 360:\tCompression: 0.207237\tAvg Compression: 0.175721\n",
      "Article 361:\tCompression: 0.187500\tAvg Compression: 0.175721\n",
      "Article 362:\tCompression: 0.148214\tAvg Compression: 0.175720\n",
      "Article 363:\tCompression: 0.187500\tAvg Compression: 0.175721\n",
      "Article 364:\tCompression: 0.137987\tAvg Compression: 0.175720\n",
      "Article 365:\tCompression: 0.147887\tAvg Compression: 0.175719\n",
      "Article 366:\tCompression: 0.129688\tAvg Compression: 0.175717\n",
      "Article 367:\tCompression: 0.326923\tAvg Compression: 0.175719\n",
      "Article 368:\tCompression: 0.181730\tAvg Compression: 0.175729\n",
      "Article 369:\tCompression: 0.154797\tAvg Compression: 0.175680\n",
      "Article 370:\tCompression: 0.239286\tAvg Compression: 0.175681\n",
      "Article 371:\tCompression: 0.210526\tAvg Compression: 0.175681\n",
      "Article 372:\tCompression: 0.216912\tAvg Compression: 0.175681\n",
      "Article 373:\tCompression: 0.223485\tAvg Compression: 0.175682\n",
      "Article 374:\tCompression: 0.218750\tAvg Compression: 0.175683\n",
      "Article 375:\tCompression: 0.187500\tAvg Compression: 0.175683\n",
      "Article 376:\tCompression: 0.186047\tAvg Compression: 0.175683\n",
      "Article 377:\tCompression: 0.224265\tAvg Compression: 0.175684\n",
      "Article 378:\tCompression: 0.186047\tAvg Compression: 0.175684\n",
      "Article 379:\tCompression: 0.234848\tAvg Compression: 0.175684\n",
      "Article 380:\tCompression: 0.199468\tAvg Compression: 0.175685\n",
      "Article 381:\tCompression: 0.321023\tAvg Compression: 0.175687\n",
      "Article 382:\tCompression: 0.261364\tAvg Compression: 0.175688\n",
      "Article 383:\tCompression: 0.162532\tAvg Compression: 0.175599\n",
      "Article 384:\tCompression: 0.192036\tAvg Compression: 0.175773\n",
      "Article 385:\tCompression: 0.196905\tAvg Compression: 0.175808\n",
      "Article 386:\tCompression: 0.195852\tAvg Compression: 0.175888\n",
      "Article 387:\tCompression: 0.182060\tAvg Compression: 0.175924\n",
      "Article 388:\tCompression: 0.188574\tAvg Compression: 0.175927\n",
      "Article 389:\tCompression: 0.191918\tAvg Compression: 0.176065\n",
      "Article 390:\tCompression: 0.208829\tAvg Compression: 0.176357\n",
      "Article 391:\tCompression: 0.183198\tAvg Compression: 0.176398\n",
      "Article 392:\tCompression: 0.210907\tAvg Compression: 0.176572\n",
      "Article 393:\tCompression: 0.175675\tAvg Compression: 0.176569\n",
      "Article 394:\tCompression: 0.161879\tAvg Compression: 0.176468\n",
      "Article 395:\tCompression: 0.263303\tAvg Compression: 0.176745\n",
      "Article 396:\tCompression: 0.193384\tAvg Compression: 0.176804\n",
      "Article 397:\tCompression: 0.312500\tAvg Compression: 0.176805\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-6687124798d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mraw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mencoded_article\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubword_text_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34mb'\\0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mcompressed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhuffman_archive_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded_article\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtotal_raw\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtotal_compressed\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcompressed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-d7cc20eea528>\u001b[0m in \u001b[0;36mhuffman_archive_size\u001b[1;34m(model, text)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbyte\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# remove the batch dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    821\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 822\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    265\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[1;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    715\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m    716\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[1;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m           \u001b[1;31m# Compute outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m           \u001b[0moutput_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m     \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    821\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 822\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcan_use_gpu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m           last_output, outputs, new_h, new_c, runtime = cudnn_lstm(\n\u001b[1;32m-> 1144\u001b[1;33m               **cudnn_lstm_kwargs)\n\u001b[0m\u001b[0;32m   1145\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m           last_output, outputs, new_h, new_c, runtime = standard_lstm(\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mcudnn_lstm\u001b[1;34m(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths)\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m   \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1331\u001b[1;33m   \u001b[0mweights\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecurrent_kernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m   \u001b[1;31m# CuDNN has an extra set of bias for inputs, we disable them (setting to 0),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m   \u001b[1;31m# so that mathematically it is same as the canonical LSTM implementation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(value, num_or_size_splits, axis, num, name)\u001b[0m\n\u001b[0;32m   1779\u001b[0m                 six.integer_types + (tensor_shape.Dimension,)):\n\u001b[0;32m   1780\u001b[0m     return gen_array_ops.split(\n\u001b[1;32m-> 1781\u001b[1;33m         axis=axis, num_split=num_or_size_splits, value=value, name=name)\n\u001b[0m\u001b[0;32m   1782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1783\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0msize_splits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joank_000\\desktop\\rnn-enwik-predictor\\env\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(axis, value, num_split, name)\u001b[0m\n\u001b[0;32m   9064\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m   9065\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Split\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9066\u001b[1;33m         tld.op_callbacks, axis, value, \"num_split\", num_split)\n\u001b[0m\u001b[0;32m   9067\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9068\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_raw = 0\n",
    "total_compressed = 0\n",
    "\n",
    "for index, article in enumerate(articles()):\n",
    "    raw = (len(article) + 1) * 8\n",
    "    encoded_article = np.array(subword_text_encoder.encode(article + b'\\0'), dtype=TYPE)\n",
    "    compressed, _ = huffman_archive_size(model, encoded_article)\n",
    "    total_raw += raw\n",
    "    total_compressed += compressed\n",
    "    print('Article %d:\\tCompression: %f\\tAvg Compression: %f' % (index, compressed/raw, total_compressed/total_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаваме компресия `~ 0.176`. По-зле от преди. Но пък и обучавахме по-малък брой епохи и имахме по-висок loss.\n",
    "\n",
    "За сметка на това всяка от по-дългите епохи доведве до по-нисък loss за сметка на по-дълго изчисление. Нищо от това не е изненадващо. Просто проверяваме, че сме на прав път."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
