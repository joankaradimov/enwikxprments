{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('page_revisions_text', 'rb') as text_file:\n",
    "    raw_data = text_file.read()\n",
    "    data = raw_data.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "885671734"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "888134586"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134044"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index('&amp;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1079"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index(';&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5;&#957;&#94'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1078:1090]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134048"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index(';#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "';#65;</tt>\" and \"<tt>&amp;#97;</'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[134048:134080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('&(((#\\d+)|([A-Za-z]+));&?)+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(1074, 1116), match='&#945;&#957;&#945;&#961;&#967;&#943;&#945;'>\n",
      "<re.Match object; span=(1117, 1159), match='&#945;&#957;&#945;&#961;&#967;&#943;&#945;'>\n",
      "<re.Match object; span=(36643, 36650), match='&mdash;'>\n",
      "<re.Match object; span=(36670, 36677), match='&mdash;'>\n",
      "<re.Match object; span=(54443, 54450), match='&mdash;'>\n",
      "<re.Match object; span=(62614, 62652), match='&alpha;&upsilon;&tau;&omicron;&sigmaf;'>\n",
      "<re.Match object; span=(63150, 63157), match='&mdash;'>\n",
      "<re.Match object; span=(65762, 65769), match='&mdash;'>\n",
      "<re.Match object; span=(70392, 70399), match='&mdash;'>\n",
      "<re.Match object; span=(72051, 72058), match='&mdash;'>\n"
     ]
    }
   ],
   "source": [
    "for match in itertools.islice(pattern.finditer(data), 0, 10):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unescape_html_symbol(escaped):\n",
    "    escaped_symbols = escaped.group(0).split(';')\n",
    "    escaped_symbols = escaped_symbols[:-1] # strip the last string empty string\n",
    "    escaped_symbols = (escaped_symbol + ';' for escaped_symbol in escaped_symbols)\n",
    "    result = ''\n",
    "\n",
    "    for escaped_symbol in escaped_symbols:\n",
    "        code = escaped_symbol[1:] if escaped_symbol[0] == '&' else escaped_symbol\n",
    "        \n",
    "        if code[0] == '#':\n",
    "            result += chr(int(code[1:-1]))\n",
    "        elif code in html.entities.html5:\n",
    "            result += html.entities.html5[code]\n",
    "        else:\n",
    "            result += escaped_symbol\n",
    "\n",
    "    return result\n",
    "\n",
    "replaced = pattern.sub(unescape_html_symbol, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(156320906, 156320912), match='&MASK;'>\n",
      "<re.Match object; span=(156321260, 156321266), match='&MASK;'>\n",
      "<re.Match object; span=(162356996, 162357003), match='&mdsah;'>\n",
      "<re.Match object; span=(185581505, 185581511), match='&head;'>\n",
      "<re.Match object; span=(199930604, 199930611), match='&ldash;'>\n",
      "<re.Match object; span=(229891451, 229891458), match='&bdash;'>\n",
      "<re.Match object; span=(240661658, 240661661), match='&i;'>\n",
      "<re.Match object; span=(296127899, 296127905), match='&nbus;'>\n",
      "<re.Match object; span=(305278060, 305278067), match='&dbquo;'>\n",
      "<re.Match object; span=(312692541, 312692550), match='&opslash;'>\n",
      "<re.Match object; span=(322957979, 322957985), match='&bnsp;'>\n",
      "<re.Match object; span=(322958348, 322958354), match='&bnsp;'>\n",
      "<re.Match object; span=(322958790, 322958796), match='&bnsp;'>\n",
      "<re.Match object; span=(340822501, 340822507), match='&mash;'>\n",
      "<re.Match object; span=(350179634, 350179642), match='&gcaron;'>\n",
      "<re.Match object; span=(350179647, 350179655), match='&gcaron;'>\n",
      "<re.Match object; span=(364830728, 364830736), match='&ndashl;'>\n",
      "<re.Match object; span=(461268047, 461268050), match='&M;'>\n",
      "<re.Match object; span=(615467310, 615467313), match='&W;'>\n",
      "<re.Match object; span=(627381824, 627381830), match='&nash;'>\n",
      "<re.Match object; span=(640097594, 640097600), match='&mash;'>\n",
      "<re.Match object; span=(730139175, 730139182), match='&nbasp;'>\n",
      "<re.Match object; span=(732608914, 732608925), match='&something;'>\n",
      "<re.Match object; span=(732609122, 732609130), match='&digits;'>\n",
      "<re.Match object; span=(753396276, 753396279), match='&T;'>\n",
      "<re.Match object; span=(769772796, 769772803), match='&laqno;'>\n",
      "<re.Match object; span=(803488417, 803488425), match='&degree;'>\n",
      "<re.Match object; span=(873171827, 873171833), match='&mdsh;'>\n"
     ]
    }
   ],
   "source": [
    "for match in pattern.finditer(replaced):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "883108375"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10797"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12092"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(replaced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10332"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(data.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11557"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(replaced.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(replaced.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(replaced.lower().encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "886243546"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replaced.lower().encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('page_revisions_text_simplified', 'wb') as text_file:\n",
    "    text_file.write(replaced.lower().encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_revisions_text_simplified = replaced.lower().encode('utf-8').split(b'\\0')\n",
    "\n",
    "subword_text_encoder = tfds.features.text.SubwordTextEncoder.build_from_corpus(page_revisions_text_simplified, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3803"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subword_text_encoder.subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the_',\n",
       " ', ',\n",
       " ' [[',\n",
       " 'of_',\n",
       " 's_',\n",
       " 'and_',\n",
       " ']] ',\n",
       " 'a_',\n",
       " '. ',\n",
       " 'in_',\n",
       " 'to_',\n",
       " 'is_',\n",
       " ' (',\n",
       " 'the',\n",
       " ']], ',\n",
       " 'd_',\n",
       " 'ed_',\n",
       " '[[',\n",
       " 'for_',\n",
       " ']]\\n[[',\n",
       " 'y_',\n",
       " 'ing_',\n",
       " 'was_',\n",
       " 'e_',\n",
       " 'are_',\n",
       " 'as_',\n",
       " 'on_',\n",
       " '% ',\n",
       " '.  ',\n",
       " 'with_',\n",
       " 'in',\n",
       " 'by_',\n",
       " 'that_',\n",
       " ')|',\n",
       " ']]',\n",
       " 'from_',\n",
       " 'of',\n",
       " 'ly_',\n",
       " 'it_',\n",
       " 'an_',\n",
       " 'or_',\n",
       " '.\\n\\n',\n",
       " ']], [[',\n",
       " 'and',\n",
       " 't_',\n",
       " 'http',\n",
       " '://',\n",
       " 'category',\n",
       " 'at_',\n",
       " 'on',\n",
       " 'his_',\n",
       " 'he_',\n",
       " \" ''\",\n",
       " ') ',\n",
       " 'census',\n",
       " 'er',\n",
       " ' \"',\n",
       " 'have_',\n",
       " 'es',\n",
       " 'be_',\n",
       " 'which_',\n",
       " '\" ',\n",
       " 'al_',\n",
       " 'or',\n",
       " 'n_',\n",
       " ': ',\n",
       " 'sup2',\n",
       " 'www',\n",
       " 'this_',\n",
       " 'l_',\n",
       " ']]. ',\n",
       " 'r_',\n",
       " 'there_',\n",
       " 'age_',\n",
       " 'ed',\n",
       " \"'' \",\n",
       " 'ing',\n",
       " \"'''\",\n",
       " ']] (',\n",
       " 'es_',\n",
       " \"''\",\n",
       " 'to',\n",
       " 'td',\n",
       " 'er_',\n",
       " ']]\\n*[[',\n",
       " 'mi',\n",
       " 'has_',\n",
       " '</',\n",
       " '=\"',\n",
       " 'is',\n",
       " ']] [[',\n",
       " 'not_',\n",
       " '{{',\n",
       " 'were_',\n",
       " 'new_',\n",
       " 'who_',\n",
       " 'as',\n",
       " '; ',\n",
       " 'also_',\n",
       " 'com',\n",
       " 'us',\n",
       " 'image',\n",
       " 'had_',\n",
       " '\\\\&undsc',\n",
       " 'one_',\n",
       " 'ng_',\n",
       " 'an',\n",
       " 'but_',\n",
       " 'km',\n",
       " 'population_',\n",
       " 'other_',\n",
       " 'al',\n",
       " 're',\n",
       " 'united_',\n",
       " 'o_',\n",
       " 'en',\n",
       " ' $',\n",
       " 'by',\n",
       " 'out_',\n",
       " 'ion_',\n",
       " 'k_',\n",
       " 'de',\n",
       " 'co',\n",
       " 'their_',\n",
       " 'all_',\n",
       " '% [[',\n",
       " 'its_',\n",
       " \"''' \",\n",
       " '10',\n",
       " 'more_',\n",
       " 'ce',\n",
       " 'under_',\n",
       " 'they_',\n",
       " 'le',\n",
       " '), ',\n",
       " 'c_',\n",
       " 'ly',\n",
       " 'county',\n",
       " 'br',\n",
       " 'ia',\n",
       " '17',\n",
       " 'do',\n",
       " 'first_',\n",
       " 'i_',\n",
       " ']]\\n* [[',\n",
       " 'na',\n",
       " 'living_',\n",
       " '16',\n",
       " 'american',\n",
       " '12',\n",
       " 'ne',\n",
       " ']].  ',\n",
       " 'm_',\n",
       " 'align',\n",
       " 'th_',\n",
       " \" ''[[\",\n",
       " 'at',\n",
       " '11',\n",
       " 've',\n",
       " 'ch',\n",
       " 'de_',\n",
       " '18_',\n",
       " 'two_',\n",
       " 'man',\n",
       " 'se',\n",
       " 'us_',\n",
       " 'st',\n",
       " '15',\n",
       " '  ',\n",
       " ']] - [[',\n",
       " 'em',\n",
       " 'da',\n",
       " 'ng',\n",
       " 'been_',\n",
       " 'some_',\n",
       " 'median_',\n",
       " '20',\n",
       " 'ion',\n",
       " \" '''\",\n",
       " 'ga',\n",
       " 'most_',\n",
       " ', [[',\n",
       " '\\n\\n',\n",
       " 'city_',\n",
       " 'are',\n",
       " 'can_',\n",
       " '14',\n",
       " 'ri',\n",
       " 'total_',\n",
       " '\\n|',\n",
       " '00',\n",
       " 'h_',\n",
       " 'ar',\n",
       " 'ra',\n",
       " 'after_',\n",
       " 'ta',\n",
       " ' = ',\n",
       " 'no_',\n",
       " 'fi',\n",
       " 'race',\n",
       " 'g_',\n",
       " 'right',\n",
       " ' - ',\n",
       " 'no',\n",
       " 'rs',\n",
       " 'used_',\n",
       " 'um',\n",
       " 'ka',\n",
       " 'ca',\n",
       " 'lo',\n",
       " '13',\n",
       " 'tion_',\n",
       " 'li',\n",
       " 'sa',\n",
       " '198',\n",
       " 'go',\n",
       " 'it',\n",
       " '; (',\n",
       " 'sub',\n",
       " 'ru',\n",
       " 'when_',\n",
       " '.\\n\\n== ',\n",
       " 'many_',\n",
       " 'years_',\n",
       " 'ha',\n",
       " '\\n*[[',\n",
       " 'ja',\n",
       " '24',\n",
       " 'te',\n",
       " 'ci',\n",
       " '197',\n",
       " 'line',\n",
       " 'jpg',\n",
       " '2000',\n",
       " 'th',\n",
       " 'such_',\n",
       " 'ho',\n",
       " 'war',\n",
       " 'ad',\n",
       " ']].\\n\\n',\n",
       " 'pa',\n",
       " 'ur',\n",
       " 'ro',\n",
       " 'lu',\n",
       " 'ce_',\n",
       " 'ul',\n",
       " '==\\n',\n",
       " '/ ',\n",
       " 'from',\n",
       " 'he',\n",
       " 'may_',\n",
       " 'ge',\n",
       " '0_',\n",
       " 'her',\n",
       " 'average_',\n",
       " '1_',\n",
       " 'into_',\n",
       " 'up_',\n",
       " 'el',\n",
       " 'le_',\n",
       " 'fe',\n",
       " 'for',\n",
       " 'ma',\n",
       " 'ment',\n",
       " 'ation_',\n",
       " 'é',\n",
       " 'ver',\n",
       " 'fa',\n",
       " 'income_',\n",
       " 'those_',\n",
       " \"'', \",\n",
       " 'mp',\n",
       " '] ',\n",
       " '65_',\n",
       " 'ers',\n",
       " '\\n| ',\n",
       " 'tr',\n",
       " '| ',\n",
       " 'hi',\n",
       " 'center',\n",
       " 'car',\n",
       " 'jo',\n",
       " 'di',\n",
       " 'only_',\n",
       " 've_',\n",
       " '.\\n\\n==',\n",
       " 'ber',\n",
       " '><',\n",
       " 'sup',\n",
       " 'ba',\n",
       " 'ki',\n",
       " 'un',\n",
       " 'ff',\n",
       " 'ko',\n",
       " 'du',\n",
       " 'nt',\n",
       " 'st_',\n",
       " '> ',\n",
       " 'org',\n",
       " 'et',\n",
       " 'made_',\n",
       " 'tion',\n",
       " '196',\n",
       " '2_',\n",
       " 'be',\n",
       " 'te_',\n",
       " '>\\n',\n",
       " ' | ',\n",
       " 'tu',\n",
       " 'town_',\n",
       " 'ste',\n",
       " 'states_',\n",
       " 'gi',\n",
       " 'son',\n",
       " 'ment_',\n",
       " '. \\n\\n',\n",
       " 'ic_',\n",
       " 'ea',\n",
       " \"'' (\",\n",
       " 'ter',\n",
       " 'over',\n",
       " 'would_',\n",
       " 'sta',\n",
       " 'these_',\n",
       " 'area_',\n",
       " 'ted_',\n",
       " 'ies_',\n",
       " 'ut',\n",
       " 'any_',\n",
       " 'va',\n",
       " '30',\n",
       " 'ts',\n",
       " 'ry',\n",
       " 'ya',\n",
       " 'her_',\n",
       " 'me',\n",
       " 'ton',\n",
       " 'inter',\n",
       " 'family_',\n",
       " 'per',\n",
       " 'than_',\n",
       " 'ti',\n",
       " 'links',\n",
       " '18',\n",
       " 'den',\n",
       " 'land',\n",
       " ']]) ',\n",
       " 'ation',\n",
       " '21',\n",
       " 'american_',\n",
       " 'every_',\n",
       " '25',\n",
       " 'si',\n",
       " 'bo',\n",
       " 'px',\n",
       " 'ac',\n",
       " '5_',\n",
       " 'hu',\n",
       " 'ic',\n",
       " '3_',\n",
       " \"]]'' \",\n",
       " 'non',\n",
       " 'user',\n",
       " 'ni',\n",
       " 'au',\n",
       " 'math',\n",
       " '. [[',\n",
       " 'known_',\n",
       " 'uk',\n",
       " 'je',\n",
       " '194',\n",
       " 'id',\n",
       " 'sm',\n",
       " '195',\n",
       " 'ee',\n",
       " 'p_',\n",
       " 'ins',\n",
       " 'so',\n",
       " \"''[[\",\n",
       " '22',\n",
       " 'up',\n",
       " 'il',\n",
       " 'la',\n",
       " 'ol',\n",
       " 'external_',\n",
       " \"' \",\n",
       " 'ai',\n",
       " 'pt',\n",
       " 'der',\n",
       " 'w_',\n",
       " 'pri',\n",
       " '- ',\n",
       " 'time_',\n",
       " 'water',\n",
       " 'rs_',\n",
       " 'white',\n",
       " 'pro',\n",
       " 'during_',\n",
       " 'art',\n",
       " '40',\n",
       " '4_',\n",
       " 'ies',\n",
       " 'wa',\n",
       " 'ry_',\n",
       " '>\\n<',\n",
       " 'za',\n",
       " 'mo',\n",
       " 'ap',\n",
       " 'ie',\n",
       " 'ham',\n",
       " 'op',\n",
       " 'os',\n",
       " ']]\\n',\n",
       " 'one',\n",
       " 'ke',\n",
       " 'about_',\n",
       " 'over_',\n",
       " 'har',\n",
       " 'people',\n",
       " 'ty_',\n",
       " 'ty',\n",
       " 'io',\n",
       " 'bar',\n",
       " 'ine',\n",
       " 'them',\n",
       " 'thumb',\n",
       " 'however',\n",
       " 'square_',\n",
       " 'lt',\n",
       " 'years',\n",
       " 'mer',\n",
       " 'ren',\n",
       " '23',\n",
       " 'name',\n",
       " 'net',\n",
       " ' ==\\n',\n",
       " 'ref',\n",
       " 'bu',\n",
       " 'tor',\n",
       " '100_',\n",
       " 'mon',\n",
       " '\\n* ',\n",
       " 'located_',\n",
       " 'mar',\n",
       " 'ze',\n",
       " ']]\\n\\n[[',\n",
       " '\\n*',\n",
       " 'po',\n",
       " 'se_',\n",
       " 'bri',\n",
       " '2005',\n",
       " 'land_',\n",
       " 'bra',\n",
       " 'ts_',\n",
       " '2003',\n",
       " 'states',\n",
       " 'see_',\n",
       " 'size_',\n",
       " 'sha',\n",
       " 'style',\n",
       " 'tri',\n",
       " '). ',\n",
       " '6_',\n",
       " 'ari',\n",
       " 'ir',\n",
       " 'od',\n",
       " 'bea',\n",
       " '26',\n",
       " 'can',\n",
       " 'rt',\n",
       " 'with',\n",
       " 'native_',\n",
       " '8_',\n",
       " 'che',\n",
       " 'nd',\n",
       " 'ten',\n",
       " 'pe',\n",
       " 'ers_',\n",
       " 'end',\n",
       " 'ger',\n",
       " 'zi',\n",
       " ']] - ',\n",
       " 'below_',\n",
       " '= ',\n",
       " 'world_',\n",
       " 'ck',\n",
       " '28',\n",
       " ' || ',\n",
       " 'fr',\n",
       " '27',\n",
       " 'mor',\n",
       " 'ct',\n",
       " 'she_',\n",
       " '7_',\n",
       " 'nt_',\n",
       " 'present',\n",
       " 'john_',\n",
       " 'x_',\n",
       " 'african_',\n",
       " '193',\n",
       " '44',\n",
       " 'ab',\n",
       " 'if_',\n",
       " 'she',\n",
       " 'en_',\n",
       " 'que',\n",
       " 'og',\n",
       " 'tal',\n",
       " 'families_',\n",
       " 'where_',\n",
       " 'ive_',\n",
       " 'races',\n",
       " 'ns',\n",
       " 'cha',\n",
       " 'bi',\n",
       " 'ent_',\n",
       " 'fo',\n",
       " 'qui',\n",
       " '>[[',\n",
       " 'hol',\n",
       " 'ec',\n",
       " 'asian',\n",
       " 'pacific_',\n",
       " 'between_',\n",
       " 'vi',\n",
       " 'redirect',\n",
       " 'older',\n",
       " 'ib',\n",
       " 'pu',\n",
       " 'household_',\n",
       " 'bb',\n",
       " 'est',\n",
       " 'national_',\n",
       " 'ven',\n",
       " '31',\n",
       " 'gra',\n",
       " 'ali',\n",
       " 'females_',\n",
       " 'ann',\n",
       " 'ow',\n",
       " 'males',\n",
       " 'ric',\n",
       " 'gen',\n",
       " '}} ',\n",
       " '19',\n",
       " 'don',\n",
       " 'small',\n",
       " 'use_',\n",
       " 'language',\n",
       " 'png',\n",
       " 'households_',\n",
       " 'ken',\n",
       " 'fu',\n",
       " 'poverty_',\n",
       " 'xi',\n",
       " '29',\n",
       " 'tan',\n",
       " 'sh',\n",
       " 'part_',\n",
       " '\\n* [[',\n",
       " 'will_',\n",
       " 'gar',\n",
       " ']]\\n|',\n",
       " 'lin',\n",
       " '* ',\n",
       " 'tt',\n",
       " 'time',\n",
       " ';).  ',\n",
       " 'gre',\n",
       " 'list_',\n",
       " 'see',\n",
       " 'ure',\n",
       " 'king_',\n",
       " 'ian',\n",
       " 'bro',\n",
       " 'per_',\n",
       " 'arm',\n",
       " 'wi',\n",
       " 'state_',\n",
       " 'oc',\n",
       " 'nic',\n",
       " 'ent',\n",
       " 'ive',\n",
       " 'dia',\n",
       " 'ye',\n",
       " '64',\n",
       " 'ate',\n",
       " 'tru',\n",
       " ']] ([[',\n",
       " 'gn',\n",
       " 'city',\n",
       " 'ge_',\n",
       " 'app',\n",
       " 'title',\n",
       " '9_',\n",
       " 'wo',\n",
       " 'music',\n",
       " 'su',\n",
       " 'ian_',\n",
       " 'ch_',\n",
       " 'while_',\n",
       " '\"|',\n",
       " 'ert',\n",
       " 'les',\n",
       " 'port',\n",
       " 'hispanic',\n",
       " 'cra',\n",
       " 'mu',\n",
       " ' [',\n",
       " 'act',\n",
       " 'ex',\n",
       " 'being_',\n",
       " 'ded_',\n",
       " '.\\n',\n",
       " '\\xa0',\n",
       " 'cal',\n",
       " 'ster',\n",
       " 'islander',\n",
       " 'air',\n",
       " 'lit',\n",
       " 'latino',\n",
       " 'pat',\n",
       " 'tar',\n",
       " 'top',\n",
       " 'west',\n",
       " 'pi',\n",
       " 'am',\n",
       " ' <',\n",
       " '2004',\n",
       " 'we',\n",
       " 'north',\n",
       " 'then_',\n",
       " '\\n\\n==',\n",
       " \"]]'\",\n",
       " 'ft',\n",
       " 'often_',\n",
       " 'tra',\n",
       " 'el_',\n",
       " 'left',\n",
       " ']\\n*[',\n",
       " 'gh',\n",
       " '192',\n",
       " 'ef',\n",
       " 'so_',\n",
       " 'ant',\n",
       " ']]\\n| ',\n",
       " '||',\n",
       " 'dis',\n",
       " 'pl',\n",
       " 'north_',\n",
       " 'gr',\n",
       " 'nn',\n",
       " 'sho',\n",
       " 'ial_',\n",
       " 'bre',\n",
       " 'cu',\n",
       " ']]\\n\\n',\n",
       " 'state',\n",
       " 'cat',\n",
       " 'south_',\n",
       " 'box',\n",
       " 'con',\n",
       " 'cor',\n",
       " 'album',\n",
       " 'ori',\n",
       " '191',\n",
       " 'led_',\n",
       " 'dep',\n",
       " 'red_',\n",
       " 'film',\n",
       " 'cre',\n",
       " 'ser',\n",
       " '35',\n",
       " 'cs',\n",
       " 'ss',\n",
       " 'vo',\n",
       " 'form',\n",
       " 'pal',\n",
       " 'pen',\n",
       " '=[[',\n",
       " 'also',\n",
       " 'ani',\n",
       " 'ms',\n",
       " 'became_',\n",
       " 'way_',\n",
       " 'ls',\n",
       " 'cho',\n",
       " '32',\n",
       " 'ak',\n",
       " 'inc',\n",
       " 'according_',\n",
       " 'both_',\n",
       " 'ku',\n",
       " 'population',\n",
       " 'ben',\n",
       " 'ob',\n",
       " 'ay',\n",
       " 'able_',\n",
       " '50',\n",
       " 'sal',\n",
       " 'des',\n",
       " 'out',\n",
       " '°',\n",
       " '}}',\n",
       " 'han',\n",
       " ']].\\n\\n==',\n",
       " 'uc',\n",
       " 'note',\n",
       " 'ist',\n",
       " 'mile',\n",
       " 'gu',\n",
       " 'ker',\n",
       " 'nu',\n",
       " 'ally_',\n",
       " 'cal_',\n",
       " 'ps',\n",
       " ']]\\n\\n==',\n",
       " '===\\n',\n",
       " 'wal',\n",
       " \"]]'' (\",\n",
       " 'ary_',\n",
       " 'main',\n",
       " '33',\n",
       " '}}\\n\\n[[',\n",
       " 'run',\n",
       " 'dan',\n",
       " ' \\n',\n",
       " 'ag',\n",
       " 'ang',\n",
       " 'show',\n",
       " 'ig',\n",
       " 'html_',\n",
       " 'through_',\n",
       " 'people_',\n",
       " 'world',\n",
       " 'page',\n",
       " 'ress',\n",
       " 'pin',\n",
       " 'hel',\n",
       " 're_',\n",
       " 'ity_',\n",
       " 'king',\n",
       " 'number_',\n",
       " 'example',\n",
       " 'pre',\n",
       " 'ard',\n",
       " 'ner',\n",
       " 'tro',\n",
       " 'wea',\n",
       " 'sp',\n",
       " 'whe',\n",
       " 'inf',\n",
       " 'fore',\n",
       " 'men',\n",
       " 'del',\n",
       " 'usa',\n",
       " 'mb',\n",
       " 'ud',\n",
       " 'ans',\n",
       " '36',\n",
       " 'ys',\n",
       " 'news',\n",
       " 'british_',\n",
       " 'im',\n",
       " 'ret',\n",
       " 'eg',\n",
       " 'ens',\n",
       " 'let',\n",
       " 'three_',\n",
       " 'very_',\n",
       " '80',\n",
       " 'sv',\n",
       " 'min',\n",
       " 'ot',\n",
       " 'law',\n",
       " 'cell',\n",
       " 'ure_',\n",
       " '==\\n\\n',\n",
       " 'shi',\n",
       " 'under',\n",
       " 'war_',\n",
       " 'later_',\n",
       " 'because_',\n",
       " 'ok',\n",
       " 'name_',\n",
       " 'cy',\n",
       " 'ds',\n",
       " '|\\n',\n",
       " ')]]',\n",
       " 'together',\n",
       " '60',\n",
       " 'well_',\n",
       " '.\\n\\n===',\n",
       " 'ana',\n",
       " 'mal',\n",
       " ']\\n* [',\n",
       " 'spe',\n",
       " 'age',\n",
       " 'tom',\n",
       " 'him_',\n",
       " 'west_',\n",
       " 'gui',\n",
       " ';) ',\n",
       " 'geo',\n",
       " 'work',\n",
       " 'rep',\n",
       " 'fin',\n",
       " 'hen',\n",
       " \"]]'', \",\n",
       " 'fact',\n",
       " 'sto',\n",
       " 'several_',\n",
       " 'win',\n",
       " 'appear',\n",
       " 'dom',\n",
       " 'rd',\n",
       " 'dar',\n",
       " 'man_',\n",
       " 'pol',\n",
       " 'nl',\n",
       " \"''' (\",\n",
       " \", ''\",\n",
       " 'code',\n",
       " 'cc',\n",
       " 'county_',\n",
       " ']]: ',\n",
       " '34',\n",
       " '37',\n",
       " 'van',\n",
       " '38',\n",
       " 'll',\n",
       " 'chi',\n",
       " 'sc',\n",
       " 'what_',\n",
       " 'ar_',\n",
       " 'tic',\n",
       " 'university_',\n",
       " 'mark',\n",
       " 'hand',\n",
       " 'ting_',\n",
       " 'col',\n",
       " 'sol',\n",
       " '190',\n",
       " 'ii',\n",
       " 'oni',\n",
       " 'wil',\n",
       " 'nce',\n",
       " 'kn',\n",
       " 'vin',\n",
       " 'rest',\n",
       " 'lle',\n",
       " 'mac',\n",
       " 'tur',\n",
       " 'dra',\n",
       " 'disc',\n",
       " 'history',\n",
       " 'ne_',\n",
       " 'qua',\n",
       " 'ui',\n",
       " 'york',\n",
       " 'eo',\n",
       " 'read',\n",
       " 'eri',\n",
       " 'same_',\n",
       " 'ae',\n",
       " 'sea',\n",
       " 'do_',\n",
       " '2002',\n",
       " '|| ',\n",
       " 'ter_',\n",
       " 'nti',\n",
       " 'sel',\n",
       " 'since_',\n",
       " 'song',\n",
       " 'ris',\n",
       " '\", ',\n",
       " 'ble_',\n",
       " 'each_',\n",
       " 'tho',\n",
       " 'before_',\n",
       " '}}\\n\\n',\n",
       " 'bit',\n",
       " 'ria',\n",
       " 'ism',\n",
       " 'not',\n",
       " 'ph',\n",
       " 'ous_',\n",
       " 'list',\n",
       " 'ei',\n",
       " 'record',\n",
       " '41',\n",
       " \" '\",\n",
       " 'year',\n",
       " 'ise',\n",
       " 'om',\n",
       " 'ate_',\n",
       " 'lan',\n",
       " 'ew',\n",
       " 'um_',\n",
       " 'val',\n",
       " 'bel',\n",
       " 'ran',\n",
       " 'park',\n",
       " 'france',\n",
       " 'ity',\n",
       " 'rate',\n",
       " 'mc',\n",
       " 'end_',\n",
       " ' ([[',\n",
       " ' = [[',\n",
       " 'wood',\n",
       " 'player',\n",
       " 'play',\n",
       " 'early_',\n",
       " 'school',\n",
       " 'bon',\n",
       " 'ora',\n",
       " 'ish',\n",
       " '39',\n",
       " 'base',\n",
       " 'die',\n",
       " '–',\n",
       " 'map',\n",
       " 'kin',\n",
       " 'god',\n",
       " 'even_',\n",
       " 'bc',\n",
       " 'ss_',\n",
       " ')\\n*',\n",
       " 'region',\n",
       " 'ship',\n",
       " 'lar',\n",
       " 'bal',\n",
       " 'ici',\n",
       " 'although_',\n",
       " 'trans',\n",
       " 'wer',\n",
       " 'ler',\n",
       " 'nat',\n",
       " 'arr',\n",
       " ' \"[[',\n",
       " '></',\n",
       " 'children_',\n",
       " 'cli',\n",
       " 'nor',\n",
       " 'start',\n",
       " 'son_',\n",
       " 'ue',\n",
       " 'mate',\n",
       " ']]), ',\n",
       " 'count',\n",
       " 'stand',\n",
       " 'star',\n",
       " 'ence_',\n",
       " 'result',\n",
       " '90',\n",
       " 'spo',\n",
       " 'uni',\n",
       " 'ose',\n",
       " 'ning_',\n",
       " 'las',\n",
       " 'change',\n",
       " '42',\n",
       " 'bor',\n",
       " '==',\n",
       " 'stor',\n",
       " 'married_',\n",
       " 'f_',\n",
       " ' & ',\n",
       " 'gan',\n",
       " '.\\n*',\n",
       " 'vis',\n",
       " ')\\n*[[',\n",
       " 'all',\n",
       " 'marriage',\n",
       " 'ute',\n",
       " 'much_',\n",
       " 'post',\n",
       " 'ort',\n",
       " '25_',\n",
       " 'hun',\n",
       " 'century',\n",
       " 'rn',\n",
       " 'use',\n",
       " 'tre',\n",
       " 'bur',\n",
       " 'could_',\n",
       " 'tle',\n",
       " 'high_',\n",
       " 'ling_',\n",
       " 'sign',\n",
       " 'like_',\n",
       " 'direct',\n",
       " 'now_',\n",
       " 'rm',\n",
       " 'b_',\n",
       " 'view',\n",
       " 'rec',\n",
       " 'england',\n",
       " 'sit',\n",
       " ...]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword_text_encoder.subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword_text_encoder.save_to_file('vocab_4096_simplified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279450198"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(subword_text_encoder.encode(x)) for x in page_revisions_text_simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword_text_encoder = tfds.features.text.SubwordTextEncoder.build_from_corpus(page_revisions_text_simplified, 8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7912"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subword_text_encoder.subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword_text_encoder.save_to_file('vocab_8192_simplified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the_',\n",
       " ', ',\n",
       " ' [[',\n",
       " 'of_',\n",
       " 'and_',\n",
       " 's_',\n",
       " ']] ',\n",
       " 'a_',\n",
       " 'in_',\n",
       " '. ',\n",
       " 'to_',\n",
       " 'is_',\n",
       " ' (',\n",
       " 'the',\n",
       " ']], ',\n",
       " 'for_',\n",
       " ']]\\n[[',\n",
       " 'was_',\n",
       " 'are_',\n",
       " 'as_',\n",
       " 'ed_',\n",
       " '% ',\n",
       " '.  ',\n",
       " 'with_',\n",
       " 'on_',\n",
       " 'ing_',\n",
       " 'd_',\n",
       " 'by_',\n",
       " 'that_',\n",
       " ')|',\n",
       " 'in',\n",
       " 'from_',\n",
       " 'of',\n",
       " '[[',\n",
       " 'it_',\n",
       " ']]',\n",
       " 'e_',\n",
       " ']], [[',\n",
       " '.\\n\\n',\n",
       " 'or_',\n",
       " 'an_',\n",
       " 'and',\n",
       " 'http',\n",
       " '://',\n",
       " 'category',\n",
       " 'at_',\n",
       " 'his_',\n",
       " 'he_',\n",
       " 'y_',\n",
       " 'census',\n",
       " \" ''\",\n",
       " ' \"',\n",
       " 'have_',\n",
       " 'which_',\n",
       " 'be_',\n",
       " '\" ',\n",
       " 'sup2',\n",
       " 'this_',\n",
       " 'www',\n",
       " 'on',\n",
       " ']]. ',\n",
       " ') ',\n",
       " 'there_',\n",
       " 'age_',\n",
       " 'ly_',\n",
       " ': ',\n",
       " ']] (',\n",
       " 'td',\n",
       " 't_',\n",
       " ']]\\n*[[',\n",
       " 'has_',\n",
       " \"'' \",\n",
       " 'n_',\n",
       " '=\"',\n",
       " 'es',\n",
       " 'er',\n",
       " ']] [[',\n",
       " 'not_',\n",
       " '</',\n",
       " 'were_',\n",
       " 'ed',\n",
       " 'is',\n",
       " 'new_',\n",
       " 'who_',\n",
       " 'also_',\n",
       " 'or',\n",
       " \"''\",\n",
       " 'had_',\n",
       " 'one_',\n",
       " 'but_',\n",
       " 'population_',\n",
       " 'mi',\n",
       " 'ing',\n",
       " 'image',\n",
       " 'other_',\n",
       " 'to',\n",
       " '; ',\n",
       " 'united_',\n",
       " 'es_',\n",
       " ' $',\n",
       " 'km',\n",
       " 'com',\n",
       " 'by',\n",
       " \"'''\",\n",
       " '{{',\n",
       " 'al_',\n",
       " 'all_',\n",
       " 'their_',\n",
       " 'r_',\n",
       " '% [[',\n",
       " 'out_',\n",
       " 'more_',\n",
       " 'under_',\n",
       " 'they_',\n",
       " 'its_',\n",
       " 'us',\n",
       " 'county',\n",
       " 'as',\n",
       " 'first_',\n",
       " 'ng_',\n",
       " ']]\\n* [[',\n",
       " \"''' \",\n",
       " 'living_',\n",
       " 'br',\n",
       " 'align',\n",
       " 'er_',\n",
       " 'an',\n",
       " 'l_',\n",
       " 'two_',\n",
       " '18_',\n",
       " 'american',\n",
       " ']].  ',\n",
       " '), ',\n",
       " 'i_',\n",
       " ']] - [[',\n",
       " 'some_',\n",
       " 'o_',\n",
       " 'been_',\n",
       " 'median_',\n",
       " \" ''[[\",\n",
       " 'most_',\n",
       " 'city_',\n",
       " 'can_',\n",
       " 'de',\n",
       " 'total_',\n",
       " 'ly',\n",
       " 'de_',\n",
       " ', [[',\n",
       " 'after_',\n",
       " 'al',\n",
       " 'right',\n",
       " ' - ',\n",
       " 'us_',\n",
       " 'no_',\n",
       " '; (',\n",
       " 'when_',\n",
       " \" '''\",\n",
       " 'many_',\n",
       " 'years_',\n",
       " 'ion_',\n",
       " 'race',\n",
       " '.\\n\\n== ',\n",
       " 'jpg',\n",
       " 'such_',\n",
       " '00',\n",
       " 'may_',\n",
       " 'st',\n",
       " 'from',\n",
       " 'average_',\n",
       " 'up_',\n",
       " 'into_',\n",
       " 'used_',\n",
       " 'are',\n",
       " ' = ',\n",
       " '\\n*[[',\n",
       " 'income_',\n",
       " 'those_',\n",
       " ']].\\n\\n',\n",
       " '65_',\n",
       " '24',\n",
       " 'tr',\n",
       " 'center',\n",
       " 'line',\n",
       " 'only_',\n",
       " '/ ',\n",
       " '2000',\n",
       " '><',\n",
       " 'c_',\n",
       " '13',\n",
       " '.\\n\\n==',\n",
       " '\\n| ',\n",
       " 'made_',\n",
       " 'sub',\n",
       " '1_',\n",
       " '14',\n",
       " 'states_',\n",
       " 'sup',\n",
       " 'it',\n",
       " 'm_',\n",
       " 'would_',\n",
       " '12',\n",
       " '20',\n",
       " 'her_',\n",
       " 'these_',\n",
       " 'area_',\n",
       " 'town_',\n",
       " 'any_',\n",
       " 'le',\n",
       " 'en',\n",
       " 'man',\n",
       " 'ng',\n",
       " 'family_',\n",
       " '10',\n",
       " 'em',\n",
       " \"'', \",\n",
       " 'than_',\n",
       " 'american_',\n",
       " 'every_',\n",
       " \"' \",\n",
       " 'ia',\n",
       " '11',\n",
       " '18',\n",
       " '2_',\n",
       " 'h_',\n",
       " 'ion',\n",
       " '. [[',\n",
       " 'math',\n",
       " 'at',\n",
       " 'external_',\n",
       " 'time_',\n",
       " 'g_',\n",
       " 'during_',\n",
       " 'white',\n",
       " ' | ',\n",
       " ']]) ',\n",
       " 'water',\n",
       " 'org',\n",
       " 'th_',\n",
       " 'for',\n",
       " 'known_',\n",
       " 'about_',\n",
       " 'links',\n",
       " 'people',\n",
       " '\\n|',\n",
       " '0_',\n",
       " 'k_',\n",
       " 'ic_',\n",
       " 'over_',\n",
       " 'non',\n",
       " '. \\n\\n',\n",
       " 'thumb',\n",
       " 'square_',\n",
       " 'years',\n",
       " 'ja',\n",
       " 'over',\n",
       " \"'' (\",\n",
       " '3_',\n",
       " ' ==\\n',\n",
       " '100_',\n",
       " 'located_',\n",
       " 'no',\n",
       " ']] - ',\n",
       " '==\\n',\n",
       " '21',\n",
       " '5_',\n",
       " 'land_',\n",
       " 'states',\n",
       " 'size_',\n",
       " 'ce',\n",
       " ']]\\n\\n[[',\n",
       " '22',\n",
       " 'see_',\n",
       " 'war',\n",
       " 'he',\n",
       " '30',\n",
       " 'ation_',\n",
       " \"]]'' \",\n",
       " 'native_',\n",
       " 'ra',\n",
       " 'ch',\n",
       " '4_',\n",
       " 'with',\n",
       " 'below_',\n",
       " '25',\n",
       " '> ',\n",
       " 'world_',\n",
       " 'she_',\n",
       " 'co',\n",
       " 'john_',\n",
       " 'where_',\n",
       " 'african_',\n",
       " 'le_',\n",
       " 'da',\n",
       " 'uk',\n",
       " 'families_',\n",
       " 'races',\n",
       " '] ',\n",
       " 'if_',\n",
       " 're',\n",
       " '23',\n",
       " 'ry',\n",
       " '44',\n",
       " 'name',\n",
       " 'pacific_',\n",
       " 'redirect',\n",
       " 'between_',\n",
       " 'however',\n",
       " 'st_',\n",
       " 'household_',\n",
       " 'el',\n",
       " 'user',\n",
       " 'older',\n",
       " 'national_',\n",
       " 'females_',\n",
       " 'asian',\n",
       " 'males',\n",
       " '>\\n<',\n",
       " '2003',\n",
       " 'png',\n",
       " 'households_',\n",
       " 'poverty_',\n",
       " 'te_',\n",
       " 'th',\n",
       " '). ',\n",
       " '6_',\n",
       " 'will_',\n",
       " 'part_',\n",
       " '31',\n",
       " 'language',\n",
       " 'te',\n",
       " 'time',\n",
       " '>[[',\n",
       " 've_',\n",
       " '26',\n",
       " ';).  ',\n",
       " '  ',\n",
       " '2005',\n",
       " 'list_',\n",
       " 'tion_',\n",
       " ']] ([[',\n",
       " 'ne',\n",
       " 'um',\n",
       " 'city',\n",
       " 'os',\n",
       " 'ton',\n",
       " 'ry_',\n",
       " '28',\n",
       " 'ter',\n",
       " 'use_',\n",
       " 'while_',\n",
       " '27',\n",
       " '7_',\n",
       " 'hispanic',\n",
       " 'being_',\n",
       " '64',\n",
       " 'islander',\n",
       " '8_',\n",
       " 'latino',\n",
       " 'ko',\n",
       " 'so_',\n",
       " 'up',\n",
       " \"]]'\",\n",
       " 'left',\n",
       " 'often_',\n",
       " 'ts',\n",
       " 'na',\n",
       " 'ka',\n",
       " 'style',\n",
       " 'é',\n",
       " '15',\n",
       " 'per',\n",
       " 'then_',\n",
       " '\\n*',\n",
       " 'ta',\n",
       " 'north_',\n",
       " '}} ',\n",
       " 'id',\n",
       " '\\n* ',\n",
       " 'south_',\n",
       " 'ted_',\n",
       " '\\n* [[',\n",
       " ' || ',\n",
       " 'them',\n",
       " 'small',\n",
       " 'ca',\n",
       " 've',\n",
       " 'film',\n",
       " '29',\n",
       " 'also',\n",
       " 'state_',\n",
       " 'became_',\n",
       " 'land',\n",
       " 'see',\n",
       " 'ty',\n",
       " 'rs',\n",
       " 'according_',\n",
       " 'both_',\n",
       " 'population',\n",
       " 'ers',\n",
       " '\\\\&undsc',\n",
       " 'se',\n",
       " 'ment_',\n",
       " '40',\n",
       " '°',\n",
       " 'top',\n",
       " 'fr',\n",
       " 'ke',\n",
       " '16',\n",
       " 'per_',\n",
       " ']\\n*[',\n",
       " '=[[',\n",
       " '19',\n",
       " '32',\n",
       " '\\n\\n',\n",
       " '9_',\n",
       " ' [',\n",
       " \"]]'' (\",\n",
       " 'li',\n",
       " 'son',\n",
       " '}}',\n",
       " 'so',\n",
       " 'ap',\n",
       " 'ol',\n",
       " 'html_',\n",
       " 'through_',\n",
       " 'people_',\n",
       " '}}\\n\\n[[',\n",
       " 'ff',\n",
       " 'number_',\n",
       " 'tion',\n",
       " 'world',\n",
       " 'ation',\n",
       " 'ies_',\n",
       " 'va',\n",
       " 'fi',\n",
       " '| ',\n",
       " '33',\n",
       " 'british_',\n",
       " \"''[[\",\n",
       " ' <',\n",
       " 'three_',\n",
       " '= ',\n",
       " 'ers_',\n",
       " '17',\n",
       " 'ru',\n",
       " 'music',\n",
       " ']]\\n',\n",
       " 'present',\n",
       " 'gr',\n",
       " '||',\n",
       " 'war_',\n",
       " 'later_',\n",
       " 'because_',\n",
       " 'name_',\n",
       " '2004',\n",
       " 'ni',\n",
       " 'ma',\n",
       " 'ce_',\n",
       " 'him_',\n",
       " 'p_',\n",
       " ';) ',\n",
       " 'well_',\n",
       " '35',\n",
       " 'several_',\n",
       " 'very_',\n",
       " 'west',\n",
       " 'county_',\n",
       " 'nt',\n",
       " 'pro',\n",
       " 'ic',\n",
       " 'university_',\n",
       " 'ga',\n",
       " 'able_',\n",
       " 'ns',\n",
       " 'ar',\n",
       " 'all',\n",
       " 'pl',\n",
       " 'go',\n",
       " 'ck',\n",
       " 'her',\n",
       " 'et',\n",
       " 'york',\n",
       " 'ho',\n",
       " 'cy',\n",
       " 'same_',\n",
       " 'ian_',\n",
       " 'pt',\n",
       " 'en_',\n",
       " 'since_',\n",
       " 'history',\n",
       " 'each_',\n",
       " 'px',\n",
       " 'fe',\n",
       " 'before_',\n",
       " 'ii',\n",
       " '34',\n",
       " 'ci',\n",
       " 'vi',\n",
       " 'year',\n",
       " \" '\",\n",
       " 'man_',\n",
       " 'bar',\n",
       " '37',\n",
       " 'mo',\n",
       " ']\\n* [',\n",
       " 'wa',\n",
       " 'early_',\n",
       " 'gar',\n",
       " 'do_',\n",
       " 'north',\n",
       " '===\\n',\n",
       " 'lo',\n",
       " 'even_',\n",
       " 'cal',\n",
       " 'although_',\n",
       " 'one',\n",
       " 'pre',\n",
       " ' ([[',\n",
       " 'ad',\n",
       " 'nl',\n",
       " 'la',\n",
       " 'children_',\n",
       " ']]\\n| ',\n",
       " '></',\n",
       " '41',\n",
       " 'state',\n",
       " 'ine',\n",
       " 'gi',\n",
       " 'xi',\n",
       " '36',\n",
       " 'men',\n",
       " 'mp',\n",
       " 'married_',\n",
       " '192',\n",
       " 'anti',\n",
       " 'der',\n",
       " '39',\n",
       " 'nt_',\n",
       " 'ive_',\n",
       " 'ism',\n",
       " ' & ',\n",
       " 'marriage',\n",
       " 'much_',\n",
       " 'century',\n",
       " 'ies',\n",
       " 'ze',\n",
       " 'could_',\n",
       " '25_',\n",
       " 'high_',\n",
       " 'tra',\n",
       " 'be',\n",
       " 'like_',\n",
       " 'now_',\n",
       " 'bc',\n",
       " 'ur',\n",
       " 'ya',\n",
       " ')\\n*[[',\n",
       " 'ble_',\n",
       " '- ',\n",
       " 'them_',\n",
       " '|\\n',\n",
       " 'ts_',\n",
       " 'za',\n",
       " 'ha',\n",
       " '90',\n",
       " '50',\n",
       " ']])\\n*[[',\n",
       " 'sv',\n",
       " 'out',\n",
       " 'x_',\n",
       " 'bgcolor',\n",
       " 'red_',\n",
       " 'what_',\n",
       " '42',\n",
       " \"]]'', \",\n",
       " 'sa',\n",
       " 'title',\n",
       " 'public_',\n",
       " 'second_',\n",
       " 'spread_',\n",
       " 'lu',\n",
       " 'pa',\n",
       " 'ure',\n",
       " 'female_',\n",
       " 'new',\n",
       " 'great_',\n",
       " 'am',\n",
       " '>(',\n",
       " 'ti',\n",
       " 'west_',\n",
       " '2002',\n",
       " 'rs_',\n",
       " 'ku',\n",
       " 'king_',\n",
       " '>\\n',\n",
       " 'ian',\n",
       " 'ary_',\n",
       " 'ist',\n",
       " 'а',\n",
       " 'general_',\n",
       " ']]\\n|',\n",
       " 'english_',\n",
       " '80',\n",
       " 'tu',\n",
       " 'based_',\n",
       " 'was',\n",
       " 'you_',\n",
       " 'party',\n",
       " '190',\n",
       " 'ment',\n",
       " 'way_',\n",
       " 'led_',\n",
       " 'history_',\n",
       " '38',\n",
       " 'best_',\n",
       " 'tt',\n",
       " '\"|',\n",
       " 'vo',\n",
       " 'bureau',\n",
       " 'me',\n",
       " '\", ',\n",
       " 'end_',\n",
       " 'against_',\n",
       " 'census_',\n",
       " 'do',\n",
       " ' = [[',\n",
       " 'geography',\n",
       " 'fa',\n",
       " 'ac',\n",
       " 'mile',\n",
       " 'ity_',\n",
       " 'hi',\n",
       " 'bo',\n",
       " 'density',\n",
       " 'work_',\n",
       " 'ity',\n",
       " 'ro',\n",
       " '\\xa0',\n",
       " 'ge',\n",
       " 'mac',\n",
       " 'sm',\n",
       " 'age',\n",
       " 'ver',\n",
       " 'se_',\n",
       " 'still_',\n",
       " 'gh',\n",
       " 'units_',\n",
       " 'f_',\n",
       " 'pe',\n",
       " 'system',\n",
       " 'jo',\n",
       " 'government_',\n",
       " 'si',\n",
       " 'sal',\n",
       " 'net',\n",
       " 'pi',\n",
       " 'france',\n",
       " 'day',\n",
       " '.\\n',\n",
       " 'ben',\n",
       " 'mal',\n",
       " 'che',\n",
       " 'older_',\n",
       " ']]), ',\n",
       " 'map',\n",
       " 'lt',\n",
       " 'band',\n",
       " 'mar',\n",
       " 'international_',\n",
       " \", ''\",\n",
       " 'ous_',\n",
       " 'own_',\n",
       " 'ster',\n",
       " 'ent',\n",
       " '45_',\n",
       " 'old_',\n",
       " 'someone_',\n",
       " 'ki',\n",
       " 'another_',\n",
       " 'together',\n",
       " 'individuals_',\n",
       " '43',\n",
       " 'la_',\n",
       " 'less_',\n",
       " 'town',\n",
       " 'zi',\n",
       " '==\\n\\n',\n",
       " '000_',\n",
       " 'el_',\n",
       " 'references',\n",
       " '191',\n",
       " '189',\n",
       " 'don',\n",
       " ']]</',\n",
       " 'major_',\n",
       " 'including_',\n",
       " 'called_',\n",
       " 'large_',\n",
       " 'ut',\n",
       " 'ju',\n",
       " 'way',\n",
       " 'box',\n",
       " 'cc',\n",
       " 'ting_',\n",
       " 'can',\n",
       " 'tan',\n",
       " 'wi',\n",
       " 'french_',\n",
       " 'ina',\n",
       " 'alone_',\n",
       " 'ref',\n",
       " '01',\n",
       " 'kilometer',\n",
       " 'post',\n",
       " 'until_',\n",
       " 'found_',\n",
       " 'housing_',\n",
       " 'well',\n",
       " 'main_',\n",
       " 'families',\n",
       " 'har',\n",
       " 'demographics',\n",
       " 'life',\n",
       " 'bra',\n",
       " 'village_',\n",
       " 'density_',\n",
       " 'william_',\n",
       " 'tar',\n",
       " 'husband_',\n",
       " 'racial_',\n",
       " 'io',\n",
       " 'htm_',\n",
       " 'day_',\n",
       " 'den',\n",
       " 'we_',\n",
       " 'ou',\n",
       " 'um_',\n",
       " \"''' (\",\n",
       " 'tri',\n",
       " 'zh',\n",
       " 'eg',\n",
       " 'political_',\n",
       " 'au',\n",
       " 'code',\n",
       " 'ham',\n",
       " 'ber',\n",
       " 'u_',\n",
       " 'versus',\n",
       " 'tic_',\n",
       " 'con',\n",
       " 'td_',\n",
       " 'males_',\n",
       " 'set_',\n",
       " 'cs',\n",
       " 'income',\n",
       " 'ec',\n",
       " 'ker',\n",
       " 'england',\n",
       " 'born_',\n",
       " '45',\n",
       " 'that',\n",
       " 'called',\n",
       " 'ne_',\n",
       " 'did_',\n",
       " 'form_',\n",
       " 'couples',\n",
       " 'township_',\n",
       " '70',\n",
       " 'lin',\n",
       " 'long_',\n",
       " ']]<',\n",
       " 'bu',\n",
       " 'capita_',\n",
       " 'eo',\n",
       " 'females',\n",
       " '.\\n* ',\n",
       " 'ms',\n",
       " ';]] (',\n",
       " '60',\n",
       " ')</',\n",
       " 'eu',\n",
       " '188',\n",
       " 'due_',\n",
       " 'different_',\n",
       " 'tor',\n",
       " 'households',\n",
       " 'makeup_',\n",
       " 'music_',\n",
       " 'ss',\n",
       " 'residing_',\n",
       " 'les',\n",
       " 'nd',\n",
       " 'album',\n",
       " 'ton_',\n",
       " ']]\\n\\n',\n",
       " 'king',\n",
       " '\">',\n",
       " 'dor',\n",
       " 'around_',\n",
       " '==',\n",
       " 'following_',\n",
       " 'sha',\n",
       " 'usually_',\n",
       " 'householder_',\n",
       " 'ali',\n",
       " '99',\n",
       " 'area',\n",
       " 'ty_',\n",
       " 'html',\n",
       " '.\\n\\n===',\n",
       " 'under',\n",
       " 'sta',\n",
       " 'w_',\n",
       " '47',\n",
       " 'cor',\n",
       " 'ey',\n",
       " 'eli',\n",
       " ': [[',\n",
       " 'system_',\n",
       " 'township',\n",
       " 'ny',\n",
       " 'han',\n",
       " 'om',\n",
       " 'ba',\n",
       " '48',\n",
       " 'river',\n",
       " 'mc',\n",
       " '|| ',\n",
       " 'ee',\n",
       " 'car',\n",
       " '46',\n",
       " ';]]).  ',\n",
       " 'ana',\n",
       " 'ds',\n",
       " '\\n[[',\n",
       " 'son_',\n",
       " '\" (',\n",
       " 'air_',\n",
       " 'il',\n",
       " 'back_',\n",
       " 'ld',\n",
       " \"''. \",\n",
       " 'last_',\n",
       " 'ps',\n",
       " 'year_',\n",
       " 'и',\n",
       " 'sh',\n",
       " 'she',\n",
       " 'within_',\n",
       " 'life_',\n",
       " 'small_',\n",
       " 'ep',\n",
       " 'un',\n",
       " 'we',\n",
       " 'canada',\n",
       " 'ble',\n",
       " 'hu',\n",
       " 'ir',\n",
       " '–',\n",
       " 'common_',\n",
       " 'ie',\n",
       " 'edu',\n",
       " 'ten',\n",
       " 'mer',\n",
       " 'est',\n",
       " '04',\n",
       " '100',\n",
       " '2001',\n",
       " 'series',\n",
       " 'pri',\n",
       " 'width',\n",
       " 'ide',\n",
       " 'if',\n",
       " 'though_',\n",
       " 're_',\n",
       " 'home',\n",
       " 'line_',\n",
       " 'dy',\n",
       " 'ea',\n",
       " 'hy',\n",
       " '51',\n",
       " 'air',\n",
       " 'ul',\n",
       " 'my_',\n",
       " 'ger',\n",
       " 'law',\n",
       " '\" | ',\n",
       " 'ste',\n",
       " 'chi',\n",
       " 'book',\n",
       " 'ner',\n",
       " '02',\n",
       " 'news',\n",
       " 'george_',\n",
       " 'par',\n",
       " 'order_',\n",
       " 'school_',\n",
       " ']].\\n\\n==',\n",
       " 'á',\n",
       " 'ht',\n",
       " 'ler',\n",
       " '* ',\n",
       " 'ae',\n",
       " ']]\\n* ',\n",
       " 'dis',\n",
       " 'mas',\n",
       " 'hor',\n",
       " '186',\n",
       " 'ser',\n",
       " 'di',\n",
       " 'power_',\n",
       " 'house_',\n",
       " 'east_',\n",
       " 'james_',\n",
       " 'ard',\n",
       " 'after',\n",
       " 'film_',\n",
       " 'fer',\n",
       " 'california',\n",
       " 'ex',\n",
       " ']]\\n\\n==',\n",
       " '}}\\n{{',\n",
       " 'wood',\n",
       " '|-',\n",
       " 'bel',\n",
       " 'gra',\n",
       " 'german_',\n",
       " 'school',\n",
       " 'inc',\n",
       " 'mel',\n",
       " 'ate_',\n",
       " '52',\n",
       " 'mil',\n",
       " '05',\n",
       " 'official_',\n",
       " \"]]'' ([[\",\n",
       " 'example',\n",
       " '03',\n",
       " 'among_',\n",
       " '55',\n",
       " 'home_',\n",
       " 'show',\n",
       " 'term',\n",
       " ']]-[[',\n",
       " '1999',\n",
       " 'rd',\n",
       " 'о',\n",
       " 'ale',\n",
       " '.\\n\\n[[',\n",
       " 'bi',\n",
       " '49',\n",
       " 'ive',\n",
       " 'des',\n",
       " 'four_',\n",
       " 'isbn_',\n",
       " 'sel',\n",
       " 'rt',\n",
       " 'modern_',\n",
       " 'ay',\n",
       " 'nor',\n",
       " 'ori',\n",
       " 'oo',\n",
       " 'yo',\n",
       " 'ves',\n",
       " 'place_',\n",
       " '95',\n",
       " '98',\n",
       " 'battle_',\n",
       " 'mon',\n",
       " 'border',\n",
       " 'military_',\n",
       " 'ks',\n",
       " 'january_',\n",
       " 'park',\n",
       " 'ast',\n",
       " 'pan',\n",
       " 'ö',\n",
       " 'high',\n",
       " 'roman_',\n",
       " 'group_',\n",
       " 'just_',\n",
       " 'press',\n",
       " 'art',\n",
       " 'mid',\n",
       " 'make_',\n",
       " 'game',\n",
       " 'ar_',\n",
       " 'ft',\n",
       " 'ven',\n",
       " 'government',\n",
       " ']]: ',\n",
       " 'trans',\n",
       " 'b_',\n",
       " ' \"[[',\n",
       " 'ct',\n",
       " 'mor',\n",
       " 'ele',\n",
       " 'popular_',\n",
       " 'hr',\n",
       " 'ama',\n",
       " ...]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword_text_encoder.subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250152226"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(subword_text_encoder.encode(x)) for x in page_revisions_text_simplified)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
